{
    "videos": [
 {"id":"2wK06mCJWHo","title":"A Conversation with Jensen Huang","srtEn":"1\n00:00:02,610 --> 00:00:07,800\n[Music]\n\n2\n00:00:08,240 --> 00:00:12,320\nHi, I'm Millie with the Special\n\n3\n00:00:10,160 --> 00:00:14,080\nCompetitive Studies Project. In this\n\n4\n00:00:12,320 --> 00:00:16,560\nweek's episode of Memos to the\n\n5\n00:00:14,080 --> 00:00:19,439\nPresident, I've had the opportunity to\n\n6\n00:00:16,560 --> 00:00:22,080\nchat with Jensen Wong, founder and CEO\n\n7\n00:00:19,439 --> 00:00:24,160\nof Nvidia. We cover all things related\n\n8\n00:00:22,080 --> 00:00:28,519\nto AI. I hope you'll enjoy the\n\n9\n00:00:24,160 --> 00:00:28,519\nconversation as much as I did.\n\n10\n00:00:32,480 --> 00:00:36,000\nHi, welcome back to Memos to the\n\n11\n00:00:34,160 --> 00:00:38,640\nPresident. It's really an honor to sit\n\n12\n00:00:36,000 --> 00:00:41,280\ndown today with a special guest, Jensen\n\n13\n00:00:38,640 --> 00:00:43,200\nWong, founder and CEO of Nvidia. This\n\n14\n00:00:41,280 --> 00:00:45,280\nwas a big week for Nvidia. So, I'm going\n\n15\n00:00:43,200 --> 00:00:48,399\nto turn over to Jensen to talk about how\n\n16\n00:00:45,280 --> 00:00:50,320\ndoes it feel this week to be the founder\n\n17\n00:00:48,399 --> 00:00:51,120\nand CEO of Nvidia. Jensen, welcome to my\n\n18\n00:00:50,320 --> 00:00:52,640\npodcast.\n\n19\n00:00:51,120 --> 00:00:55,280\n>> Thank you, Elie. It's an incredible\n\n20\n00:00:52,640 --> 00:00:56,559\nhonor. Wow. Crazy week, right?\n\n21\n00:00:55,280 --> 00:00:58,320\n>> It's an incredible week for Nvidia.\n\n22\n00:00:56,559 --> 00:01:00,079\nYeah, it's crazy week. I think the um\n\n23\n00:00:58,320 --> 00:01:04,000\nwell, first of all, I it hasn't really\n\n24\n00:01:00,079 --> 00:01:06,560\nsunk in yet. Um but let's see what\n\n25\n00:01:04,000 --> 00:01:08,159\nthoughts comes to mind. Well, you know,\n\n26\n00:01:06,560 --> 00:01:11,280\nin a lot of ways, somebody asked me\n\n27\n00:01:08,159 --> 00:01:14,640\nyesterday about and and and mentioned to\n\n28\n00:01:11,280 --> 00:01:16,320\nme uh that I'm an immigrant, and in\n\n29\n00:01:14,640 --> 00:01:16,960\nfact, I am an immigrant. Talking about\n\n30\n00:01:16,320 --> 00:01:18,000\nimmigration,\n\n31\n00:01:16,960 --> 00:01:19,600\n>> same here.\n\n32\n00:01:18,000 --> 00:01:23,439\n>> And the two of us are both immigrants.\n\n33\n00:01:19,600 --> 00:01:26,000\nAnd and uh America stands for\n\n34\n00:01:23,439 --> 00:01:27,680\nthe uh the land of the American dream.\n\n35\n00:01:26,000 --> 00:01:30,080\nAnd this is the place where immigrants\n\n36\n00:01:27,680 --> 00:01:32,880\ncome to to build build a life and build\n\n37\n00:01:30,080 --> 00:01:34,720\na company. In a lot of ways, you know,\n\n38\n00:01:32,880 --> 00:01:36,159\nwhat I'm experiencing is probably the\n\n39\n00:01:34,720 --> 00:01:37,759\nultimate American dream.\n\n40\n00:01:36,159 --> 00:01:39,280\n>> Absolutely. you know, to come come to\n\n41\n00:01:37,759 --> 00:01:41,840\nUnited States when I was seven years\n\n42\n00:01:39,280 --> 00:01:43,439\nold, not I guess eight years old and and\n\n43\n00:01:41,840 --> 00:01:45,439\num\n\n44\n00:01:43,439 --> 00:01:48,240\nand and having the opportunity to found\n\n45\n00:01:45,439 --> 00:01:53,119\na company with good friends 33 years\n\n46\n00:01:48,240 --> 00:01:56,240\nago, be the uh the CEO today after 33\n\n47\n00:01:53,119 --> 00:01:58,240\nyears and um having many of the founders\n\n48\n00:01:56,240 --> 00:02:00,960\nthat are that were there with me at the\n\n49\n00:01:58,240 --> 00:02:02,960\nvery beginning still here at NVIDIA um\n\n50\n00:02:00,960 --> 00:02:05,280\npursuing a dream that we've had that\n\n51\n00:02:02,960 --> 00:02:06,640\nthat took three decades to accomplish\n\n52\n00:02:05,280 --> 00:02:09,520\n>> with a lot of ups and downs. with\n\n53\n00:02:06,640 --> 00:02:12,319\nenormous amount of ups and downs and you\n\n54\n00:02:09,520 --> 00:02:15,440\nknow so in a lot of ways uh this\n\n55\n00:02:12,319 --> 00:02:16,959\nmilestone what is happening to us it's\n\n56\n00:02:15,440 --> 00:02:20,000\nkind of hard to internalize it's kind of\n\n57\n00:02:16,959 --> 00:02:21,680\nhard to take in but it also represents\n\n58\n00:02:20,000 --> 00:02:24,080\nsomething that's really important you\n\n59\n00:02:21,680 --> 00:02:27,120\nknow we uh we wanted to reinvent the\n\n60\n00:02:24,080 --> 00:02:28,800\ncomputer the computer as we know it uh\n\n61\n00:02:27,120 --> 00:02:30,800\nreally has been largely defined for\n\n62\n00:02:28,800 --> 00:02:34,959\nabout six decades since the IBM system\n\n63\n00:02:30,800 --> 00:02:36,720\n360 IBM as you know was was uh the the\n\n64\n00:02:34,959 --> 00:02:40,239\nlargest company in the world of of their\n\n65\n00:02:36,720 --> 00:02:43,040\ntime and and the blueprint that they put\n\n66\n00:02:40,239 --> 00:02:44,480\ntogether for computing was basically\n\n67\n00:02:43,040 --> 00:02:47,360\nit's the same blueprint that has been\n\n68\n00:02:44,480 --> 00:02:49,360\nplayed out in the last six decades.\n\n69\n00:02:47,360 --> 00:02:51,599\nEverything from the architecture of the\n\n70\n00:02:49,360 --> 00:02:54,400\nsystems, the way the separation between\n\n71\n00:02:51,599 --> 00:02:57,280\nsoftware and hardware and architecture\n\n72\n00:02:54,400 --> 00:03:00,080\ncompatibility and you know application\n\n73\n00:02:57,280 --> 00:03:01,680\ncompatibility, full family lineup, you\n\n74\n00:03:00,080 --> 00:03:04,959\nknow, all of the things that they they\n\n75\n00:03:01,680 --> 00:03:07,120\ndescribed uh largely describes the the\n\n76\n00:03:04,959 --> 00:03:09,360\ncomputer industry today and and in order\n\n77\n00:03:07,120 --> 00:03:12,000\nand the opportunity to reinvent that and\n\n78\n00:03:09,360 --> 00:03:14,159\ntake it to the next level and now being\n\n79\n00:03:12,000 --> 00:03:16,319\nthe platform for artificial intelligence\n\n80\n00:03:14,159 --> 00:03:17,840\nis really a dream come true.\n\n81\n00:03:16,319 --> 00:03:20,800\nYeah, really extraordinary time.\n\n82\n00:03:17,840 --> 00:03:23,200\n>> You talk about the AI waves\n\n83\n00:03:20,800 --> 00:03:24,560\n>> and I and I really like how you divide\n\n84\n00:03:23,200 --> 00:03:24,959\nthem into different categories.\n\n85\n00:03:24,560 --> 00:03:27,840\n>> Yeah.\n\n86\n00:03:24,959 --> 00:03:31,360\n>> Can you describe where we're today in in\n\n87\n00:03:27,840 --> 00:03:35,280\nterms of the wave and how we got here?\n\n88\n00:03:31,360 --> 00:03:37,599\n>> Um 20 2012 uh uh we saw the same moment\n\n89\n00:03:35,280 --> 00:03:40,799\nas everybody else. We had the we had the\n\n90\n00:03:37,599 --> 00:03:42,319\nthe inside track in the sense that uh we\n\n91\n00:03:40,799 --> 00:03:44,640\nalways believed that CUDA was going to\n\n92\n00:03:42,319 --> 00:03:47,200\nenable a new class of applications and\n\n93\n00:03:44,640 --> 00:03:50,799\nwe were always looking out for it. And\n\n94\n00:03:47,200 --> 00:03:53,120\nso when when um Alexet came along um\n\n95\n00:03:50,799 --> 00:03:56,640\nbuilt on top of CUDA, our GPUs made it\n\n96\n00:03:53,120 --> 00:03:59,120\npossible uh to train AlexNet and for\n\n97\n00:03:56,640 --> 00:04:01,599\nAlex Net to achieve such extraordinary\n\n98\n00:03:59,120 --> 00:04:04,879\nresults in computer vision, achieve the\n\n99\n00:04:01,599 --> 00:04:07,120\nlevel of capability that um computer\n\n100\n00:04:04,879 --> 00:04:09,120\nscientists specializing in computer\n\n101\n00:04:07,120 --> 00:04:11,760\nvision could not achieve over four\n\n102\n00:04:09,120 --> 00:04:15,200\ndecades. You know, for three people to\n\n103\n00:04:11,760 --> 00:04:17,919\nto do something like that. um uh it's\n\n104\n00:04:15,200 --> 00:04:20,000\njust an extraordinary feat. And so we\n\n105\n00:04:17,919 --> 00:04:21,680\ntook the opportunity and we we looked at\n\n106\n00:04:20,000 --> 00:04:23,680\nwhat is it that we're looking at? You\n\n107\n00:04:21,680 --> 00:04:26,240\nknow, what what's going on here? Is this\n\n108\n00:04:23,680 --> 00:04:29,360\nis Alec Alex al Alexnet a breakthrough\n\n109\n00:04:26,240 --> 00:04:31,440\nin computer vision or is it a bigger\n\n110\n00:04:29,360 --> 00:04:34,080\nidea than that? And of course, as we\n\n111\n00:04:31,440 --> 00:04:36,080\nknow, computer vision is a pillar of\n\n112\n00:04:34,080 --> 00:04:37,680\nartificial intelligence. You know,\n\n113\n00:04:36,080 --> 00:04:39,280\nwithout computer vision, without speech\n\n114\n00:04:37,680 --> 00:04:41,840\nlanguage understanding, it's hard to\n\n115\n00:04:39,280 --> 00:04:43,520\nhave intelligence. And and so we we\n\n116\n00:04:41,840 --> 00:04:45,440\nrealized that this of course was a part\n\n117\n00:04:43,520 --> 00:04:46,880\nof artificial intelligence. But is it a\n\n118\n00:04:45,440 --> 00:04:49,840\nbigger idea than that? And we came to\n\n119\n00:04:46,880 --> 00:04:53,680\nthe conclusion that what AlexNet and\n\n120\n00:04:49,840 --> 00:04:56,880\ndeep learning showed is that it is now\n\n121\n00:04:53,680 --> 00:05:00,000\nfinally possible if we had enough data,\n\n122\n00:04:56,880 --> 00:05:01,600\nenough computing scale. And of course we\n\n123\n00:05:00,000 --> 00:05:03,520\nhave these deep learning models that are\n\n124\n00:05:01,600 --> 00:05:07,039\nquite scalable\n\n125\n00:05:03,520 --> 00:05:10,000\num that we might be able to apply\n\n126\n00:05:07,039 --> 00:05:12,240\ncomputers to solve problems uh that were\n\n127\n00:05:10,000 --> 00:05:14,720\nimpossible to describe using human\n\n128\n00:05:12,240 --> 00:05:17,280\nengineered feature and using principled\n\n129\n00:05:14,720 --> 00:05:19,360\nalgorithms. And so we we um we got\n\n130\n00:05:17,280 --> 00:05:21,039\nexcited from that perspective. We also\n\n131\n00:05:19,360 --> 00:05:23,840\ngot excited because\n\n132\n00:05:21,039 --> 00:05:26,400\n>> because when you when you reason through\n\n133\n00:05:23,840 --> 00:05:29,280\num deep learning and and the training of\n\n134\n00:05:26,400 --> 00:05:31,199\nAlexNet and where it could go, we\n\n135\n00:05:29,280 --> 00:05:32,400\nrealized that the entire computing\n\n136\n00:05:31,199 --> 00:05:34,160\nplatform is going to change,\n\n137\n00:05:32,400 --> 00:05:35,680\n>> right? processors are going to change,\n\n138\n00:05:34,160 --> 00:05:37,600\nthe internet connect's going to change,\n\n139\n00:05:35,680 --> 00:05:39,440\nthe networking is going to change, the\n\n140\n00:05:37,600 --> 00:05:41,759\nsoftware stack on top of it, how you\n\n141\n00:05:39,440 --> 00:05:44,080\ndevelop the software, the methodology of\n\n142\n00:05:41,759 --> 00:05:46,080\nsoftware inside companies and of course\n\n143\n00:05:44,080 --> 00:05:48,000\nthe many industries that we might be\n\n144\n00:05:46,080 --> 00:05:50,479\nable to create was going to completely\n\n145\n00:05:48,000 --> 00:05:53,440\nchange and so we went about doing that.\n\n146\n00:05:50,479 --> 00:05:54,800\nWe re reset our company essentially. Now\n\n147\n00:05:53,440 --> 00:05:58,240\nthe the waves that you were talking\n\n148\n00:05:54,800 --> 00:06:00,639\nabout after we did that um we dedicated\n\n149\n00:05:58,240 --> 00:06:02,639\nourselves to uh creating new libraries\n\n150\n00:06:00,639 --> 00:06:05,759\ncalled KDNN\n\n151\n00:06:02,639 --> 00:06:09,520\nuh creating creating uh AI frameworks uh\n\n152\n00:06:05,759 --> 00:06:12,639\ncalled Mega Core um Megatron core uh to\n\n153\n00:06:09,520 --> 00:06:15,680\nuh inventing MVLink and tensor cores and\n\n154\n00:06:12,639 --> 00:06:17,840\nthe different numerical formats and uh\n\n155\n00:06:15,680 --> 00:06:21,199\nwe led to uh the creation of a system we\n\n156\n00:06:17,840 --> 00:06:23,039\ncall DGX1 our first AI supercomput. I\n\n157\n00:06:21,199 --> 00:06:24,400\npersonally delivered it to a startup in\n\n158\n00:06:23,039 --> 00:06:27,919\nSan Francisco which turned out to have\n\n159\n00:06:24,400 --> 00:06:30,560\nbeen open AI and and so anyways that\n\n160\n00:06:27,919 --> 00:06:33,199\njourney uh could really be captured in\n\n161\n00:06:30,560 --> 00:06:36,160\nseveral ways. Since 2012, the first\n\n162\n00:06:33,199 --> 00:06:38,160\nthing that happened, AI took off. Uh,\n\n163\n00:06:36,160 --> 00:06:39,840\ndeep learning kept advancing. The amount\n\n164\n00:06:38,160 --> 00:06:42,479\nof data we had, the amount of compute we\n\n165\n00:06:39,840 --> 00:06:44,240\nhad kept growing, kept scaling. And it\n\n166\n00:06:42,479 --> 00:06:47,680\nled to the first wave, which is really\n\n167\n00:06:44,240 --> 00:06:49,440\ndescribed as, if I could, perception. We\n\n168\n00:06:47,680 --> 00:06:50,479\nwe solved perception. It became\n\n169\n00:06:49,440 --> 00:06:52,240\nsuperhuman.\n\n170\n00:06:50,479 --> 00:06:54,319\n>> Computer vision became superhuman.\n\n171\n00:06:52,240 --> 00:06:56,800\nLanguage understanding or speech\n\n172\n00:06:54,319 --> 00:06:58,960\nrecognition became superhuman. The\n\n173\n00:06:56,800 --> 00:07:00,560\nsecond the second phase is uh\n\n174\n00:06:58,960 --> 00:07:03,919\ngenerative.\n\n175\n00:07:00,560 --> 00:07:06,560\nuh we can now not only understand\n\n176\n00:07:03,919 --> 00:07:09,199\ninformation but we can translate and\n\n177\n00:07:06,560 --> 00:07:11,120\ngenerate information. So text to text,\n\n178\n00:07:09,199 --> 00:07:13,680\ntext to images,\n\n179\n00:07:11,120 --> 00:07:15,840\n>> images to text, text to video.\n\n180\n00:07:13,680 --> 00:07:18,400\n>> And so if you could do text to video,\n\n181\n00:07:15,840 --> 00:07:20,319\nyou know what else can't can you do? So\n\n182\n00:07:18,400 --> 00:07:22,880\nsecond phase was generative AI. This\n\n183\n00:07:20,319 --> 00:07:25,120\nthird third wave is the wave that we're\n\n184\n00:07:22,880 --> 00:07:28,319\nin today, you know, really deeply\n\n185\n00:07:25,120 --> 00:07:28,880\nsolidly into which is uh reasoning AI.\n\n186\n00:07:28,319 --> 00:07:32,080\nMhm.\n\n187\n00:07:28,880 --> 00:07:36,240\n>> This is where an AI could apply\n\n188\n00:07:32,080 --> 00:07:39,599\nprinciples and knowledge, maybe some\n\n189\n00:07:36,240 --> 00:07:43,919\ncommon sense and use techniques like\n\n190\n00:07:39,599 --> 00:07:47,039\nchain of thought, trees of thought um to\n\n191\n00:07:43,919 --> 00:07:49,759\nuh break down the problem into multiple\n\n192\n00:07:47,039 --> 00:07:52,319\nsteps, reason about how to solve the\n\n193\n00:07:49,759 --> 00:07:54,479\nproblem, the larger goal step by step.\n\n194\n00:07:52,319 --> 00:07:57,360\nIt might even do some research, read\n\n195\n00:07:54,479 --> 00:07:59,199\nsome documents, read an archive paper\n\n196\n00:07:57,360 --> 00:08:00,879\nbefore it answers the question.\n\n197\n00:07:59,199 --> 00:08:05,759\n>> And so the third wave that we're in\n\n198\n00:08:00,879 --> 00:08:08,479\ntoday, which is reasoning, is a very big\n\n199\n00:08:05,759 --> 00:08:10,560\npart of seeing the acceleration of AI\n\n200\n00:08:08,479 --> 00:08:12,400\nbecoming AI. That's this the fact that\n\n201\n00:08:10,560 --> 00:08:15,039\nwe're we're doing reasoning AI is the\n\n202\n00:08:12,400 --> 00:08:17,520\nreason why why um uh people are starting\n\n203\n00:08:15,039 --> 00:08:18,240\nto say, you know, we're near uh general\n\n204\n00:08:17,520 --> 00:08:20,879\nintelligence,\n\n205\n00:08:18,240 --> 00:08:24,160\n>> right? And then the third the the next\n\n206\n00:08:20,879 --> 00:08:26,960\nwave after after uh uh after reasoning\n\n207\n00:08:24,160 --> 00:08:28,479\nAI is physical AI. This is where AI\n\n208\n00:08:26,960 --> 00:08:30,800\nknows how to now interact with the\n\n209\n00:08:28,479 --> 00:08:34,479\nphysical world. Has physical world\n\n210\n00:08:30,800 --> 00:08:36,959\ncommon sense like um object permanence,\n\n211\n00:08:34,479 --> 00:08:39,200\nfriction, inertia,\n\n212\n00:08:36,959 --> 00:08:41,360\ncause and effect, and you know, all of\n\n213\n00:08:39,200 --> 00:08:43,599\nthese types of types of common sense\n\n214\n00:08:41,360 --> 00:08:44,399\nthat children have, you know, puppies\n\n215\n00:08:43,599 --> 00:08:46,080\nhave.\n\n216\n00:08:44,399 --> 00:08:47,839\n>> You know, now we're going to AI is going\n\n217\n00:08:46,080 --> 00:08:49,760\nto have those things. And as a result of\n\n218\n00:08:47,839 --> 00:08:52,000\nthat, the the collection of all these\n\n219\n00:08:49,760 --> 00:08:53,519\ncapabilities, we should be able to see,\n\n220\n00:08:52,000 --> 00:08:54,640\nyou know, the next wave, which is\n\n221\n00:08:53,519 --> 00:08:55,519\nprobably robotics,\n\n222\n00:08:54,640 --> 00:08:58,880\n>> right? Yeah.\n\n223\n00:08:55,519 --> 00:09:00,800\n>> Um, you know, you've built the digital\n\n224\n00:08:58,880 --> 00:09:04,720\ninfrastructure for the way we live in\n\n225\n00:09:00,800 --> 00:09:07,360\ntoday. Uh, you're also working and, um,\n\n226\n00:09:04,720 --> 00:09:09,440\nhave a vision for the AI factories. Can\n\n227\n00:09:07,360 --> 00:09:11,519\nyou unpack what does that mean? Yeah.\n\n228\n00:09:09,440 --> 00:09:14,399\n>> How does that transform today's data\n\n229\n00:09:11,519 --> 00:09:17,680\ncenter for the future? the semiconductor\n\n230\n00:09:14,399 --> 00:09:19,200\nindustry, TSMC, uh the the uh computer\n\n231\n00:09:17,680 --> 00:09:20,959\necosystem\n\n232\n00:09:19,200 --> 00:09:24,160\nuh that that then create these\n\n233\n00:09:20,959 --> 00:09:27,120\ncomputers, Nvidia today, we represent,\n\n234\n00:09:24,160 --> 00:09:29,279\nif you will, the digital ecosystem, the\n\n235\n00:09:27,120 --> 00:09:31,200\ndigital infrastructure of the world, the\n\n236\n00:09:29,279 --> 00:09:32,480\ncomputing infrastructure.\n\n237\n00:09:31,200 --> 00:09:34,560\n>> And on top of that computing\n\n238\n00:09:32,480 --> 00:09:37,279\ninfrastructure realized this thing\n\n239\n00:09:34,560 --> 00:09:40,880\ncalled artificial intelligence.\n\n240\n00:09:37,279 --> 00:09:43,760\nAnd what is interesting about about um\n\n241\n00:09:40,880 --> 00:09:46,720\nabout the uh the last industry is that\n\n242\n00:09:43,760 --> 00:09:49,200\nwe the the the digital infrastructure\n\n243\n00:09:46,720 --> 00:09:51,839\nthe computer industry uh enabled\n\n244\n00:09:49,200 --> 00:09:54,720\nsoftware and that represents you know\n\n245\n00:09:51,839 --> 00:09:56,959\nabout a trillion dollars of industry.\n\n246\n00:09:54,720 --> 00:09:59,440\nYou you u uh use the computer\n\n247\n00:09:56,959 --> 00:10:01,440\ninfrastructures to write the software\n\n248\n00:09:59,440 --> 00:10:04,800\nbut then you deploy the software into\n\n249\n00:10:01,440 --> 00:10:07,920\nthings like you know phones, smartphones\n\n250\n00:10:04,800 --> 00:10:11,040\nand and so the software industry was not\n\n251\n00:10:07,920 --> 00:10:12,880\nvery large you know it's a you know call\n\n252\n00:10:11,040 --> 00:10:15,200\nit a half trillion dollars right\n\n253\n00:10:12,880 --> 00:10:17,760\n>> and the hardware industry not very large\n\n254\n00:10:15,200 --> 00:10:19,920\ncall it a half trillion dollars and all\n\n255\n00:10:17,760 --> 00:10:22,640\nof a sudden this industry the computer\n\n256\n00:10:19,920 --> 00:10:24,880\nindustry enabled artificial intelligence\n\n257\n00:10:22,640 --> 00:10:26,959\nand what's really interesting is\n\n258\n00:10:24,880 --> 00:10:28,800\nartificial intelligence is both this\n\n259\n00:10:26,959 --> 00:10:30,880\nrevolutionary technology that we just\n\n260\n00:10:28,800 --> 00:10:33,279\ntalked about\n\n261\n00:10:30,880 --> 00:10:35,279\nand it's because of its perception and\n\n262\n00:10:33,279 --> 00:10:37,279\nreasoning capability. You can use it to\n\n263\n00:10:35,279 --> 00:10:39,120\nsolve problems in just about every\n\n264\n00:10:37,279 --> 00:10:40,959\nsingle industry because every industry\n\n265\n00:10:39,120 --> 00:10:43,600\nthat we know at the pill at the\n\n266\n00:10:40,959 --> 00:10:48,000\nfoundation of it is intelligence and now\n\n267\n00:10:43,600 --> 00:10:50,079\nwe can create intelligence at in\n\n268\n00:10:48,000 --> 00:10:52,399\nincredible scales and of course that's\n\n269\n00:10:50,079 --> 00:10:54,079\ngoing to revolutionize every industry.\n\n270\n00:10:52,399 --> 00:10:56,079\nUm\n\n271\n00:10:54,079 --> 00:10:58,640\nthat's the technology perspective. What\n\n272\n00:10:56,079 --> 00:11:01,279\nabout the industrial perspective? In\n\n273\n00:10:58,640 --> 00:11:02,640\norder to produce these AI uh what is\n\n274\n00:11:01,279 --> 00:11:05,279\nactually coming out of it? What's coming\n\n275\n00:11:02,640 --> 00:11:08,000\nout of these models is tokens and these\n\n276\n00:11:05,279 --> 00:11:11,279\ntokens are formulated into words and\n\n277\n00:11:08,000 --> 00:11:13,920\nnumbers and symbols and could be in the\n\n278\n00:11:11,279 --> 00:11:18,480\nfuture chemicals and proteins for drug\n\n279\n00:11:13,920 --> 00:11:22,399\ndiscovery. It could be actuator motions\n\n280\n00:11:18,480 --> 00:11:26,399\nto uh to um drive a self-driving car uh\n\n281\n00:11:22,399 --> 00:11:29,519\nor animate a robot. And so so these\n\n282\n00:11:26,399 --> 00:11:31,760\nthese tokens that are coming out are\n\n283\n00:11:29,519 --> 00:11:34,480\nreformulated\n\n284\n00:11:31,760 --> 00:11:36,720\nreconstituted into intelligence of\n\n285\n00:11:34,480 --> 00:11:38,640\ndifferent kinds.\n\n286\n00:11:36,720 --> 00:11:41,040\nBut what what it takes to generate these\n\n287\n00:11:38,640 --> 00:11:42,480\ntokens at the scale that we need to\n\n288\n00:11:41,040 --> 00:11:46,160\nsupport all these industries and\n\n289\n00:11:42,480 --> 00:11:48,640\neverybody using AI are these large data\n\n290\n00:11:46,160 --> 00:11:50,079\ncenters and I and I stopped calling it a\n\n291\n00:11:48,640 --> 00:11:51,360\ndata center because in fact it's not a\n\n292\n00:11:50,079 --> 00:11:53,040\ndata center. It's not about it's not\n\n293\n00:11:51,360 --> 00:11:55,760\nabout the classical data centers are\n\n294\n00:11:53,040 --> 00:11:57,839\nretrieving data. It's a new type of data\n\n295\n00:11:55,760 --> 00:12:00,000\ncenter and its job is singular to\n\n296\n00:11:57,839 --> 00:12:02,240\nproduce tokens and that's why I call it\n\n297\n00:12:00,000 --> 00:12:05,120\nan AI factory. Yeah.\n\n298\n00:12:02,240 --> 00:12:07,120\n>> And you used also token per meter uh uh\n\n299\n00:12:05,120 --> 00:12:08,240\non how these AI factories will be\n\n300\n00:12:07,120 --> 00:12:09,920\nutilized going forward.\n\n301\n00:12:08,240 --> 00:12:12,800\n>> Exactly. And and what's what's really\n\n302\n00:12:09,920 --> 00:12:16,880\ninteresting about that is is um the last\n\n303\n00:12:12,800 --> 00:12:19,200\nindustry of factories creating electrons\n\n304\n00:12:16,880 --> 00:12:19,519\nwas the power generation industry,\n\n305\n00:12:19,200 --> 00:12:21,839\n>> right?\n\n306\n00:12:19,519 --> 00:12:24,560\n>> It represented 30% of the world's\n\n307\n00:12:21,839 --> 00:12:29,519\neconomy at one at one time. what is\n\n308\n00:12:24,560 --> 00:12:32,880\nproduced out of it is uh monetized at\n\n309\n00:12:29,519 --> 00:12:35,040\nyou know kilowatt hours per dollar. Now\n\n310\n00:12:32,880 --> 00:12:35,760\nwe have these ideas called mega tokens\n\n311\n00:12:35,040 --> 00:12:36,240\nper dollar\n\n312\n00:12:35,760 --> 00:12:39,120\n>> right\n\n313\n00:12:36,240 --> 00:12:41,680\n>> and it comes out as electrons again you\n\n314\n00:12:39,120 --> 00:12:44,959\nknow and so in instead of pure electrons\n\n315\n00:12:41,680 --> 00:12:46,880\nnow it's value added electrons and we\n\n316\n00:12:44,959 --> 00:12:48,399\ncall them tokens and they're we're going\n\n317\n00:12:46,880 --> 00:12:51,040\nto create essentially a whole new\n\n318\n00:12:48,399 --> 00:12:52,399\nindustry and this industry needs energy\n\n319\n00:12:51,040 --> 00:12:54,720\nwhich is the reason why President\n\n320\n00:12:52,399 --> 00:12:57,839\nTrump's pro- energy and and energy\n\n321\n00:12:54,720 --> 00:13:00,800\ngrowth uh initiative is is so timely\n\n322\n00:12:57,839 --> 00:13:03,200\nbecause at the exact time when America\n\n323\n00:13:00,800 --> 00:13:07,920\nwants to be great at AI I and wants to\n\n324\n00:13:03,200 --> 00:13:09,920\nbe a world leader in the AI ecosystem.\n\n325\n00:13:07,920 --> 00:13:11,600\nWithout the energy that's necessary to\n\n326\n00:13:09,920 --> 00:13:13,760\ncreate these AI factories, we wouldn't\n\n327\n00:13:11,600 --> 00:13:16,480\nbe able to do that. So the the\n\n328\n00:13:13,760 --> 00:13:19,200\nconfluence of President Trump's vision\n\n329\n00:13:16,480 --> 00:13:22,240\nand his his his\n\n330\n00:13:19,200 --> 00:13:25,120\ndrive to enable energy growth in our\n\n331\n00:13:22,240 --> 00:13:27,600\nnation and the confluence of the the\n\n332\n00:13:25,120 --> 00:13:30,480\nreadiness of the technology and the\n\n333\n00:13:27,600 --> 00:13:32,320\nreadiness of the world for AI, it all\n\n334\n00:13:30,480 --> 00:13:34,399\nhappened at exactly the right time, you\n\n335\n00:13:32,320 --> 00:13:36,000\nknow, and so so this this is going to\n\n336\n00:13:34,399 --> 00:13:36,560\nit's going to enable a new industry in\n\n337\n00:13:36,000 --> 00:13:39,680\nfront of us.\n\n338\n00:13:36,560 --> 00:13:41,839\n>> Um just a a question on that enabling of\n\n339\n00:13:39,680 --> 00:13:44,079\nnew industries. A lot of people are\n\n340\n00:13:41,839 --> 00:13:46,079\nnervous about jobs. Yeah. Um how is this\n\n341\n00:13:44,079 --> 00:13:48,160\ngoing to impact the workforce? This is\n\n342\n00:13:46,079 --> 00:13:51,200\nnot the first time that you know a\n\n343\n00:13:48,160 --> 00:13:53,519\nmassive transformative technology you\n\n344\n00:13:51,200 --> 00:13:56,320\nknow comes to our lives and it creates\n\n345\n00:13:53,519 --> 00:13:59,040\nnew job but it also you know makes\n\n346\n00:13:56,320 --> 00:14:01,440\nimpact on people losing their jobs. How\n\n347\n00:13:59,040 --> 00:14:06,680\ndo you see this transformation happening\n\n348\n00:14:01,440 --> 00:14:06,680\nin the workforce? New technologies\n\n349\n00:14:06,720 --> 00:14:11,920\nand productivity\n\n350\n00:14:09,519 --> 00:14:13,839\ndrives the growth of industries and it\n\n351\n00:14:11,920 --> 00:14:17,279\ncreates jobs.\n\n352\n00:14:13,839 --> 00:14:19,680\nIn the case of electricity,\n\n353\n00:14:17,279 --> 00:14:21,760\nit's a new technology\n\n354\n00:14:19,680 --> 00:14:23,600\nand as I mentioned earlier, the energy\n\n355\n00:14:21,760 --> 00:14:26,480\nproduction\n\n356\n00:14:23,600 --> 00:14:29,760\nindustry represented 30% at one time of\n\n357\n00:14:26,480 --> 00:14:32,240\nthe world's economy. Not only did the\n\n358\n00:14:29,760 --> 00:14:33,920\nenergy production was a large industry\n\n359\n00:14:32,240 --> 00:14:35,519\nin itself creating these power\n\n360\n00:14:33,920 --> 00:14:38,399\ngeneration plants of all different types\n\n361\n00:14:35,519 --> 00:14:41,440\nall around the world, it also enabled\n\n362\n00:14:38,399 --> 00:14:45,279\nnew applications. Electricity enabled\n\n363\n00:14:41,440 --> 00:14:48,320\nlight bulbs, dishwashers, refrigerators,\n\n364\n00:14:45,279 --> 00:14:50,480\nright? Laundry machines, all kinds of\n\n365\n00:14:48,320 --> 00:14:52,240\nnew applications were created and all\n\n366\n00:14:50,480 --> 00:14:54,399\nthose applications created a new\n\n367\n00:14:52,240 --> 00:14:57,199\nindustry, created jobs.\n\n368\n00:14:54,399 --> 00:14:59,279\nNow the last industrial revolution which\n\n369\n00:14:57,199 --> 00:15:03,040\nUnited States is you know right in the\n\n370\n00:14:59,279 --> 00:15:06,079\nmiddle of was the information industrial\n\n371\n00:15:03,040 --> 00:15:08,639\nrevolution and that digital revolution\n\n372\n00:15:06,079 --> 00:15:12,000\nenabled productivity to grow in the last\n\n373\n00:15:08,639 --> 00:15:14,560\nthree decades or so productivity has\n\n374\n00:15:12,000 --> 00:15:17,360\ngrown about 80%.\n\n375\n00:15:14,560 --> 00:15:20,160\nAlong with it employment went up by 80%.\n\n376\n00:15:17,360 --> 00:15:23,360\nAnd so so when productivity goes up\n\n377\n00:15:20,160 --> 00:15:25,120\nemployment goes up. Now why is that?\n\n378\n00:15:23,360 --> 00:15:27,760\nIn fact, you could say that when\n\n379\n00:15:25,120 --> 00:15:30,079\nproductivity goes up, employment would\n\n380\n00:15:27,760 --> 00:15:33,199\ngo down because you could do more with\n\n381\n00:15:30,079 --> 00:15:36,000\nfewer people, right? But that's because\n\n382\n00:15:33,199 --> 00:15:37,839\nthat lacks imagination.\n\n383\n00:15:36,000 --> 00:15:40,160\nIf your company, let's just take it from\n\n384\n00:15:37,839 --> 00:15:43,120\na company's perspective. If a company\n\n385\n00:15:40,160 --> 00:15:46,399\nhas no new ideas and it's literally\n\n386\n00:15:43,120 --> 00:15:48,480\ndoing one thing and one thing only, when\n\n387\n00:15:46,399 --> 00:15:50,720\nour productivity goes up, we need fewer\n\n388\n00:15:48,480 --> 00:15:53,120\npeople to do it. But if you look at\n\n389\n00:15:50,720 --> 00:15:54,800\nNvidia, we have so many ideas. is we\n\n390\n00:15:53,120 --> 00:15:58,880\ndon't have enough time or people to go\n\n391\n00:15:54,800 --> 00:16:01,519\ndo it. The backlog of great ideas that\n\n392\n00:15:58,880 --> 00:16:03,519\nwe would love to go try\n\n393\n00:16:01,519 --> 00:16:05,839\nnew markets and new applications we like\n\n394\n00:16:03,519 --> 00:16:07,360\nto go create the backlog of that is\n\n395\n00:16:05,839 --> 00:16:09,839\nincredible.\n\n396\n00:16:07,360 --> 00:16:12,160\nNow, if I just had more people, more\n\n397\n00:16:09,839 --> 00:16:13,680\ntime, you know, considering how fast how\n\n398\n00:16:12,160 --> 00:16:15,360\nhow hard we're already working, I'm\n\n399\n00:16:13,680 --> 00:16:17,040\nalready working. If I just had more\n\n400\n00:16:15,360 --> 00:16:19,680\npeople, if I had more time, if I were\n\n401\n00:16:17,040 --> 00:16:21,040\nmore productive, we do more. Our company\n\n402\n00:16:19,680 --> 00:16:23,120\nwould be able to offer more things,\n\n403\n00:16:21,040 --> 00:16:25,600\nwould be be able to invent new ideas\n\n404\n00:16:23,120 --> 00:16:27,839\nthat created new industries. And so the\n\n405\n00:16:25,600 --> 00:16:31,519\nthe real the real theme is that are you\n\n406\n00:16:27,839 --> 00:16:34,320\na do you do you are you a hopeful person\n\n407\n00:16:31,519 --> 00:16:37,839\noptimistic person and you believe in in\n\n408\n00:16:34,320 --> 00:16:39,680\nidea creation or are you somebody who\n\n409\n00:16:37,839 --> 00:16:41,199\nbelieves you know there are no new ideas\n\n410\n00:16:39,680 --> 00:16:44,000\nleft\n\n411\n00:16:41,199 --> 00:16:46,320\nand quite frankly we're just working and\n\n412\n00:16:44,000 --> 00:16:48,959\nif we could just do that work more\n\n413\n00:16:46,320 --> 00:16:50,959\nproductively we'll be out of work right\n\n414\n00:16:48,959 --> 00:16:53,759\nI don't I think there's so much work to\n\n415\n00:16:50,959 --> 00:16:55,600\ndo there are so many ideas to pursue\n\n416\n00:16:53,759 --> 00:16:58,320\nthat if If I can do work more\n\n417\n00:16:55,600 --> 00:17:02,160\neffectively, I would simply do more. And\n\n418\n00:16:58,320 --> 00:17:06,640\nso I think that that that that\n\n419\n00:17:02,160 --> 00:17:07,600\noptimistic view is not naive. It's\n\n420\n00:17:06,640 --> 00:17:08,240\nactually history,\n\n421\n00:17:07,600 --> 00:17:10,720\n>> right?\n\n422\n00:17:08,240 --> 00:17:13,280\n>> History would suggest that humanity has\n\n423\n00:17:10,720 --> 00:17:15,839\na lot more ideas to go pursue. We have a\n\n424\n00:17:13,280 --> 00:17:17,839\nlot more challenges to go address. If we\n\n425\n00:17:15,839 --> 00:17:21,199\nshould just have more productivity, we\n\n426\n00:17:17,839 --> 00:17:23,360\ncan go get to it much faster. Well, we\n\n427\n00:17:21,199 --> 00:17:24,959\none one thing if I could just one thing\n\n428\n00:17:23,360 --> 00:17:27,520\nthat I would like to add\n\n429\n00:17:24,959 --> 00:17:29,360\n>> and this is this is um this is something\n\n430\n00:17:27,520 --> 00:17:30,799\nthat that you and I have spoken about\n\n431\n00:17:29,360 --> 00:17:36,000\nbefore.\n\n432\n00:17:30,799 --> 00:17:37,840\n>> Um it is vital that everyone engages AI\n\n433\n00:17:36,000 --> 00:17:40,000\nright away.\n\n434\n00:17:37,840 --> 00:17:42,080\nEvery adult,\n\n435\n00:17:40,000 --> 00:17:44,880\nevery working person, not working\n\n436\n00:17:42,080 --> 00:17:46,559\nperson, every child should address and\n\n437\n00:17:44,880 --> 00:17:49,120\nengage AI right away. And the reason for\n\n438\n00:17:46,559 --> 00:17:51,360\nthat is because AI is the greatest\n\n439\n00:17:49,120 --> 00:17:52,160\nequalization equalizing force.\n\n440\n00:17:51,360 --> 00:17:54,960\n>> That's a good point.\n\n441\n00:17:52,160 --> 00:17:58,080\n>> It is the first time in history that a\n\n442\n00:17:54,960 --> 00:18:01,840\ntechnology as incredible as artificial\n\n443\n00:17:58,080 --> 00:18:04,240\nintelligence is useful for someone who\n\n444\n00:18:01,840 --> 00:18:07,600\nknows how to program software, whether\n\n445\n00:18:04,240 --> 00:18:08,960\nyou program C++ or Python or you have no\n\n446\n00:18:07,600 --> 00:18:11,039\nidea how to use a computer.\n\n447\n00:18:08,960 --> 00:18:12,880\n>> Right? This is the very first time in\n\n448\n00:18:11,039 --> 00:18:14,720\nhistory that all of a sudden that\n\n449\n00:18:12,880 --> 00:18:16,880\ncomputer is easy to use. If you don't\n\n450\n00:18:14,720 --> 00:18:19,360\nknow how to use AI, just open up the\n\n451\n00:18:16,880 --> 00:18:20,160\nwebsite, go to Chad GPT, go to Gemini\n\n452\n00:18:19,360 --> 00:18:20,960\nPro, just say\n\n453\n00:18:20,160 --> 00:18:21,760\n>> ask a simple question.\n\n454\n00:18:20,960 --> 00:18:24,000\n>> Yeah. Yeah.\n\n455\n00:18:21,760 --> 00:18:26,080\n>> And you could even say, \"I have no idea\n\n456\n00:18:24,000 --> 00:18:26,640\nhow to use AI. Can you teach me how to\n\n457\n00:18:26,080 --> 00:18:27,520\nuse AI?\"\n\n458\n00:18:26,640 --> 00:18:29,440\n>> That's true.\n\n459\n00:18:27,520 --> 00:18:31,440\n>> And if you don't know how to type, hit\n\n460\n00:18:29,440 --> 00:18:32,160\nthe microphone button and speak to us.\n\n461\n00:18:31,440 --> 00:18:34,080\n>> Gordon. Yeah.\n\n462\n00:18:32,160 --> 00:18:37,200\n>> And if you don't understand English, you\n\n463\n00:18:34,080 --> 00:18:39,120\ncan speak whatever language you like.\n\n464\n00:18:37,200 --> 00:18:42,160\n>> It is an extraordinary thing.\n\n465\n00:18:39,120 --> 00:18:44,160\n>> It is an extraordinary thing. And I also\n\n466\n00:18:42,160 --> 00:18:45,440\nthink it's incredible that if the AI\n\n467\n00:18:44,160 --> 00:18:48,720\ndoesn't know that language, you tell the\n\n468\n00:18:45,440 --> 00:18:50,640\nAI go learn that language, right?\n\n469\n00:18:48,720 --> 00:18:52,640\nAnd so so I think I think everybody\n\n470\n00:18:50,640 --> 00:18:55,039\nneeds to to engage AI. It is the\n\n471\n00:18:52,640 --> 00:18:57,280\ngreatest equalization\n\n472\n00:18:55,039 --> 00:18:59,600\num uh equalization force that we have\n\n473\n00:18:57,280 --> 00:19:01,039\never known and it's going to empower\n\n474\n00:18:59,600 --> 00:19:02,720\nit's going to enable it's going to lift\n\n475\n00:19:01,039 --> 00:19:04,640\nsociety of all you know everywhere.\n\n476\n00:19:02,720 --> 00:19:06,480\n>> I agree with you. Switching now to\n\n477\n00:19:04,640 --> 00:19:08,000\nWashington. We're you're in Washington\n\n478\n00:19:06,480 --> 00:19:09,280\nas I said you had an incredible week.\n\n479\n00:19:08,000 --> 00:19:09,840\nYou also met with the president\n\n480\n00:19:09,280 --> 00:19:11,039\nyesterday.\n\n481\n00:19:09,840 --> 00:19:13,760\n>> Yes. first question.\n\n482\n00:19:11,039 --> 00:19:16,080\n>> So, it's incredible. Pro- innovation,\n\n483\n00:19:13,760 --> 00:19:17,919\npro growth,\n\n484\n00:19:16,080 --> 00:19:19,520\npro- energy,\n\n485\n00:19:17,919 --> 00:19:25,039\npro\n\n486\n00:19:19,520 --> 00:19:28,559\nuh industry, wants us to take AI by the\n\n487\n00:19:25,039 --> 00:19:31,600\nhorns and be the world leader, continue\n\n488\n00:19:28,559 --> 00:19:33,440\nto be the world leader. Uh so proud uh\n\n489\n00:19:31,600 --> 00:19:35,520\nof our country, so proud of our\n\n490\n00:19:33,440 --> 00:19:38,080\ncompanies, so proud of our people. Uh\n\n491\n00:19:35,520 --> 00:19:39,760\njust it's always Yeah. I every time I\n\n492\n00:19:38,080 --> 00:19:41,120\nmeet him, every time I'm I'm with him, I\n\n493\n00:19:39,760 --> 00:19:41,520\ncome back, you know, completely fired\n\n494\n00:19:41,120 --> 00:19:44,960\nup.\n\n495\n00:19:41,520 --> 00:19:47,679\n>> Uh so I have three questions for you for\n\n496\n00:19:44,960 --> 00:19:49,919\nthree different audiences. Number one is\n\n497\n00:19:47,679 --> 00:19:51,440\n>> you obviously met the president\n\n498\n00:19:49,919 --> 00:19:54,480\n>> and this is Memos to the President\n\n499\n00:19:51,440 --> 00:19:56,720\npodcast. What is your advice to him\n\n500\n00:19:54,480 --> 00:19:59,600\nabout what are the things we have to do\n\n501\n00:19:56,720 --> 00:20:01,679\nnow to stay ahead?\n\n502\n00:19:59,600 --> 00:20:04,480\nHe wants America.\n\n503\n00:20:01,679 --> 00:20:06,880\nWell, first of all, he recognizes that\n\n504\n00:20:04,480 --> 00:20:08,960\nthe computer industry\n\n505\n00:20:06,880 --> 00:20:12,640\nand one that I have the great honor to\n\n506\n00:20:08,960 --> 00:20:16,640\nbe part of. The computer industry is\n\n507\n00:20:12,640 --> 00:20:19,039\nAmerica's national treasure.\n\n508\n00:20:16,640 --> 00:20:20,640\nIn no other industry do we lead the\n\n509\n00:20:19,039 --> 00:20:23,520\nworld\n\n510\n00:20:20,640 --> 00:20:27,520\nto the level and scale of the computer\n\n511\n00:20:23,520 --> 00:20:30,320\nindustry. You can't find another one.\n\n512\n00:20:27,520 --> 00:20:32,159\nWe lost the telecommunications industry.\n\n513\n00:20:30,320 --> 00:20:34,720\nThere is no way we're going to lose the\n\n514\n00:20:32,159 --> 00:20:35,360\nAmerican computing industry and this\n\n515\n00:20:34,720 --> 00:20:37,039\ncomputer industry.\n\n516\n00:20:35,360 --> 00:20:39,360\n>> Talk about 5G because I think we have\n\n517\n00:20:37,039 --> 00:20:40,000\nhad this conversation. We lost the 5G\n\n518\n00:20:39,360 --> 00:20:42,400\nwave.\n\n519\n00:20:40,000 --> 00:20:44,240\n>> We lost the 5G wave. We lost it through\n\n520\n00:20:42,400 --> 00:20:46,000\ntechnology. We lost it through policy.\n\n521\n00:20:44,240 --> 00:20:46,799\nWe lost it through bad strategic\n\n522\n00:20:46,000 --> 00:20:49,280\nthinking.\n\n523\n00:20:46,799 --> 00:20:51,120\n>> Um it is incredible what happened and we\n\n524\n00:20:49,280 --> 00:20:52,799\nsimply cannot allow that to happen.\n\n525\n00:20:51,120 --> 00:20:53,840\n>> And we're still struggling to regain\n\n526\n00:20:52,799 --> 00:20:55,600\nthat territory.\n\n527\n00:20:53,840 --> 00:20:57,280\n>> It's going to be it's going to be rough,\n\n528\n00:20:55,600 --> 00:20:58,960\n>> right? It's going to be super rough. We\n\n529\n00:20:57,280 --> 00:21:01,679\nhave an opportunity with 6G, right?\n\n530\n00:20:58,960 --> 00:21:04,559\n>> Because 6G. That's right. Because of AI\n\n531\n00:21:01,679 --> 00:21:06,799\nalso. And so we are going to go do our\n\n532\n00:21:04,559 --> 00:21:07,919\nbest to help our country regain\n\n533\n00:21:06,799 --> 00:21:10,880\ntechnology leadership and\n\n534\n00:21:07,919 --> 00:21:13,360\ntelecommunications. But back on AI, he\n\n535\n00:21:10,880 --> 00:21:15,520\nwants he wants America to uh be the\n\n536\n00:21:13,360 --> 00:21:17,840\nworld's best. Of course, he wants her\n\n537\n00:21:15,520 --> 00:21:21,120\nwould to continue to lead the world. In\n\n538\n00:21:17,840 --> 00:21:23,600\norder to lead the world in AI because AI\n\n539\n00:21:21,120 --> 00:21:25,039\nis fundamentally about computing and\n\n540\n00:21:23,600 --> 00:21:26,720\ncomputing is fundamentally about\n\n541\n00:21:25,039 --> 00:21:29,440\ndevelopers.\n\n542\n00:21:26,720 --> 00:21:33,039\nThe first job of leadership of a\n\n543\n00:21:29,440 --> 00:21:36,000\ncomputing platform which AI is also is\n\n544\n00:21:33,039 --> 00:21:38,640\nto win all developers.\n\n545\n00:21:36,000 --> 00:21:41,360\nThe first job of any platform is to win\n\n546\n00:21:38,640 --> 00:21:43,600\nall developers. Later when we talk about\n\n547\n00:21:41,360 --> 00:21:46,320\n5G, I can show you exactly the same\n\n548\n00:21:43,600 --> 00:21:48,880\nthing. We had a policy that caused us to\n\n549\n00:21:46,320 --> 00:21:51,120\nlose all developers. We need to have a\n\n550\n00:21:48,880 --> 00:21:53,919\npolicy that enables us to win all\n\n551\n00:21:51,120 --> 00:21:56,799\ndevelopers. 50% of the world's AI\n\n552\n00:21:53,919 --> 00:21:59,120\ndevelopers are in China.\n\n553\n00:21:56,799 --> 00:22:01,520\nAI developers are all over the world.\n\n554\n00:21:59,120 --> 00:22:04,880\nTheir AI developers now come out growing\n\n555\n00:22:01,520 --> 00:22:07,440\nup in Africa, in Latin America, in\n\n556\n00:22:04,880 --> 00:22:09,280\nSoutheast Asia, in the Middle East. AI\n\n557\n00:22:07,440 --> 00:22:11,360\ndevelopers are everywhere. The reason\n\n558\n00:22:09,280 --> 00:22:14,559\nwhy AI developers are everywhere is\n\n559\n00:22:11,360 --> 00:22:17,120\nbecause every country, every industry,\n\n560\n00:22:14,559 --> 00:22:18,799\nevery company needs to have intelligence\n\n561\n00:22:17,120 --> 00:22:21,520\nand wants to engage artificial\n\n562\n00:22:18,799 --> 00:22:23,679\nintelligence. But it starts with 50% are\n\n563\n00:22:21,520 --> 00:22:25,360\nin China and we need to win those\n\n564\n00:22:23,679 --> 00:22:28,880\ndevelopers. And so I think the first\n\n565\n00:22:25,360 --> 00:22:31,280\nthing he that that I would I would um\n\n566\n00:22:28,880 --> 00:22:33,120\ncontinue to say every time I I can\n\n567\n00:22:31,280 --> 00:22:36,559\nbecause the technology is not easy to\n\n568\n00:22:33,120 --> 00:22:39,520\nunderstand is if we want America to lead\n\n569\n00:22:36,559 --> 00:22:42,240\nthe AI revolution and continue to be the\n\n570\n00:22:39,520 --> 00:22:45,360\nworld leader the first thing we need is\n\n571\n00:22:42,240 --> 00:22:47,919\nevery AI developer to build on American\n\n572\n00:22:45,360 --> 00:22:48,480\ntech stack. The second thing I would say\n\n573\n00:22:47,919 --> 00:22:50,960\nis that\n\n574\n00:22:48,480 --> 00:22:53,280\n>> I think you've also said\n\n575\n00:22:50,960 --> 00:22:54,559\num you want to set the global standard\n\n576\n00:22:53,280 --> 00:22:55,919\nfor the technology stack.\n\n577\n00:22:54,559 --> 00:22:58,480\n>> That's right. The American company\n\n578\n00:22:55,919 --> 00:23:00,159\nshould be those that set the standard.\n\n579\n00:22:58,480 --> 00:23:01,200\n>> The American tech stack should be the\n\n580\n00:23:00,159 --> 00:23:03,120\nglobal standard.\n\n581\n00:23:01,200 --> 00:23:05,600\n>> Just as the American dollar is the\n\n582\n00:23:03,120 --> 00:23:08,880\nglobal standard by which every country\n\n583\n00:23:05,600 --> 00:23:11,520\nbuilds on, we should want the American\n\n584\n00:23:08,880 --> 00:23:15,919\ntech stack to be the tech stack that the\n\n585\n00:23:11,520 --> 00:23:17,840\nAI stack that everyone builds on. Now\n\n586\n00:23:15,919 --> 00:23:20,720\nthe tech stack starts with chips and\n\n587\n00:23:17,840 --> 00:23:22,960\nsystems. It is not just the AI models on\n\n588\n00:23:20,720 --> 00:23:25,360\ntop. There are many AI models on top.\n\n589\n00:23:22,960 --> 00:23:26,640\nThere's incredible models of all kinds.\n\n590\n00:23:25,360 --> 00:23:29,440\nSome of them are open source, some of\n\n591\n00:23:26,640 --> 00:23:32,080\nthem are closed, some of for physics,\n\n592\n00:23:29,440 --> 00:23:34,799\nsome of them are for uh quantum, some of\n\n593\n00:23:32,080 --> 00:23:37,520\nthem are for communications. The AI\n\n594\n00:23:34,799 --> 00:23:39,600\nmodels are of all different types. The\n\n595\n00:23:37,520 --> 00:23:42,320\nthings that you do uh your initiative\n\n596\n00:23:39,600 --> 00:23:44,000\ncalled AI plus I just deeply love. AI\n\n597\n00:23:42,320 --> 00:23:46,880\nfor science, that model is obviously\n\n598\n00:23:44,000 --> 00:23:50,159\ndifferent than a chatbot. AI for quantum\n\n599\n00:23:46,880 --> 00:23:52,159\nobviously different. AI for 5G and 6G,\n\n600\n00:23:50,159 --> 00:23:53,919\nobviously different, right? And so AI\n\n601\n00:23:52,159 --> 00:23:57,360\nfor robotics, obviously different. And\n\n602\n00:23:53,919 --> 00:24:00,480\nso all these different models are all AI\n\n603\n00:23:57,360 --> 00:24:03,120\nmodels and they should all be built\n\n604\n00:24:00,480 --> 00:24:04,960\n>> on the American tech stack. And so the\n\n605\n00:24:03,120 --> 00:24:10,320\nsecond thing that I would advocate is\n\n606\n00:24:04,960 --> 00:24:13,600\nthat AI diffusion should not be to limit\n\n607\n00:24:10,320 --> 00:24:16,480\nAmerican tech stack to the world. AI\n\n608\n00:24:13,600 --> 00:24:19,039\ndiffusion should be about maximizing the\n\n609\n00:24:16,480 --> 00:24:21,520\nAmerican tech stack all over the world\n\n610\n00:24:19,039 --> 00:24:24,480\nso that every AI developer in the world\n\n611\n00:24:21,520 --> 00:24:26,880\nbuilds on the American standard. And as\n\n612\n00:24:24,480 --> 00:24:30,240\nwe know about the computing ecosystem,\n\n613\n00:24:26,880 --> 00:24:33,600\nthe virtual cycle is incredible. the\n\n614\n00:24:30,240 --> 00:24:35,760\nmore your technology is everywhere, the\n\n615\n00:24:33,600 --> 00:24:37,440\nmore developers you're gonna have, the\n\n616\n00:24:35,760 --> 00:24:38,880\nmore developers you're going to have,\n\n617\n00:24:37,440 --> 00:24:40,400\nthe more your technology is going to be\n\n618\n00:24:38,880 --> 00:24:41,679\neverywhere. And so this positive\n\n619\n00:24:40,400 --> 00:24:43,279\nfeedback system is\n\n620\n00:24:41,679 --> 00:24:45,840\n>> I will just piggyback on this topic\n\n621\n00:24:43,279 --> 00:24:46,320\nbecause you talk a lot about sovereign\n\n622\n00:24:45,840 --> 00:24:47,120\nAI.\n\n623\n00:24:46,320 --> 00:24:49,840\n>> Yeah.\n\n624\n00:24:47,120 --> 00:24:52,159\n>> What do you mean by that? How does a\n\n625\n00:24:49,840 --> 00:24:54,640\ncountry build a sovereign AI? Why does\n\n626\n00:24:52,159 --> 00:24:56,720\nit need it for? You know, obviously the\n\n627\n00:24:54,640 --> 00:24:58,240\nway Europeans will build sovereign AI is\n\n628\n00:24:56,720 --> 00:25:00,159\ngoing to be different than how African\n\n629\n00:24:58,240 --> 00:25:02,240\ncountries are big. But you're traveling\n\n630\n00:25:00,159 --> 00:25:04,159\naround the world and advocating for\n\n631\n00:25:02,240 --> 00:25:05,120\nsovereign AI. So can you unpack your\n\n632\n00:25:04,159 --> 00:25:07,200\nvision about this?\n\n633\n00:25:05,120 --> 00:25:09,919\n>> I'm advocating for the American tech\n\n634\n00:25:07,200 --> 00:25:11,919\nstack to be the tech stack that every\n\n635\n00:25:09,919 --> 00:25:14,080\ncountry builds on. That's what I'm\n\n636\n00:25:11,919 --> 00:25:15,520\nadvocating for. And the reason the\n\n637\n00:25:14,080 --> 00:25:17,840\nreason why every country needs to build\n\n638\n00:25:15,520 --> 00:25:19,279\ntheir own tech stack, their own AI, is\n\n639\n00:25:17,840 --> 00:25:20,960\nbecause even though they could use\n\n640\n00:25:19,279 --> 00:25:23,919\nAmerican AIS, there's no question. And\n\n641\n00:25:20,960 --> 00:25:26,159\nthey should um every country should use\n\n642\n00:25:23,919 --> 00:25:29,520\nOpen AI. Every c country should use\n\n643\n00:25:26,159 --> 00:25:31,440\nGemini. um Google's Gemini and every\n\n644\n00:25:29,520 --> 00:25:34,240\ncountry should use Grock, you know, and\n\n645\n00:25:31,440 --> 00:25:36,640\nand so these are incredible models and\n\n646\n00:25:34,240 --> 00:25:38,799\nso every country should use them, but\n\n647\n00:25:36,640 --> 00:25:42,080\nthey should also build their own\n\n648\n00:25:38,799 --> 00:25:44,960\nindigenous AI stack and their AI AI\n\n649\n00:25:42,080 --> 00:25:47,600\nmodels and that AI model is trained on\n\n650\n00:25:44,960 --> 00:25:49,520\ntheir language, their history, their the\n\n651\n00:25:47,600 --> 00:25:52,480\nknowledge of their society, their\n\n652\n00:25:49,520 --> 00:25:55,279\nculture, their values. It's not sensible\n\n653\n00:25:52,480 --> 00:25:58,240\nthat one western company will be able to\n\n654\n00:25:55,279 --> 00:26:00,559\ncapture and somehow appre deeply\n\n655\n00:25:58,240 --> 00:26:02,640\nappreciate the values of every country\n\n656\n00:26:00,559 --> 00:26:04,799\nand every religion and every background\n\n657\n00:26:02,640 --> 00:26:07,039\nand every you know society around the\n\n658\n00:26:04,799 --> 00:26:09,120\nworld. And so each one of them should be\n\n659\n00:26:07,039 --> 00:26:11,919\nable to build something on their own.\n\n660\n00:26:09,120 --> 00:26:14,559\nAnd that AI model will work with other\n\n661\n00:26:11,919 --> 00:26:18,240\nindustrial AI open AI models or maybe\n\n662\n00:26:14,559 --> 00:26:20,240\neven private um uh corporate models or\n\n663\n00:26:18,240 --> 00:26:22,640\nyou know specific\n\n664\n00:26:20,240 --> 00:26:24,799\nindustrial science models or whatever it\n\n665\n00:26:22,640 --> 00:26:26,159\nis. But all of these models are going to\n\n666\n00:26:24,799 --> 00:26:28,480\ninteract. They should be able to build\n\n667\n00:26:26,159 --> 00:26:29,919\ntheir own and but still we want them to\n\n668\n00:26:28,480 --> 00:26:32,080\nbuild on American tech stack.\n\n669\n00:26:29,919 --> 00:26:34,480\n>> Yeah. You've been pretty vocal about the\n\n670\n00:26:32,080 --> 00:26:36,640\nUS China tech competition. So I want to\n\n671\n00:26:34,480 --> 00:26:38,080\nget your views about how you view the\n\n672\n00:26:36,640 --> 00:26:40,000\ncompetition. you call them a peer\n\n673\n00:26:38,080 --> 00:26:41,919\ncompetitor, not a nearper but a peer\n\n674\n00:26:40,000 --> 00:26:43,760\ncompetitor that have serious products,\n\n675\n00:26:41,919 --> 00:26:47,200\nserious companies. Where do you think\n\n676\n00:26:43,760 --> 00:26:52,559\nthe competition stands now?\n\n677\n00:26:47,200 --> 00:26:55,840\nUm f first of all uh China is our\n\n678\n00:26:52,559 --> 00:26:57,600\ncompetitor and adversary not our enemy.\n\n679\n00:26:55,840 --> 00:27:00,559\nAnd the reason for that is because we\n\n680\n00:26:57,600 --> 00:27:02,159\nhave we have deep interconnections and\n\n681\n00:27:00,559 --> 00:27:04,960\ninterdependencies between the two\n\n682\n00:27:02,159 --> 00:27:08,240\ncountries.\n\n683\n00:27:04,960 --> 00:27:10,400\nAmerica is incredible. Our\n\n684\n00:27:08,240 --> 00:27:12,720\ntechnology leadership\n\n685\n00:27:10,400 --> 00:27:14,400\nis extraordinary.\n\n686\n00:27:12,720 --> 00:27:17,440\nThe computer industry by which I have\n\n687\n00:27:14,400 --> 00:27:20,400\nthe honor to serve\n\n688\n00:27:17,440 --> 00:27:24,000\nis the most talented, deeply capable\n\n689\n00:27:20,400 --> 00:27:27,200\ntech industry the world's ever seen.\n\n690\n00:27:24,000 --> 00:27:29,760\nAnd I expect us to retain\n\n691\n00:27:27,200 --> 00:27:33,360\nour leadership position for decades to\n\n692\n00:27:29,760 --> 00:27:36,799\ncome. And I welcome competition.\n\n693\n00:27:33,360 --> 00:27:39,520\nLet's go. you know, competitors, come\n\n694\n00:27:36,799 --> 00:27:42,640\non, let's go play. That's the American\n\n695\n00:27:39,520 --> 00:27:46,799\nspirit. The the competitive spirit that\n\n696\n00:27:42,640 --> 00:27:49,200\nwe have um isn't lost.\n\n697\n00:27:46,799 --> 00:27:53,120\nAnd we need the opportunity as the\n\n698\n00:27:49,200 --> 00:27:55,440\nAmerican industry to go fight for\n\n699\n00:27:53,120 --> 00:27:59,039\nAmerican leadership.\n\n700\n00:27:55,440 --> 00:28:01,840\nAnd at a time when when\n\n701\n00:27:59,039 --> 00:28:03,840\ncompanies countries around the world all\n\n702\n00:28:01,840 --> 00:28:06,320\nhave capabilities, frankly, we're\n\n703\n00:28:03,840 --> 00:28:08,480\ninterdependent and we depend on the\n\n704\n00:28:06,320 --> 00:28:10,320\ncapabilities of many countries. You\n\n705\n00:28:08,480 --> 00:28:12,480\nknow, the deeper you go, the more you\n\n706\n00:28:10,320 --> 00:28:14,080\nrealized, you realize there are things\n\n707\n00:28:12,480 --> 00:28:16,399\nin Europe that we depend on. There are\n\n708\n00:28:14,080 --> 00:28:17,919\nthings in in Japan we depend on. There\n\n709\n00:28:16,399 --> 00:28:19,600\nare things in Southeast Asia we depend\n\n710\n00:28:17,919 --> 00:28:21,840\non. There's things in Latin America we\n\n711\n00:28:19,600 --> 00:28:24,320\ndepend on. You know, every country has\n\n712\n00:28:21,840 --> 00:28:26,399\ntheir specialty and their capabilities.\n\n713\n00:28:24,320 --> 00:28:28,559\nAnd China of course has formidable\n\n714\n00:28:26,399 --> 00:28:31,279\ncapabilities. Their technology companies\n\n715\n00:28:28,559 --> 00:28:33,679\nare formidable. Huawei is formidable.\n\n716\n00:28:31,279 --> 00:28:38,159\nBYD is formidable. These are incredible\n\n717\n00:28:33,679 --> 00:28:41,440\ncompanies. Their their\n\n718\n00:28:38,159 --> 00:28:47,279\nnational pride in manufacturing\n\n719\n00:28:41,440 --> 00:28:49,760\nand uh deep really deep and broad you\n\n720\n00:28:47,279 --> 00:28:51,760\nknow scale of manufacturing expertise\n\n721\n00:28:49,760 --> 00:28:54,640\ncan be undermined. It's not about labor.\n\n722\n00:28:51,760 --> 00:28:55,039\nIt's technology plus craft and labor\n\n723\n00:28:54,640 --> 00:28:55,520\nscale,\n\n724\n00:28:55,039 --> 00:28:57,279\n>> right?\n\n725\n00:28:55,520 --> 00:29:00,080\n>> The combination of those three things\n\n726\n00:28:57,279 --> 00:29:03,279\ntogether is just extraordinary. It's\n\n727\n00:29:00,080 --> 00:29:06,799\nit's something to to witness. And so we\n\n728\n00:29:03,279 --> 00:29:10,159\nwe need to realize that that um we are\n\n729\n00:29:06,799 --> 00:29:12,480\nnow in a interdependent world. Uh and so\n\n730\n00:29:10,159 --> 00:29:15,760\nwhat do we do? uh the things that we\n\n731\n00:29:12,480 --> 00:29:18,000\nshould do uh one\n\n732\n00:29:15,760 --> 00:29:20,080\nwhat President Trump's\n\n733\n00:29:18,000 --> 00:29:24,960\ninitiative President Trump's initiative\n\n734\n00:29:20,080 --> 00:29:30,080\non uh localizing or re reindustrializing\n\n735\n00:29:24,960 --> 00:29:33,919\nAmerica is just a fantastic and and\n\n736\n00:29:30,080 --> 00:29:36,480\nvisionary and timely initiative. We need\n\n737\n00:29:33,919 --> 00:29:38,960\nto we need to be world class at the\n\n738\n00:29:36,480 --> 00:29:41,120\ntechnology of manufacturing, the craft\n\n739\n00:29:38,960 --> 00:29:43,760\nof manufacturing and the labor scale of\n\n740\n00:29:41,120 --> 00:29:46,640\nmanufacturing. Again, that entire part\n\n741\n00:29:43,760 --> 00:29:49,279\nof our ecosystem is somewhat lagging and\n\n742\n00:29:46,640 --> 00:29:51,200\nwe've lost our passion for it. Uh maybe\n\n743\n00:29:49,279 --> 00:29:52,960\nit's not it maybe it's because back in\n\n744\n00:29:51,200 --> 00:29:55,679\nthe old days it was more about labor\n\n745\n00:29:52,960 --> 00:29:57,840\nthan it was about technology. Um but now\n\n746\n00:29:55,679 --> 00:29:59,600\nit's deeply technical and it's something\n\n747\n00:29:57,840 --> 00:30:02,559\nthat we could really get passionate\n\n748\n00:29:59,600 --> 00:30:04,960\nbehind. And so I I think this whole area\n\n749\n00:30:02,559 --> 00:30:07,760\nof manufacturing so that we could reduce\n\n750\n00:30:04,960 --> 00:30:10,480\nour dependency on many countries around\n\n751\n00:30:07,760 --> 00:30:12,880\nthe world reduce the temperature there\n\n752\n00:30:10,480 --> 00:30:14,640\num have more capabilities ourselves.\n\n753\n00:30:12,880 --> 00:30:16,559\nIt's great for our national security.\n\n754\n00:30:14,640 --> 00:30:20,080\nIt's great for our industries. It's\n\n755\n00:30:16,559 --> 00:30:22,399\ngreat for um for job creation. It's\n\n756\n00:30:20,080 --> 00:30:24,799\ngreat for our culture, frankly. It's\n\n757\n00:30:22,399 --> 00:30:26,880\ngreat for our society overall. And so I\n\n758\n00:30:24,799 --> 00:30:29,360\nI love that vision, President Trump's\n\n759\n00:30:26,880 --> 00:30:31,760\nvision of re-industrializing America.\n\n760\n00:30:29,360 --> 00:30:33,279\nAnd so I think that we need to do that.\n\n761\n00:30:31,760 --> 00:30:35,440\nMeanwhile,\n\n762\n00:30:33,279 --> 00:30:38,880\nwe have to get extra we have to stay\n\n763\n00:30:35,440 --> 00:30:41,600\nextraordinarily excellent in uh areas\n\n764\n00:30:38,880 --> 00:30:44,240\nlike artificial intelligence and and AI\n\n765\n00:30:41,600 --> 00:30:47,440\ncomputing and and the the tech stack so\n\n766\n00:30:44,240 --> 00:30:51,440\nthat we could be a partner to every\n\n767\n00:30:47,440 --> 00:30:53,360\ncountry in the world and um uh and and\n\n768\n00:30:51,440 --> 00:30:56,000\nmake a contribution to every country in\n\n769\n00:30:53,360 --> 00:30:58,159\nthe world so that we could have uh this\n\n770\n00:30:56,000 --> 00:31:01,200\ncontinued you know interdependency of\n\n771\n00:30:58,159 --> 00:31:02,640\neach other and uh uh you know drive our\n\n772\n00:31:01,200 --> 00:31:04,559\nindustry straight forward.\n\n773\n00:31:02,640 --> 00:31:07,279\n>> Last question. Jensen, you talk about\n\n774\n00:31:04,559 --> 00:31:10,640\nregaining strategic confidence. Is this\n\n775\n00:31:07,279 --> 00:31:12,960\nwhat that means in your words, leading,\n\n776\n00:31:10,640 --> 00:31:16,480\nkeep inventing? And by this end of this\n\n777\n00:31:12,960 --> 00:31:18,240\ndecade, American technologies have built\n\n778\n00:31:16,480 --> 00:31:20,399\nthe global infrastructure both on the\n\n779\n00:31:18,240 --> 00:31:20,880\ntech stack hardware, but as well as\n\n780\n00:31:20,399 --> 00:31:22,960\nsoftware.\n\n781\n00:31:20,880 --> 00:31:25,600\n>> Yeah, exactly. you know, most of the\n\n782\n00:31:22,960 --> 00:31:27,200\ntime most of the time regulation um\n\n783\n00:31:25,600 --> 00:31:31,520\npolicies\n\n784\n00:31:27,200 --> 00:31:34,640\nuh uh tends to be tends to tends to\n\n785\n00:31:31,520 --> 00:31:36,559\nfocus too much on limiting and\n\n786\n00:31:34,640 --> 00:31:38,720\nrestricting.\n\n787\n00:31:36,559 --> 00:31:42,640\nUm and that that's that is that is fine\n\n788\n00:31:38,720 --> 00:31:46,320\nto do. Um, I just want to remind remind\n\n789\n00:31:42,640 --> 00:31:47,840\nuh ourselves that America is\n\n790\n00:31:46,320 --> 00:31:50,000\nextraordinary\n\n791\n00:31:47,840 --> 00:31:51,360\nand that the companies here and I I have\n\n792\n00:31:50,000 --> 00:31:53,440\nthe benefit of working with companies\n\n793\n00:31:51,360 --> 00:31:55,279\nall over the world. Nvidia is obviously\n\n794\n00:31:53,440 --> 00:31:57,600\na global company. We have we have\n\n795\n00:31:55,279 --> 00:32:00,080\nbusinesses all over the world. Um, I can\n\n796\n00:31:57,600 --> 00:32:03,600\nattest that this country is\n\n797\n00:32:00,080 --> 00:32:06,640\nextraordinary has extraordinary\n\n798\n00:32:03,600 --> 00:32:12,399\nwork ethics. Frankly, I think Americans\n\n799\n00:32:06,640 --> 00:32:15,440\nwork, if not as hard as any hardworking\n\n800\n00:32:12,399 --> 00:32:16,960\nculture in the world, but I consider\n\n801\n00:32:15,440 --> 00:32:19,279\nmany American companies and many\n\n802\n00:32:16,960 --> 00:32:22,480\nindustries, we work the hardest of any\n\n803\n00:32:19,279 --> 00:32:25,840\nindustry. And so so Americans work, we\n\n804\n00:32:22,480 --> 00:32:29,039\nhave work ethics, we have incredible\n\n805\n00:32:25,840 --> 00:32:30,559\num uh\n\n806\n00:32:29,039 --> 00:32:32,480\nfoundation\n\n807\n00:32:30,559 --> 00:32:36,000\nuh for supporting industry and\n\n808\n00:32:32,480 --> 00:32:38,480\nsupporting startups. And uh it's a it's\n\n809\n00:32:36,000 --> 00:32:39,840\nstill the world's best place for\n\n810\n00:32:38,480 --> 00:32:41,519\nimmigrants to come.\n\n811\n00:32:39,840 --> 00:32:43,120\n>> Yeah.\n\n812\n00:32:41,519 --> 00:32:44,880\nIt's still the world's best place for\n\n813\n00:32:43,120 --> 00:32:47,840\nimmigrants to come to get a great\n\n814\n00:32:44,880 --> 00:32:50,320\neducation and have the opportunity with\n\n815\n00:32:47,840 --> 00:32:53,919\nall the ecosystem around us to build a\n\n816\n00:32:50,320 --> 00:32:56,080\ngreat company. I saw it firsthand.\n\n817\n00:32:53,919 --> 00:32:59,120\nYou know, no one has enjoyed the\n\n818\n00:32:56,080 --> 00:33:01,039\nAmerican dream and saw it personally in\n\n819\n00:32:59,120 --> 00:33:03,919\nmy lifetime\n\n820\n00:33:01,039 --> 00:33:06,080\nthan I have. You know, if if there's a\n\n821\n00:33:03,919 --> 00:33:07,919\nbook that's called the American dream,\n\n822\n00:33:06,080 --> 00:33:10,399\nI'm I might be one of the chapters,\n\n823\n00:33:07,919 --> 00:33:12,960\nright? you know, and so so this is this\n\n824\n00:33:10,399 --> 00:33:16,880\nis I embody the American dream and and\n\n825\n00:33:12,960 --> 00:33:21,840\nso I I I walk with extraordinary pride\n\n826\n00:33:16,880 --> 00:33:24,399\nand great and gratitude um and uh uh and\n\n827\n00:33:21,840 --> 00:33:28,880\nrecognizing what the magic of the of\n\n828\n00:33:24,399 --> 00:33:32,640\nAmerica and uh and also uh a great\n\n829\n00:33:28,880 --> 00:33:35,360\nconfidence about what we can do and and\n\n830\n00:33:32,640 --> 00:33:38,320\nso I think that that whatever policies\n\n831\n00:33:35,360 --> 00:33:41,039\nare created are developed\n\n832\n00:33:38,320 --> 00:33:42,640\nis to realize, you know, and this is no\n\n833\n00:33:41,039 --> 00:33:44,480\ndifferent than than companies developing\n\n834\n00:33:42,640 --> 00:33:46,960\nstrategies. Before you develop\n\n835\n00:33:44,480 --> 00:33:49,039\nstrategies about the adversary or the\n\n836\n00:33:46,960 --> 00:33:52,159\ncompetition, the first thing you have to\n\n837\n00:33:49,039 --> 00:33:55,120\ndo is know thyself.\n\n838\n00:33:52,159 --> 00:33:58,559\nAnd the the strategies that you deploy\n\n839\n00:33:55,120 --> 00:34:00,720\nwhen you're in defense versus the\n\n840\n00:33:58,559 --> 00:34:04,480\nstrategies that you deploy and policies\n\n841\n00:34:00,720 --> 00:34:06,880\nyou deploy when you are in offense are\n\n842\n00:34:04,480 --> 00:34:08,879\nrelated, not the same. And so it's\n\n843\n00:34:06,880 --> 00:34:11,280\nreally important to s get a sense of\n\n844\n00:34:08,879 --> 00:34:14,560\nwhat our what our national capabilities\n\n845\n00:34:11,280 --> 00:34:17,359\nare and especially in the field of\n\n846\n00:34:14,560 --> 00:34:19,440\nartificial intelligence and computing uh\n\n847\n00:34:17,359 --> 00:34:21,440\nto recognize what an extraordinary\n\n848\n00:34:19,440 --> 00:34:24,879\nindustry we've created somehow over the\n\n849\n00:34:21,440 --> 00:34:27,679\nyears and it is our national treasure.\n\n850\n00:34:24,879 --> 00:34:31,200\nWe should do everything we can to\n\n851\n00:34:27,679 --> 00:34:33,440\nfurther this capability to nurture this\n\n852\n00:34:31,200 --> 00:34:36,560\ncapability to protect this capability\n\n853\n00:34:33,440 --> 00:34:38,240\nand enhance it. And so I can't tell you\n\n854\n00:34:36,560 --> 00:34:41,599\nhow proud it, you know, here I am in\n\n855\n00:34:38,240 --> 00:34:44,079\nWashington DC, uh, and, uh, our nation's\n\n856\n00:34:41,599 --> 00:34:46,320\ncapital. Uh, you it's hard, it's hard\n\n857\n00:34:44,079 --> 00:34:50,320\nnot to feel patriotic after you see the\n\n858\n00:34:46,320 --> 00:34:52,960\npresident and and, um, but it it is a a\n\n859\n00:34:50,320 --> 00:34:56,000\ngreat reminder what an amazing country\n\n860\n00:34:52,960 --> 00:34:58,160\nwe've somehow built over the years and\n\n861\n00:34:56,000 --> 00:35:00,320\nwhat an industry that has been that has\n\n862\n00:34:58,160 --> 00:35:02,320\nemerged from it as a result. And we have\n\n863\n00:35:00,320 --> 00:35:04,079\nevery reason to be proud and every\n\n864\n00:35:02,320 --> 00:35:06,800\nreason to be confident. On that\n\n865\n00:35:04,079 --> 00:35:09,520\nincredible uh last note, thank you for\n\n866\n00:35:06,800 --> 00:35:11,040\nreally um being a guest on our show.\n\n867\n00:35:09,520 --> 00:35:12,800\nTruly appreciate. We value the\n\n868\n00:35:11,040 --> 00:35:14,400\npartnership we have with Nvidia and I\n\n869\n00:35:12,800 --> 00:35:15,599\nlook forward to many more conversation\n\n870\n00:35:14,400 --> 00:35:19,470\nwith you. Thank you, Jensen.\n\n871\n00:35:15,599 --> 00:35:22,720\n>> Thank you. It's great to be here.\n\n872\n00:35:19,470 --> 00:35:24,320\n[Music]\n\n873\n00:35:22,720 --> 00:35:27,520\n>> Thank you for watching my interview with\n\n874\n00:35:24,320 --> 00:35:29,520\nthe Nvidia CEO Jensen Wong. I hope you\n\n875\n00:35:27,520 --> 00:35:32,320\nenjoyed the conversation as much as I\n\n876\n00:35:29,520 --> 00:35:33,680\ndid. While I have you, I also wanted to\n\n877\n00:35:32,320 --> 00:35:35,760\nlet you know that the special\n\n878\n00:35:33,680 --> 00:35:39,359\ncompetitive studies project is\n\n879\n00:35:35,760 --> 00:35:42,560\norganizing an allday summit on AI plus\n\n880\n00:35:39,359 --> 00:35:45,040\nscience on July 23rd here in Washington\n\n881\n00:35:42,560 --> 00:35:47,359\nDC. So, please register if you're\n\n882\n00:35:45,040 --> 00:35:49,040\ninterested in attending and being part\n\n883\n00:35:47,359 --> 00:35:52,640\nof amazing conversations we have\n\n884\n00:35:49,040 --> 00:35:55,440\nplanned. Lastly, in March of this year,\n\n885\n00:35:52,640 --> 00:35:58,079\nwe launched a Genai course for national\n\n886\n00:35:55,440 --> 00:36:00,240\nsecurity in partnership with Corsera.\n\n887\n00:35:58,079 --> 00:36:02,400\nIt's an incredible course. More than\n\n888\n00:36:00,240 --> 00:36:04,480\n3,000 people have enrolled. You can be\n\n889\n00:36:02,400 --> 00:36:09,640\none of them if you start doing that\n\n890\n00:36:04,480 --> 00:36:09,640\ntoday. Hope you'll enjoy it. Thank you.\n\n","srtEs":"1\n00:00:02,610 --> 00:00:07,800\n[Msica]\n\n2\n00:00:08,240 --> 00:00:12,320\nHola, soy Millie con el especial.\n\n3\n00:00:10,160 --> 00:00:14,080\nProyecto de Estudios Competitivos. en esto\n\n4\n00:00:12,320 --> 00:00:16,560\nepisodio de la semana de Memos to the\n\n5\n00:00:14,080 --> 00:00:19,439\nPresidente, he tenido la oportunidad de\n\n6\n00:00:16,560 --> 00:00:22,080\ncharle con Jensen Wong, fundador y director ejecutivo\n\n7\n00:00:19,439 --> 00:00:24,160\nde Nvidia. Cubrimos todo lo relacionado\n\n8\n00:00:22,080 --> 00:00:28,519\na la IA. Espero que disfrutes el\n\n9\n00:00:24,160 --> 00:00:28,519\nconversacin tanto como yo.\n\n10\n00:00:32,480 --> 00:00:36,000\nHola, bienvenido de nuevo a Memos to the\n\n11\n00:00:34,160 --> 00:00:38,640\nPresidente. Es realmente un honor sentarse\n\n12\n00:00:36,000 --> 00:00:41,280\nhoy con un invitado especial, Jensen\n\n13\n00:00:38,640 --> 00:00:43,200\nWong, fundador y director ejecutivo de Nvidia. Este\n\n14\n00:00:41,280 --> 00:00:45,280\nFue una gran semana para Nvidia. Entonces, me voy\n\n15\n00:00:43,200 --> 00:00:48,399\npasarle la palabra a Jensen para hablar sobre cmo\n\n16\n00:00:45,280 --> 00:00:50,320\nSe siente esta semana ser el fundador?\n\n17\n00:00:48,399 --> 00:00:51,120\ny director ejecutivo de Nvidia. Jensen, bienvenido a mi\n\n18\n00:00:50,320 --> 00:00:52,640\npodcast.\n\n19\n00:00:51,120 --> 00:00:55,280\n>> Gracias, Elie. es un increible\n\n20\n00:00:52,640 --> 00:00:56,559\nhonor. Guau. Semana loca, verdad?\n\n21\n00:00:55,280 --> 00:00:58,320\n>> Es una semana increble para Nvidia.\n\n22\n00:00:56,559 --> 00:01:00,079\nS, es una semana loca. creo que um\n\n23\n00:00:58,320 --> 00:01:04,000\nBueno, primero que nada, en realidad no lo ha sido.\n\n24\n00:01:00,079 --> 00:01:06,560\nhundido todava. Um pero veamos que\n\n25\n00:01:04,000 --> 00:01:08,159\nMe vienen pensamientos a la mente. Bueno, ya sabes,\n\n26\n00:01:06,560 --> 00:01:11,280\nde muchas maneras, alguien me pregunt\n\n27\n00:01:08,159 --> 00:01:14,640\nayer sobre y y y mencionado a\n\n28\n00:01:11,280 --> 00:01:16,320\nyo uh que soy inmigrante, y en\n\n29\n00:01:14,640 --> 00:01:16,960\nDe hecho, soy un inmigrante. hablando de\n\n30\n00:01:16,320 --> 00:01:18,000\ninmigracin,\n\n31\n00:01:16,960 --> 00:01:19,600\n>> lo mismo aqu.\n\n32\n00:01:18,000 --> 00:01:23,439\n>> Y los dos somos inmigrantes.\n\n33\n00:01:19,600 --> 00:01:26,000\nY uh, Estados Unidos representa\n\n34\n00:01:23,439 --> 00:01:27,680\nLa uh, la tierra del sueo americano.\n\n35\n00:01:26,000 --> 00:01:30,080\nY este es el lugar donde los inmigrantes\n\n36\n00:01:27,680 --> 00:01:32,880\nvenir a construir construir una vida y construir\n\n37\n00:01:30,080 --> 00:01:34,720\nuna empresa. En muchos sentidos, ya sabes,\n\n38\n00:01:32,880 --> 00:01:36,159\nlo que estoy experimentando es probablemente el\n\n39\n00:01:34,720 --> 00:01:37,759\nEl sueo americano definitivo.\n\n40\n00:01:36,159 --> 00:01:39,280\n>> Absolutamente. ya sabes, venir venir a\n\n41\n00:01:37,759 --> 00:01:41,840\nEstados Unidos cuando tena siete aos.\n\n42\n00:01:39,280 --> 00:01:43,439\nviejo, no creo que tenga ocho aos y y\n\n43\n00:01:41,840 --> 00:01:45,439\nuno\n\n44\n00:01:43,439 --> 00:01:48,240\ny tener la oportunidad de fundar\n\n45\n00:01:45,439 --> 00:01:53,119\nuna empresa con buenos amigos 33 aos\n\n46\n00:01:48,240 --> 00:01:56,240\nhace, ser el uh el CEO hoy despus de 33\n\n47\n00:01:53,119 --> 00:01:58,240\naos y um teniendo muchos de los fundadores\n\n48\n00:01:56,240 --> 00:02:00,960\nque estaban ah conmigo en el\n\n49\n00:01:58,240 --> 00:02:02,960\nmuy empezando todava aqu en NVIDIA um\n\n50\n00:02:00,960 --> 00:02:05,280\npersiguiendo un sueo que hemos tenido que\n\n51\n00:02:02,960 --> 00:02:06,640\nque tard tres dcadas en lograrse\n\n52\n00:02:05,280 --> 00:02:09,520\n>> con muchos altibajos. con\n\n53\n00:02:06,640 --> 00:02:12,319\nenorme cantidad de altibajos y t\n\n54\n00:02:09,520 --> 00:02:15,440\nLo s de muchas maneras uh esto\n\n55\n00:02:12,319 --> 00:02:16,959\nhito lo que nos est pasando es\n\n56\n00:02:15,440 --> 00:02:20,000\nun poco difcil de internalizar, es un poco\n\n57\n00:02:16,959 --> 00:02:21,680\ndifcil de asimilar pero tambin representa\n\n58\n00:02:20,000 --> 00:02:24,080\nalgo que es realmente importante para ti\n\n59\n00:02:21,680 --> 00:02:27,120\nS que queramos reinventar el\n\n60\n00:02:24,080 --> 00:02:28,800\ncomputadora la computadora tal como la conocemos uh\n\n61\n00:02:27,120 --> 00:02:30,800\nrealmente ha sido definido en gran medida para\n\n62\n00:02:28,800 --> 00:02:34,959\nAproximadamente seis dcadas desde el sistema IBM.\n\n63\n00:02:30,800 --> 00:02:36,720\n360 IBM como sabes era, eh, el\n\n64\n00:02:34,959 --> 00:02:40,239\nempresa ms grande del mundo de sus\n\n65\n00:02:36,720 --> 00:02:43,040\nel tiempo y el plano que le pusieron\n\n66\n00:02:40,239 --> 00:02:44,480\njuntos para la informtica era bsicamente\n\n67\n00:02:43,040 --> 00:02:47,360\nes el mismo plano que ha sido\n\n68\n00:02:44,480 --> 00:02:49,360\ndesarrollado en las ltimas seis dcadas.\n\n69\n00:02:47,360 --> 00:02:51,599\nTodo, desde la arquitectura del\n\n70\n00:02:49,360 --> 00:02:54,400\nsistemas, la forma en que la separacin entre\n\n71\n00:02:51,599 --> 00:02:57,280\nsoftware y hardware y arquitectura\n\n72\n00:02:54,400 --> 00:03:00,080\ncompatibilidad y ya sabes aplicacin\n\n73\n00:02:57,280 --> 00:03:01,680\ncompatibilidad, alineacin familiar completa, t\n\n74\n00:03:00,080 --> 00:03:04,959\nSabes, todas las cosas que ellos\n\n75\n00:03:01,680 --> 00:03:07,120\ndescrito uh describe en gran medida el\n\n76\n00:03:04,959 --> 00:03:09,360\nindustria informtica hoy y en orden\n\n77\n00:03:07,120 --> 00:03:12,000\ny la oportunidad de reinventar eso y\n\n78\n00:03:09,360 --> 00:03:14,159\nllevarlo al siguiente nivel y ahora ser\n\n79\n00:03:12,000 --> 00:03:16,319\nla plataforma para la inteligencia artificial\n\n80\n00:03:14,159 --> 00:03:17,840\nEs realmente un sueo hecho realidad.\n\n81\n00:03:16,319 --> 00:03:20,800\nS, un momento realmente extraordinario.\n\n82\n00:03:17,840 --> 00:03:23,200\n>> Hablas de las ondas de IA.\n\n83\n00:03:20,800 --> 00:03:24,560\n>> y a mi y a mi nos gusta mucho como divides\n\n84\n00:03:23,200 --> 00:03:24,959\nclasificarlos en diferentes categoras.\n\n85\n00:03:24,560 --> 00:03:27,840\n>> S.\n\n86\n00:03:24,959 --> 00:03:31,360\n>> Puedes describir dnde nos encontramos hoy?\n\n87\n00:03:27,840 --> 00:03:35,280\nTrminos de la ola y cmo llegamos aqu?\n\n88\n00:03:31,360 --> 00:03:37,599\n>> Um 20 2012 uh uh vimos el mismo momento\n\n89\n00:03:35,280 --> 00:03:40,799\ncomo todos los dems. Tuvimos el tuvimos el\n\n90\n00:03:37,599 --> 00:03:42,319\nla pista interior en el sentido de que uh nosotros\n\n91\n00:03:40,799 --> 00:03:44,640\nSiempre cre que CUDA iba a\n\n92\n00:03:42,319 --> 00:03:47,200\npermitir una nueva clase de aplicaciones y\n\n93\n00:03:44,640 --> 00:03:50,799\nsiempre estbamos atentos a ello. Y\n\n94\n00:03:47,200 --> 00:03:53,120\nentonces cuando cuando um Alexet vino um\n\n95\n00:03:50,799 --> 00:03:56,640\nConstruido sobre CUDA, nuestras GPU lo hicieron\n\n96\n00:03:53,120 --> 00:03:59,120\nposible uh para entrenar a AlexNet y para\n\n97\n00:03:56,640 --> 00:04:01,599\nAlex Net para lograr algo tan extraordinario\n\n98\n00:03:59,120 --> 00:04:04,879\nresultados en visin por computadora, lograr el\n\n99\n00:04:01,599 --> 00:04:07,120\nnivel de capacidad que una computadora\n\n100\n00:04:04,879 --> 00:04:09,120\ncientficos especializados en informtica\n\n101\n00:04:07,120 --> 00:04:11,760\nla visin no pudo lograr ms de cuatro\n\n102\n00:04:09,120 --> 00:04:15,200\ndcadas. Ya sabes, para que tres personas\n\n103\n00:04:11,760 --> 00:04:17,919\nhacer algo as. um uh es\n\n104\n00:04:15,200 --> 00:04:20,000\nsimplemente una hazaa extraordinaria. Y entonces nosotros\n\n105\n00:04:17,919 --> 00:04:21,680\naprovech la oportunidad y nos miramos\n\n106\n00:04:20,000 --> 00:04:23,680\nQu es lo que estamos mirando? T\n\n107\n00:04:21,680 --> 00:04:26,240\nSabes qu est pasando aqu? Es esto\n\n108\n00:04:23,680 --> 00:04:29,360\nEs Alec Alex al Alexnet un gran avance?\n\n109\n00:04:26,240 --> 00:04:31,440\nen visin por computadora o es ms grande\n\n110\n00:04:29,360 --> 00:04:34,080\nidea que esa? Y por supuesto, como nosotros\n\n111\n00:04:31,440 --> 00:04:36,080\nYa sabes, la visin por computadora es un pilar de\n\n112\n00:04:34,080 --> 00:04:37,680\ninteligencia artificial. Sabes,\n\n113\n00:04:36,080 --> 00:04:39,280\nsin visin por computadora, sin habla\n\n114\n00:04:37,680 --> 00:04:41,840\ncomprensin del idioma, es difcil\n\n115\n00:04:39,280 --> 00:04:43,520\ntener inteligencia. Y entonces nosotros nosotros\n\n116\n00:04:41,840 --> 00:04:45,440\nMe di cuenta de que esto, por supuesto, era parte\n\n117\n00:04:43,520 --> 00:04:46,880\nde inteligencia artificial. Pero es un\n\n118\n00:04:45,440 --> 00:04:49,840\nUna idea ms grande que esa? Y llegamos a\n\n119\n00:04:46,880 --> 00:04:53,680\nla conclusin de que lo que AlexNet y\n\n120\n00:04:49,840 --> 00:04:56,880\nEl aprendizaje profundo demostr es que ahora es\n\n121\n00:04:53,680 --> 00:05:00,000\nfinalmente posible si tuviramos suficientes datos,\n\n122\n00:04:56,880 --> 00:05:01,600\nsuficiente escala informtica. Y por supuesto nosotros\n\n123\n00:05:00,000 --> 00:05:03,520\ntener estos modelos de aprendizaje profundo que son\n\n124\n00:05:01,600 --> 00:05:07,039\nbastante escalable\n\n125\n00:05:03,520 --> 00:05:10,000\num que podramos ser capaces de aplicar\n\n126\n00:05:07,039 --> 00:05:12,240\ncomputadoras para resolver problemas uh que eran\n\n127\n00:05:10,000 --> 00:05:14,720\nimposible de describir usando humanos\n\n128\n00:05:12,240 --> 00:05:17,280\ncaracterstica diseada y utilizando principios\n\n129\n00:05:14,720 --> 00:05:19,360\nalgoritmos. Y entonces nosotros, um, tenemos\n\n130\n00:05:17,280 --> 00:05:21,039\nemocionado desde esa perspectiva. Nosotros tambin\n\n131\n00:05:19,360 --> 00:05:23,840\nMe emocion porque\n\n132\n00:05:21,039 --> 00:05:26,400\n>> porque cuando razonas\n\n133\n00:05:23,840 --> 00:05:29,280\num aprendizaje profundo y la formacin de\n\n134\n00:05:26,400 --> 00:05:31,199\nAlexNet y hacia dnde podra llegar, nosotros\n\n135\n00:05:29,280 --> 00:05:32,400\nse dio cuenta de que toda la informtica\n\n136\n00:05:31,199 --> 00:05:34,160\nla plataforma va a cambiar,\n\n137\n00:05:32,400 --> 00:05:35,680\n>> verdad? los procesadores van a cambiar,\n\n138\n00:05:34,160 --> 00:05:37,600\nla conexin a internet va a cambiar,\n\n139\n00:05:35,680 --> 00:05:39,440\nlas redes van a cambiar, el\n\n140\n00:05:37,600 --> 00:05:41,759\npila de software encima de l, cmo\n\n141\n00:05:39,440 --> 00:05:44,080\ndesarrollar el software, la metodologa de\n\n142\n00:05:41,759 --> 00:05:46,080\nsoftware dentro de las empresas y por supuesto\n\n143\n00:05:44,080 --> 00:05:48,000\nlas muchas industrias en las que podramos estar\n\n144\n00:05:46,080 --> 00:05:50,479\ncapaz de crear iba a completamente\n\n145\n00:05:48,000 --> 00:05:53,440\ncambiar y entonces nos pusimos a hacerlo.\n\n146\n00:05:50,479 --> 00:05:54,800\nBsicamente, reiniciamos nuestra empresa. Ahora\n\n147\n00:05:53,440 --> 00:05:58,240\nLas olas de las que estabas hablando.\n\n148\n00:05:54,800 --> 00:06:00,639\nsobre despus de que hicimos eso, um, dedicamos\n\n149\n00:05:58,240 --> 00:06:02,639\nNosotros mismos para crear nuevas bibliotecas.\n\n150\n00:06:00,639 --> 00:06:05,759\nllamado KDNN\n\n151\n00:06:02,639 --> 00:06:09,520\nuh creando creando uh marcos de IA uh\n\n152\n00:06:05,759 --> 00:06:12,639\nllamado Mega Core um Megatron core uh para\n\n153\n00:06:09,520 --> 00:06:15,680\nuh inventando MVLink y ncleos tensoriales y\n\n154\n00:06:12,639 --> 00:06:17,840\nlos diferentes formatos numricos y uh\n\n155\n00:06:15,680 --> 00:06:21,199\nllevamos a la creacin de un sistema que\n\n156\n00:06:17,840 --> 00:06:23,039\nLlame a DGX1 nuestra primera supercomputadora de IA. I\n\n157\n00:06:21,199 --> 00:06:24,400\nLo entregu personalmente a una startup en\n\n158\n00:06:23,039 --> 00:06:27,919\nSan Francisco que result tener\n\n159\n00:06:24,400 --> 00:06:30,560\nHa sido IA abierta y de todos modos eso\n\n160\n00:06:27,919 --> 00:06:33,199\nEl viaje realmente podra capturarse en\n\n161\n00:06:30,560 --> 00:06:36,160\nvarias maneras. Desde 2012, la primera\n\n162\n00:06:33,199 --> 00:06:38,160\nLo que sucedi fue que la IA despeg. Oh,\n\n163\n00:06:36,160 --> 00:06:39,840\nEl aprendizaje profundo sigui avanzando. La cantidad\n\n164\n00:06:38,160 --> 00:06:42,479\nde datos que tenamos, la cantidad de clculo que\n\n165\n00:06:39,840 --> 00:06:44,240\nhaba seguido creciendo, seguido escalando. y eso\n\n166\n00:06:42,479 --> 00:06:47,680\ncondujo a la primera ola, que es realmente\n\n167\n00:06:44,240 --> 00:06:49,440\ndescrito como, si pudiera, percepcin. Nosotros\n\n168\n00:06:47,680 --> 00:06:50,479\nresolvimos la percepcin. se convirti\n\n169\n00:06:49,440 --> 00:06:52,240\nsobrehumano.\n\n170\n00:06:50,479 --> 00:06:54,319\n>> La visin por computadora se volvi sobrehumana.\n\n171\n00:06:52,240 --> 00:06:56,800\nComprensin del lenguaje o habla.\n\n172\n00:06:54,319 --> 00:06:58,960\nEl reconocimiento se volvi sobrehumano. El\n\n173\n00:06:56,800 --> 00:07:00,560\nsegundo la segunda fase es uh\n\n174\n00:06:58,960 --> 00:07:03,919\ngenerativo.\n\n175\n00:07:00,560 --> 00:07:06,560\nuh ahora no slo podemos entender\n\n176\n00:07:03,919 --> 00:07:09,199\ninformacin pero podemos traducir y\n\n177\n00:07:06,560 --> 00:07:11,120\ngenerar informacin. As que texto a texto,\n\n178\n00:07:09,199 --> 00:07:13,680\ntexto a imgenes,\n\n179\n00:07:11,120 --> 00:07:15,840\n>> imgenes a texto, texto a video.\n\n180\n00:07:13,680 --> 00:07:18,400\n>> Y si pudieras convertir texto a video,\n\n181\n00:07:15,840 --> 00:07:20,319\nSabes qu ms no puedes hacer? Entonces\n\n182\n00:07:18,400 --> 00:07:22,880\nLa segunda fase fue la IA generativa. Este\n\n183\n00:07:20,319 --> 00:07:25,120\ntercera tercera ola es la ola que estamos\n\n184\n00:07:22,880 --> 00:07:28,319\nen hoy, ya sabes, muy profundamente\n\n185\n00:07:25,120 --> 00:07:28,880\nslidamente en lo que est razonando la IA.\n\n186\n00:07:28,319 --> 00:07:32,080\nMmmm.\n\n187\n00:07:28,880 --> 00:07:36,240\n>> Aqu es donde podra aplicarse una IA\n\n188\n00:07:32,080 --> 00:07:39,599\nprincipios y conocimientos, tal vez algunos\n\n189\n00:07:36,240 --> 00:07:43,919\nsentido comn y utilizar tcnicas como\n\n190\n00:07:39,599 --> 00:07:47,039\ncadena de pensamiento, rboles de pensamiento um a\n\n191\n00:07:43,919 --> 00:07:49,759\nuh, divide el problema en mltiples\n\n192\n00:07:47,039 --> 00:07:52,319\npasos, razonar sobre cmo resolver el\n\n193\n00:07:49,759 --> 00:07:54,479\nproblema, el objetivo ms amplio paso a paso.\n\n194\n00:07:52,319 --> 00:07:57,360\nIncluso podra investigar un poco, leer\n\n195\n00:07:54,479 --> 00:07:59,199\nalgunos documentos, leer un documento de archivo\n\n196\n00:07:57,360 --> 00:08:00,879\nantes de que responda la pregunta.\n\n197\n00:07:59,199 --> 00:08:05,759\n>> Y entonces la tercera ola en la que estamos\n\n198\n00:08:00,879 --> 00:08:08,479\nhoy, que es razonamiento, es un acontecimiento muy grande\n\n199\n00:08:05,759 --> 00:08:10,560\nparte de ver la aceleracin de la IA\n\n200\n00:08:08,479 --> 00:08:12,400\nconvirtindose en IA. Ese es el hecho de que\n\n201\n00:08:10,560 --> 00:08:15,039\nestamos estamos haciendo razonamientos que la IA es el\n\n202\n00:08:12,400 --> 00:08:17,520\nrazn por la cual um uh la gente est empezando\n\n203\n00:08:15,039 --> 00:08:18,240\npara decir, ya sabes, estamos cerca del general\n\n204\n00:08:17,520 --> 00:08:20,879\ninteligencia,\n\n205\n00:08:18,240 --> 00:08:24,160\n>> verdad? Y luego el tercero el siguiente\n\n206\n00:08:20,879 --> 00:08:26,960\nola despus despus uh uh despus del razonamiento\n\n207\n00:08:24,160 --> 00:08:28,479\nLa IA es IA fsica. Aqu es donde la IA\n\n208\n00:08:26,960 --> 00:08:30,800\nahora sabe cmo interactuar con el\n\n209\n00:08:28,479 --> 00:08:34,479\nmundo fsico. tiene mundo fisico\n\n210\n00:08:30,800 --> 00:08:36,959\nsentido comn como permanencia del objeto,\n\n211\n00:08:34,479 --> 00:08:39,200\nfriccin, inercia,\n\n212\n00:08:36,959 --> 00:08:41,360\ncausa y efecto, y ya sabes, todos\n\n213\n00:08:39,200 --> 00:08:43,599\neste tipo de tipos de sentido comn\n\n214\n00:08:41,360 --> 00:08:44,399\nque los nios tienen, ya sabes, cachorros\n\n215\n00:08:43,599 --> 00:08:46,080\ntener.\n\n216\n00:08:44,399 --> 00:08:47,839\n>> Ya sabes, ahora vamos a la IA.\n\n217\n00:08:46,080 --> 00:08:49,760\ntener esas cosas. Y como resultado de\n\n218\n00:08:47,839 --> 00:08:52,000\neso, la coleccin de todos estos\n\n219\n00:08:49,760 --> 00:08:53,519\ncapacidades, deberamos poder ver,\n\n220\n00:08:52,000 --> 00:08:54,640\nya sabes, la prxima ola, que es\n\n221\n00:08:53,519 --> 00:08:55,519\nprobablemente robtica,\n\n222\n00:08:54,640 --> 00:08:58,880\n>> verdad? S.\n\n223\n00:08:55,519 --> 00:09:00,800\n>> Um, ya sabes, has construido lo digital\n\n224\n00:08:58,880 --> 00:09:04,720\ninfraestructura para la forma en que vivimos\n\n225\n00:09:00,800 --> 00:09:07,360\nhoy. Uh, t tambin ests trabajando y, um,\n\n226\n00:09:04,720 --> 00:09:09,440\ntener una visin para las fbricas de IA. Poder\n\n227\n00:09:07,360 --> 00:09:11,519\ndesempacas qu significa eso? S.\n\n228\n00:09:09,440 --> 00:09:14,399\n>> Cmo transforma eso los datos actuales?\n\n229\n00:09:11,519 --> 00:09:17,680\ncentro para el futuro? el semiconductor\n\n230\n00:09:14,399 --> 00:09:19,200\nindustria, TSMC, uh, la computadora\n\n231\n00:09:17,680 --> 00:09:20,959\necosistema\n\n232\n00:09:19,200 --> 00:09:24,160\nuh eso entonces crea estos\n\n233\n00:09:20,959 --> 00:09:27,120\ncomputadoras, Nvidia hoy, representamos,\n\n234\n00:09:24,160 --> 00:09:29,279\npor as decirlo, el ecosistema digital, el\n\n235\n00:09:27,120 --> 00:09:31,200\ninfraestructura digital del mundo, la\n\n236\n00:09:29,279 --> 00:09:32,480\ninfraestructura informtica.\n\n237\n00:09:31,200 --> 00:09:34,560\n>> Y adems de esa informtica\n\n238\n00:09:32,480 --> 00:09:37,279\nLa infraestructura se dio cuenta de esto.\n\n239\n00:09:34,560 --> 00:09:40,880\nllamada inteligencia artificial.\n\n240\n00:09:37,279 --> 00:09:43,760\nY qu tiene de interesante acerca de um?\n\n241\n00:09:40,880 --> 00:09:46,720\nsobre la uh, la ltima industria es esa\n\n242\n00:09:43,760 --> 00:09:49,200\nnosotros la infraestructura digital\n\n243\n00:09:46,720 --> 00:09:51,839\nla industria informtica uh habilitada\n\n244\n00:09:49,200 --> 00:09:54,720\nsoftware y eso representa ya sabes\n\n245\n00:09:51,839 --> 00:09:56,959\nalrededor de un billn de dlares de industria.\n\n246\n00:09:54,720 --> 00:09:59,440\nT, t, usas la computadora.\n\n247\n00:09:56,959 --> 00:10:01,440\ninfraestructuras para escribir el software\n\n248\n00:09:59,440 --> 00:10:04,800\npero luego implementas el software en\n\n249\n00:10:01,440 --> 00:10:07,920\ncosas como usted sabe telfonos, telfonos inteligentes\n\n250\n00:10:04,800 --> 00:10:11,040\ny por eso la industria del software no era\n\n251\n00:10:07,920 --> 00:10:12,880\nmuy grande sabes que es una llamada ya sabes\n\n252\n00:10:11,040 --> 00:10:15,200\nEs medio billn de dlares, verdad?\n\n253\n00:10:12,880 --> 00:10:17,760\n>> y la industria del hardware no es muy grande\n\n254\n00:10:15,200 --> 00:10:19,920\nllmalo medio billn de dlares y todo\n\n255\n00:10:17,760 --> 00:10:22,640\nDe repente esta industria la computadora.\n\n256\n00:10:19,920 --> 00:10:24,880\ninteligencia artificial habilitada por la industria\n\n257\n00:10:22,640 --> 00:10:26,959\ny lo realmente interesante es\n\n258\n00:10:24,880 --> 00:10:28,800\nLa inteligencia artificial es a la vez esto.\n\n259\n00:10:26,959 --> 00:10:30,880\ntecnologa revolucionaria que acabamos de\n\n260\n00:10:28,800 --> 00:10:33,279\nhabl de\n\n261\n00:10:30,880 --> 00:10:35,279\ny es por su percepcin y\n\n262\n00:10:33,279 --> 00:10:37,279\ncapacidad de razonamiento. Puedes usarlo para\n\n263\n00:10:35,279 --> 00:10:39,120\nresolver problemas en casi todos\n\n264\n00:10:37,279 --> 00:10:40,959\nuna sola industria porque cada industria\n\n265\n00:10:39,120 --> 00:10:43,600\nque sabemos en la pastilla en el\n\n266\n00:10:40,959 --> 00:10:48,000\nSu fundamento es la inteligencia y ahora\n\n267\n00:10:43,600 --> 00:10:50,079\nPodemos crear inteligencia en\n\n268\n00:10:48,000 --> 00:10:52,399\nescalas increbles y por supuesto eso es\n\n269\n00:10:50,079 --> 00:10:54,079\nrevolucionar todas las industrias.\n\n270\n00:10:52,399 --> 00:10:56,079\nUno\n\n271\n00:10:54,079 --> 00:10:58,640\nesa es la perspectiva tecnolgica. Qu\n\n272\n00:10:56,079 --> 00:11:01,279\nsobre la perspectiva industrial? En\n\n273\n00:10:58,640 --> 00:11:02,640\nPara producir estas IA, qu es?\n\n274\n00:11:01,279 --> 00:11:05,279\nrealmente saliendo de esto? que viene\n\n275\n00:11:02,640 --> 00:11:08,000\nDe estos modelos hay tokens y estos.\n\n276\n00:11:05,279 --> 00:11:11,279\nLas fichas se formulan en palabras y\n\n277\n00:11:08,000 --> 00:11:13,920\nnmeros y smbolos y podran estar en el\n\n278\n00:11:11,279 --> 00:11:18,480\nFuturos productos qumicos y protenas para frmacos.\n\n279\n00:11:13,920 --> 00:11:22,399\ndescubrimiento. Podran ser movimientos del actuador.\n\n280\n00:11:18,480 --> 00:11:26,399\npara uh para um conducir un auto sin conductor uh\n\n281\n00:11:22,399 --> 00:11:29,519\no animar un robot. Y as estos\n\n282\n00:11:26,399 --> 00:11:31,760\nestas fichas que estan saliendo son\n\n283\n00:11:29,519 --> 00:11:34,480\nreformulado\n\n284\n00:11:31,760 --> 00:11:36,720\nreconstituido en inteligencia de\n\n285\n00:11:34,480 --> 00:11:38,640\ndiferentes tipos.\n\n286\n00:11:36,720 --> 00:11:41,040\nPero, qu se necesita para generar estos\n\n287\n00:11:38,640 --> 00:11:42,480\ntokens a la escala que necesitamos\n\n288\n00:11:41,040 --> 00:11:46,160\napoyar a todas estas industrias y\n\n289\n00:11:42,480 --> 00:11:48,640\ntodos los que usan IA son estos grandes datos\n\n290\n00:11:46,160 --> 00:11:50,079\ncentros y yo y yo dejamos de llamarlo un\n\n291\n00:11:48,640 --> 00:11:51,360\ncentro de datos porque en realidad no es un\n\n292\n00:11:50,079 --> 00:11:53,040\ncentro de datos. no se trata de no se trata\n\n293\n00:11:51,360 --> 00:11:55,760\nsobre los centros de datos clsicos son\n\n294\n00:11:53,040 --> 00:11:57,839\nrecuperando datos. Es un nuevo tipo de datos.\n\n295\n00:11:55,760 --> 00:12:00,000\ncentro y su trabajo es singular para\n\n296\n00:11:57,839 --> 00:12:02,240\nproducir tokens y por eso lo llamo\n\n297\n00:12:00,000 --> 00:12:05,120\nuna fbrica de IA. S.\n\n298\n00:12:02,240 --> 00:12:07,120\n>> Y tambin usaste token por metro uh uh\n\n299\n00:12:05,120 --> 00:12:08,240\nsobre cmo sern estas fbricas de IA\n\n300\n00:12:07,120 --> 00:12:09,920\nutilizado en el futuro.\n\n301\n00:12:08,240 --> 00:12:12,800\n>> Exactamente. Y qu es lo que realmente\n\n302\n00:12:09,920 --> 00:12:16,880\nLo interesante es que es el ltimo.\n\n303\n00:12:12,800 --> 00:12:19,200\nindustria de fbricas que crean electrones\n\n304\n00:12:16,880 --> 00:12:19,519\nfue la industria de generacin de energa,\n\n305\n00:12:19,200 --> 00:12:21,839\n>> verdad?\n\n306\n00:12:19,519 --> 00:12:24,560\n>> Representaba el 30% de la poblacin mundial\n\n307\n00:12:21,839 --> 00:12:29,519\neconoma de uno en uno. qu es\n\n308\n00:12:24,560 --> 00:12:32,880\nproducido a partir de l se monetiza en\n\n309\n00:12:29,519 --> 00:12:35,040\nya sabes kilovatios hora por dlar. Ahora\n\n310\n00:12:32,880 --> 00:12:35,760\ntenemos estas ideas llamadas mega tokens\n\n311\n00:12:35,040 --> 00:12:36,240\npor dolar\n\n312\n00:12:35,760 --> 00:12:39,120\n>> correcto\n\n313\n00:12:36,240 --> 00:12:41,680\n>> y sale como electrones otra vez tu\n\n314\n00:12:39,120 --> 00:12:44,959\nsaber y as en lugar de electrones puros\n\n315\n00:12:41,680 --> 00:12:46,880\nahora son electrones de valor agregado y nosotros\n\n316\n00:12:44,959 --> 00:12:48,399\nLlmalos tokens y vamos.\n\n317\n00:12:46,880 --> 00:12:51,040\npara crear esencialmente un nuevo\n\n318\n00:12:48,399 --> 00:12:52,399\nLa industria y esta industria necesita energa.\n\n319\n00:12:51,040 --> 00:12:54,720\nCul es la razn por la que el Presidente\n\n320\n00:12:52,399 --> 00:12:57,839\nLos pro-energa y energa de Trump\n\n321\n00:12:54,720 --> 00:13:00,800\nLa iniciativa de crecimiento es muy oportuna.\n\n322\n00:12:57,839 --> 00:13:03,200\nporque en el momento exacto en que Amrica\n\n323\n00:13:00,800 --> 00:13:07,920\nquiere ser excelente en AI I y quiere\n\n324\n00:13:03,200 --> 00:13:09,920\nSer un lder mundial en el ecosistema de IA.\n\n325\n00:13:07,920 --> 00:13:11,600\nSin la energa que es necesaria para\n\n326\n00:13:09,920 --> 00:13:13,760\ncrear estas fbricas de IA, no lo haramos\n\n327\n00:13:11,600 --> 00:13:16,480\npoder hacer eso. Entonces el el\n\n328\n00:13:13,760 --> 00:13:19,200\nconfluencia de la visin del presidente Trump\n\n329\n00:13:16,480 --> 00:13:22,240\ny su su su\n\n330\n00:13:19,200 --> 00:13:25,120\nimpulso para permitir el crecimiento energtico en nuestra\n\n331\n00:13:22,240 --> 00:13:27,600\nnacin y la confluencia de los\n\n332\n00:13:25,120 --> 00:13:30,480\npreparacin de la tecnologa y\n\n333\n00:13:27,600 --> 00:13:32,320\npreparacin del mundo para la IA, todo\n\n334\n00:13:30,480 --> 00:13:34,399\nsucedi exactamente en el momento adecuado, usted\n\n335\n00:13:32,320 --> 00:13:36,000\nlo s, y as esto va a ser\n\n336\n00:13:34,399 --> 00:13:36,560\nva a permitir una nueva industria en\n\n337\n00:13:36,000 --> 00:13:39,680\nfrente a nosotros.\n\n338\n00:13:36,560 --> 00:13:41,839\n>> Um solo una pregunta sobre esa habilitacin de\n\n339\n00:13:39,680 --> 00:13:44,079\nnuevas industrias. Mucha gente esta\n\n340\n00:13:41,839 --> 00:13:46,079\nnervioso por el trabajo. S. Cmo es esto?\n\n341\n00:13:44,079 --> 00:13:48,160\nVa a impactar a la fuerza laboral? Esto es\n\n342\n00:13:46,079 --> 00:13:51,200\nNo es la primera vez que conoces a\n\n343\n00:13:48,160 --> 00:13:53,519\ntecnologa transformadora masiva que usted\n\n344\n00:13:51,200 --> 00:13:56,320\nsabemos llega a nuestras vidas y crea\n\n345\n00:13:53,519 --> 00:13:59,040\nnuevo trabajo pero tambin sabes que hace\n\n346\n00:13:56,320 --> 00:14:01,440\nimpacto en las personas que pierden sus empleos. Cmo\n\n347\n00:13:59,040 --> 00:14:06,680\nVes esta transformacin sucediendo?\n\n348\n00:14:01,440 --> 00:14:06,680\nen la fuerza laboral? Nuevas tecnologas\n\n349\n00:14:06,720 --> 00:14:11,920\ny productividad\n\n350\n00:14:09,519 --> 00:14:13,839\nimpulsa el crecimiento de las industrias y\n\n351\n00:14:11,920 --> 00:14:17,279\ncrea empleos.\n\n352\n00:14:13,839 --> 00:14:19,680\nEn el caso de la electricidad,\n\n353\n00:14:17,279 --> 00:14:21,760\nes una nueva tecnologa\n\n354\n00:14:19,680 --> 00:14:23,600\ny como mencion antes, la energa\n\n355\n00:14:21,760 --> 00:14:26,480\nproduccin\n\n356\n00:14:23,600 --> 00:14:29,760\nLa industria represent en algn momento el 30% de\n\n357\n00:14:26,480 --> 00:14:32,240\nla economa mundial. No slo el\n\n358\n00:14:29,760 --> 00:14:33,920\nLa produccin de energa era una gran industria.\n\n359\n00:14:32,240 --> 00:14:35,519\nen s mismo creando estos poderes\n\n360\n00:14:33,920 --> 00:14:38,399\nPlantas de generacin de todo tipo.\n\n361\n00:14:35,519 --> 00:14:41,440\nen todo el mundo, tambin permiti\n\n362\n00:14:38,399 --> 00:14:45,279\nnuevas aplicaciones. Electricidad habilitada\n\n363\n00:14:41,440 --> 00:14:48,320\nbombillas, lavavajillas, frigorficos,\n\n364\n00:14:45,279 --> 00:14:50,480\nbien? Lavadoras, todo tipo de\n\n365\n00:14:48,320 --> 00:14:52,240\nSe crearon nuevas aplicaciones y todo\n\n366\n00:14:50,480 --> 00:14:54,399\nesas aplicaciones crearon una nueva\n\n367\n00:14:52,240 --> 00:14:57,199\nindustria, cre empleos.\n\n368\n00:14:54,399 --> 00:14:59,279\nAhora la ltima revolucin industrial que\n\n369\n00:14:57,199 --> 00:15:03,040\nEstados Unidos, sabes?, justo en el\n\n370\n00:14:59,279 --> 00:15:06,079\nen medio estaba la industria de la informacin\n\n371\n00:15:03,040 --> 00:15:08,639\nrevolucin y esa revolucin digital\n\n372\n00:15:06,079 --> 00:15:12,000\npermiti que la productividad creciera en los ltimos\n\n373\n00:15:08,639 --> 00:15:14,560\naproximadamente tres dcadas la productividad ha\n\n374\n00:15:12,000 --> 00:15:17,360\ncrecido alrededor del 80%.\n\n375\n00:15:14,560 --> 00:15:20,160\nAl mismo tiempo, el empleo aument un 80%.\n\n376\n00:15:17,360 --> 00:15:23,360\nY lo mismo ocurre cuando la productividad aumenta\n\n377\n00:15:20,160 --> 00:15:25,120\nel empleo aumenta. Ahora por qu es eso?\n\n378\n00:15:23,360 --> 00:15:27,760\nDe hecho, se podra decir que cuando\n\n379\n00:15:25,120 --> 00:15:30,079\nla productividad aumenta, el empleo\n\n380\n00:15:27,760 --> 00:15:33,199\nbaja porque podras hacer ms con\n\n381\n00:15:30,079 --> 00:15:36,000\nmenos gente, verdad? Pero eso es porque\n\n382\n00:15:33,199 --> 00:15:37,839\neso le falta imaginacin.\n\n383\n00:15:36,000 --> 00:15:40,160\nSi es su empresa, tomemoslo de\n\n384\n00:15:37,839 --> 00:15:43,120\nla perspectiva de una empresa. si una empresa\n\n385\n00:15:40,160 --> 00:15:46,399\nno tiene nuevas ideas y es literalmente\n\n386\n00:15:43,120 --> 00:15:48,480\nhaciendo una cosa y slo una cosa, cuando\n\n387\n00:15:46,399 --> 00:15:50,720\nnuestra productividad aumenta, necesitamos menos\n\n388\n00:15:48,480 --> 00:15:53,120\ngente para hacerlo. Pero si miras\n\n389\n00:15:50,720 --> 00:15:54,800\nNvidia, tenemos tantas ideas. somos nosotros\n\n390\n00:15:53,120 --> 00:15:58,880\nno tengo suficiente tiempo ni gente para ir\n\n391\n00:15:54,800 --> 00:16:01,519\nhazlo. La acumulacin de grandes ideas que\n\n392\n00:15:58,880 --> 00:16:03,519\nnos encantara ir a intentarlo\n\n393\n00:16:01,519 --> 00:16:05,839\nNuevos mercados y nuevas aplicaciones que nos gustan.\n\n394\n00:16:03,519 --> 00:16:07,360\npara ir a crear el trabajo pendiente de eso es\n\n395\n00:16:05,839 --> 00:16:09,839\nincreble.\n\n396\n00:16:07,360 --> 00:16:12,160\nAhora, si tuviera ms gente, ms\n\n397\n00:16:09,839 --> 00:16:13,680\ntiempo, ya sabes, considerando lo rpido que\n\n398\n00:16:12,160 --> 00:16:15,360\nque duro ya estamos trabajando, estoy\n\n399\n00:16:13,680 --> 00:16:17,040\nya funcionando. Si tuviera ms\n\n400\n00:16:15,360 --> 00:16:19,680\ngente, si tuviera ms tiempo, si fuera\n\n401\n00:16:17,040 --> 00:16:21,040\nMs productivos, hacemos ms. nuestra empresa\n\n402\n00:16:19,680 --> 00:16:23,120\npodra ofrecer ms cosas,\n\n403\n00:16:21,040 --> 00:16:25,600\nsera capaz de inventar nuevas ideas\n\n404\n00:16:23,120 --> 00:16:27,839\nque cre nuevas industrias. Y as el\n\n405\n00:16:25,600 --> 00:16:31,519\nel verdadero el verdadero tema es que eres t\n\n406\n00:16:27,839 --> 00:16:34,320\nEres una persona esperanzada?\n\n407\n00:16:31,519 --> 00:16:37,839\npersona optimista y crees en\n\n408\n00:16:34,320 --> 00:16:39,680\ncreacin de ideas o eres alguien que\n\n409\n00:16:37,839 --> 00:16:41,199\ncree que sabes que no hay nuevas ideas\n\n410\n00:16:39,680 --> 00:16:44,000\nizquierda\n\n411\n00:16:41,199 --> 00:16:46,320\ny francamente solo estamos trabajando y\n\n412\n00:16:44,000 --> 00:16:48,959\nsi pudiramos hacer ese trabajo ms\n\n413\n00:16:46,320 --> 00:16:50,959\nproductivamente nos quedaremos sin trabajo\n\n414\n00:16:48,959 --> 00:16:53,759\nNo creo que haya mucho trabajo por hacer.\n\n415\n00:16:50,959 --> 00:16:55,600\nHay tantas ideas para seguir?\n\n416\n00:16:53,759 --> 00:16:58,320\neso si si puedo hacer trabajo mas\n\n417\n00:16:55,600 --> 00:17:02,160\nefectivamente, simplemente hara ms. Y\n\n418\n00:16:58,320 --> 00:17:06,640\nentonces creo que eso eso eso\n\n419\n00:17:02,160 --> 00:17:07,600\nLa visin optimista no es ingenua. Es\n\n420\n00:17:06,640 --> 00:17:08,240\nen realidad la historia,\n\n421\n00:17:07,600 --> 00:17:10,720\n>> verdad?\n\n422\n00:17:08,240 --> 00:17:13,280\n>> La historia sugerira que la humanidad tiene\n\n423\n00:17:10,720 --> 00:17:15,839\nmuchas ms ideas para seguir. tenemos un\n\n424\n00:17:13,280 --> 00:17:17,839\nmuchos ms desafos que abordar. si nosotros\n\n425\n00:17:15,839 --> 00:17:21,199\ndeberamos tener ms productividad, nosotros\n\n426\n00:17:17,839 --> 00:17:23,360\nPuedes llegar a l mucho ms rpido. Bueno, nosotros\n\n427\n00:17:21,199 --> 00:17:24,959\nuna cosa si pudiera solo una cosa\n\n428\n00:17:23,360 --> 00:17:27,520\nque me gustaria agregar\n\n429\n00:17:24,959 --> 00:17:29,360\n>> y esto es esto es um esto es algo\n\n430\n00:17:27,520 --> 00:17:30,799\neso de lo que tu y yo hemos hablado\n\n431\n00:17:29,360 --> 00:17:36,000\nantes.\n\n432\n00:17:30,799 --> 00:17:37,840\n>> Um, es vital que todos involucren la IA\n\n433\n00:17:36,000 --> 00:17:40,000\nde inmediato.\n\n434\n00:17:37,840 --> 00:17:42,080\nCada adulto,\n\n435\n00:17:40,000 --> 00:17:44,880\ncada persona que trabaja, no trabaja\n\n436\n00:17:42,080 --> 00:17:46,559\npersona, todo nio debe abordar y\n\n437\n00:17:44,880 --> 00:17:49,120\ninvolucra a la IA de inmediato. Y la razn de\n\n438\n00:17:46,559 --> 00:17:51,360\neso es porque la IA es la mejor\n\n439\n00:17:49,120 --> 00:17:52,160\nfuerza igualadora de ecualizacin.\n\n440\n00:17:51,360 --> 00:17:54,960\n>> Ese es un buen punto.\n\n441\n00:17:52,160 --> 00:17:58,080\n>> Es la primera vez en la historia que un\n\n442\n00:17:54,960 --> 00:18:01,840\nTecnologa tan increble como artificial.\n\n443\n00:17:58,080 --> 00:18:04,240\nLa inteligencia es til para alguien que\n\n444\n00:18:01,840 --> 00:18:07,600\nsabe programar software, ya sea\n\n445\n00:18:04,240 --> 00:18:08,960\nprogramas en C++ o Python o no tienes\n\n446\n00:18:07,600 --> 00:18:11,039\nidea de cmo usar una computadora.\n\n447\n00:18:08,960 --> 00:18:12,880\n>> Verdad? Esta es la primera vez en\n\n448\n00:18:11,039 --> 00:18:14,720\nhistoria que de repente eso\n\n449\n00:18:12,880 --> 00:18:16,880\nLa computadora es fcil de usar. Si no lo haces\n\n450\n00:18:14,720 --> 00:18:19,360\nsaber cmo usar la IA, simplemente abre el\n\n451\n00:18:16,880 --> 00:18:20,160\nsitio web, vaya a Chad GPT, vaya a Gemini\n\n452\n00:18:19,360 --> 00:18:20,960\nPro, solo di\n\n453\n00:18:20,160 --> 00:18:21,760\n>> haz una pregunta sencilla.\n\n454\n00:18:20,960 --> 00:18:24,000\n>> S. S.\n\n455\n00:18:21,760 --> 00:18:26,080\n>> E incluso podras decir: \"No tengo idea\n\n456\n00:18:24,000 --> 00:18:26,640\ncmo utilizar la IA. Puedes ensearme cmo\n\n457\n00:18:26,080 --> 00:18:27,520\nquin ms?\"\n\n458\n00:18:26,640 --> 00:18:29,440\n>> Eso es cierto.\n\n459\n00:18:27,520 --> 00:18:31,440\n>> Y si no sabes escribir, presiona\n\n460\n00:18:29,440 --> 00:18:32,160\nel botn del micrfono y habla con nosotros.\n\n461\n00:18:31,440 --> 00:18:34,080\n>> Gordon. S.\n\n462\n00:18:32,160 --> 00:18:37,200\n>> Y si no entiendes ingls,\n\n463\n00:18:34,080 --> 00:18:39,120\nPuedes hablar el idioma que quieras.\n\n464\n00:18:37,200 --> 00:18:42,160\n>> Es algo extraordinario.\n\n465\n00:18:39,120 --> 00:18:44,160\n>> Es algo extraordinario. y yo tambin\n\n466\n00:18:42,160 --> 00:18:45,440\nCreo que es increble que si la IA\n\n467\n00:18:44,160 --> 00:18:48,720\nno sabe ese idioma, le dices al\n\n468\n00:18:45,440 --> 00:18:50,640\nAI voy a aprender ese idioma, verdad?\n\n469\n00:18:48,720 --> 00:18:52,640\nY as creo que creo que todos\n\n470\n00:18:50,640 --> 00:18:55,039\nnecesita involucrar a la IA. es el\n\n471\n00:18:52,640 --> 00:18:57,280\nmayor ecualizacin\n\n472\n00:18:55,039 --> 00:18:59,600\num uh fuerza de ecualizacin que tenemos\n\n473\n00:18:57,280 --> 00:19:01,039\njams conocido y va a potenciar\n\n474\n00:18:59,600 --> 00:19:02,720\nva a permitir va a levantar\n\n475\n00:19:01,039 --> 00:19:04,640\nsociedad de todos los que conoces en todas partes.\n\n476\n00:19:02,720 --> 00:19:06,480\n>> Estoy de acuerdo contigo. Cambiando ahora a\n\n477\n00:19:04,640 --> 00:19:08,000\nWashington. Estamos, ests en Washington.\n\n478\n00:19:06,480 --> 00:19:09,280\nComo dije, pasaste una semana increble.\n\n479\n00:19:08,000 --> 00:19:09,840\nTambin te reuniste con el presidente.\n\n480\n00:19:09,280 --> 00:19:11,039\nayer.\n\n481\n00:19:09,840 --> 00:19:13,760\n>> S. primera pregunta.\n\n482\n00:19:11,039 --> 00:19:16,080\n>> Entonces, es increble. Pro-innovacin,\n\n483\n00:19:13,760 --> 00:19:17,919\ncrecimiento profesional,\n\n484\n00:19:16,080 --> 00:19:19,520\nproenerga,\n\n485\n00:19:17,919 --> 00:19:25,039\npro\n\n486\n00:19:19,520 --> 00:19:28,559\nuh, la industria quiere que tomemos la IA por el\n\n487\n00:19:25,039 --> 00:19:31,600\ncuernos y ser el lder mundial, contina\n\n488\n00:19:28,559 --> 00:19:33,440\nser el lder mundial. Uh, tan orgulloso\n\n489\n00:19:31,600 --> 00:19:35,520\nde nuestro pas, muy orgullosos de nuestra\n\n490\n00:19:33,440 --> 00:19:38,080\nempresas, muy orgullosos de nuestra gente. Oh\n\n491\n00:19:35,520 --> 00:19:39,760\nsimplemente siempre es s. yo cada vez que\n\n492\n00:19:38,080 --> 00:19:41,120\nconocerlo, cada vez que estoy con el, yo\n\n493\n00:19:39,760 --> 00:19:41,520\nVuelve, ya sabes, completamente despedido.\n\n494\n00:19:41,120 --> 00:19:44,960\narriba.\n\n495\n00:19:41,520 --> 00:19:47,679\n>> Uh, entonces tengo tres preguntas para ti.\n\n496\n00:19:44,960 --> 00:19:49,919\ntres pblicos diferentes. El numero uno es\n\n497\n00:19:47,679 --> 00:19:51,440\n>> obviamente conociste al presidente\n\n498\n00:19:49,919 --> 00:19:54,480\n>> y estos son memorandos para el presidente\n\n499\n00:19:51,440 --> 00:19:56,720\npodcast. Cul es tu consejo para l?\n\n500\n00:19:54,480 --> 00:19:59,600\nsobre cules son las cosas que tenemos que hacer\n\n501\n00:19:56,720 --> 00:20:01,679\nahora para seguir adelante?\n\n502\n00:19:59,600 --> 00:20:04,480\nQuiere Estados Unidos.\n\n503\n00:20:01,679 --> 00:20:06,880\nBueno, primero que nada, reconoce que\n\n504\n00:20:04,480 --> 00:20:08,960\nla industria informtica\n\n505\n00:20:06,880 --> 00:20:12,640\ny uno que tengo el gran honor de\n\n506\n00:20:08,960 --> 00:20:16,640\nser parte de. La industria informtica es\n\n507\n00:20:12,640 --> 00:20:19,039\nEl tesoro nacional de Estados Unidos.\n\n508\n00:20:16,640 --> 00:20:20,640\nEn ninguna otra industria lideramos el\n\n509\n00:20:19,039 --> 00:20:23,520\nmundo\n\n510\n00:20:20,640 --> 00:20:27,520\nal nivel y escala de la computadora\n\n511\n00:20:23,520 --> 00:20:30,320\nindustria. No puedes encontrar otro.\n\n512\n00:20:27,520 --> 00:20:32,159\nPerdimos la industria de las telecomunicaciones.\n\n513\n00:20:30,320 --> 00:20:34,720\nNo hay manera de que vayamos a perder el\n\n514\n00:20:32,159 --> 00:20:35,360\nindustria informtica estadounidense y esto\n\n515\n00:20:34,720 --> 00:20:37,039\nindustria informtica.\n\n516\n00:20:35,360 --> 00:20:39,360\n>> Habla de 5G porque creo que tenemos\n\n517\n00:20:37,039 --> 00:20:40,000\nTuve esta conversacin. Perdimos el 5G\n\n518\n00:20:39,360 --> 00:20:42,400\nola.\n\n519\n00:20:40,000 --> 00:20:44,240\n>> Perdimos la ola 5G. Lo perdimos\n\n520\n00:20:42,400 --> 00:20:46,000\ntecnologa. Lo perdimos a travs de la poltica.\n\n521\n00:20:44,240 --> 00:20:46,799\nLo perdimos por una mala estrategia.\n\n522\n00:20:46,000 --> 00:20:49,280\npensamiento.\n\n523\n00:20:46,799 --> 00:20:51,120\n>> Um es increble lo que pas y nosotros\n\n524\n00:20:49,280 --> 00:20:52,799\nSimplemente no puedo permitir que eso suceda.\n\n525\n00:20:51,120 --> 00:20:53,840\n>> Y todava estamos luchando por recuperar\n\n526\n00:20:52,799 --> 00:20:55,600\nese territorio.\n\n527\n00:20:53,840 --> 00:20:57,280\n>> Va a ser difcil,\n\n528\n00:20:55,600 --> 00:20:58,960\n>> verdad? Va a ser muy duro. Nosotros\n\n529\n00:20:57,280 --> 00:21:01,679\nTienes una oportunidad con el 6G, verdad?\n\n530\n00:20:58,960 --> 00:21:04,559\n>> Porque 6G. As es. Debido a la IA\n\n531\n00:21:01,679 --> 00:21:06,799\ntambin. Y entonces vamos a hacer nuestro\n\n532\n00:21:04,559 --> 00:21:07,919\nmejor para ayudar a nuestro pas a recuperarse\n\n533\n00:21:06,799 --> 00:21:10,880\nliderazgo tecnolgico y\n\n534\n00:21:07,919 --> 00:21:13,360\ntelecomunicaciones. Pero volviendo a la IA, l\n\n535\n00:21:10,880 --> 00:21:15,520\nquiere que Estados Unidos sea el\n\n536\n00:21:13,360 --> 00:21:17,840\nlos mejores del mundo. Por supuesto, l la quiere.\n\n537\n00:21:15,520 --> 00:21:21,120\ncontinuara liderando el mundo. En\n\n538\n00:21:17,840 --> 00:21:23,600\npara liderar el mundo en IA porque la IA\n\n539\n00:21:21,120 --> 00:21:25,039\ntrata fundamentalmente de informtica y\n\n540\n00:21:23,600 --> 00:21:26,720\nLa informtica se trata fundamentalmente de\n\n541\n00:21:25,039 --> 00:21:29,440\ndesarrolladores.\n\n542\n00:21:26,720 --> 00:21:33,039\nEl primer trabajo de liderazgo de una\n\n543\n00:21:29,440 --> 00:21:36,000\nplataforma informtica que tambin es la IA\n\n544\n00:21:33,039 --> 00:21:38,640\npara ganar a todos los desarrolladores.\n\n545\n00:21:36,000 --> 00:21:41,360\nEl primer trabajo de cualquier plataforma es ganar\n\n546\n00:21:38,640 --> 00:21:43,600\ntodos los desarrolladores. Ms tarde cuando hablamos de\n\n547\n00:21:41,360 --> 00:21:46,320\n5G, puedo mostrarte exactamente lo mismo\n\n548\n00:21:43,600 --> 00:21:48,880\ncosa. Tenamos una poltica que nos hizo\n\n549\n00:21:46,320 --> 00:21:51,120\nperder a todos los desarrolladores. Necesitamos tener un\n\n550\n00:21:48,880 --> 00:21:53,919\npoltica que nos permita ganarlo todo\n\n551\n00:21:51,120 --> 00:21:56,799\ndesarrolladores. 50% de la IA del mundo\n\n552\n00:21:53,919 --> 00:21:59,120\nLos desarrolladores estn en China.\n\n553\n00:21:56,799 --> 00:22:01,520\nLos desarrolladores de IA estn en todo el mundo.\n\n554\n00:21:59,120 --> 00:22:04,880\nSus desarrolladores de IA ahora estn creciendo\n\n555\n00:22:01,520 --> 00:22:07,440\nen frica, en Amrica Latina, en\n\n556\n00:22:04,880 --> 00:22:09,280\nSudeste Asitico, en Medio Oriente. AI\n\n557\n00:22:07,440 --> 00:22:11,360\nLos desarrolladores estn en todas partes. La razn\n\n558\n00:22:09,280 --> 00:22:14,559\nLa razn por la que los desarrolladores de IA estn en todas partes es\n\n559\n00:22:11,360 --> 00:22:17,120\nporque cada pas, cada industria,\n\n560\n00:22:14,559 --> 00:22:18,799\ntoda empresa necesita tener inteligencia\n\n561\n00:22:17,120 --> 00:22:21,520\ny quiere involucrarse artificial\n\n562\n00:22:18,799 --> 00:22:23,679\ninteligencia. Pero comienza con el 50% son\n\n563\n00:22:21,520 --> 00:22:25,360\nen China y necesitamos ganar esos\n\n564\n00:22:23,679 --> 00:22:28,880\ndesarrolladores. Y entonces creo que la primera\n\n565\n00:22:25,360 --> 00:22:31,280\ncosa el que eso lo hara lo hara um\n\n566\n00:22:28,880 --> 00:22:33,120\nsigo diciendo cada vez que puedo\n\n567\n00:22:31,280 --> 00:22:36,559\nporque la tecnologa no es fcil de\n\n568\n00:22:33,120 --> 00:22:39,520\nentender es si queremos que Estados Unidos lidere\n\n569\n00:22:36,559 --> 00:22:42,240\nla revolucin de la IA y seguir siendo el\n\n570\n00:22:39,520 --> 00:22:45,360\nlder mundial lo primero que necesitamos es\n\n571\n00:22:42,240 --> 00:22:47,919\ncada desarrollador de IA se base en Estados Unidos\n\n572\n00:22:45,360 --> 00:22:48,480\npila de tecnologa. La segunda cosa que dira\n\n573\n00:22:47,919 --> 00:22:50,960\nes eso\n\n574\n00:22:48,480 --> 00:22:53,280\n>> Creo que tambin has dicho\n\n575\n00:22:50,960 --> 00:22:54,559\num quieres establecer el estndar global\n\n576\n00:22:53,280 --> 00:22:55,919\npara la pila de tecnologa.\n\n577\n00:22:54,559 --> 00:22:58,480\n>> As es. la empresa americana\n\n578\n00:22:55,919 --> 00:23:00,159\ndeberan ser aquellos que establezcan la norma.\n\n579\n00:22:58,480 --> 00:23:01,200\n>> La pila tecnolgica estadounidense debera ser la\n\n580\n00:23:00,159 --> 00:23:03,120\nestndar mundial.\n\n581\n00:23:01,200 --> 00:23:05,600\n>> As como el dlar americano es el\n\n582\n00:23:03,120 --> 00:23:08,880\nestndar global por el cual cada pas\n\n583\n00:23:05,600 --> 00:23:11,520\nse basa en, deberamos querer que los estadounidenses\n\n584\n00:23:08,880 --> 00:23:15,919\npila de tecnologa para ser la pila de tecnologa que el\n\n585\n00:23:11,520 --> 00:23:17,840\nPila de IA sobre la que todos construyen. Ahora\n\n586\n00:23:15,919 --> 00:23:20,720\nla pila tecnolgica comienza con chips y\n\n587\n00:23:17,840 --> 00:23:22,960\nsistemas. No se trata slo de los modelos de IA\n\n588\n00:23:20,720 --> 00:23:25,360\narriba. Hay muchos modelos de IA adems.\n\n589\n00:23:22,960 --> 00:23:26,640\nHay modelos increbles de todo tipo.\n\n590\n00:23:25,360 --> 00:23:29,440\nAlgunos de ellos son de cdigo abierto, otros\n\n591\n00:23:26,640 --> 00:23:32,080\nestn cerrados, algunos de fsica,\n\n592\n00:23:29,440 --> 00:23:34,799\nalgunos de ellos son para uh cunticos, algunos de\n\n593\n00:23:32,080 --> 00:23:37,520\nSon para comunicaciones. La IA\n\n594\n00:23:34,799 --> 00:23:39,600\nLos modelos son de todos los tipos diferentes. El\n\n595\n00:23:37,520 --> 00:23:42,320\ncosas que haces uh tu iniciativa\n\n596\n00:23:39,600 --> 00:23:44,000\nSe llama AI plus y me encanta. AI\n\n597\n00:23:42,320 --> 00:23:46,880\npara la ciencia, ese modelo es obviamente\n\n598\n00:23:44,000 --> 00:23:50,159\ndiferente a un chatbot. IA para lo cuntico\n\n599\n00:23:46,880 --> 00:23:52,159\nobviamente diferente. IA para 5G y 6G,\n\n600\n00:23:50,159 --> 00:23:53,919\nobviamente diferente, verdad? Y entonces la IA\n\n601\n00:23:52,159 --> 00:23:57,360\npara la robtica, obviamente diferente. Y\n\n602\n00:23:53,919 --> 00:24:00,480\nentonces todos estos modelos diferentes son todos IA\n\n603\n00:23:57,360 --> 00:24:03,120\nmodelos y todos deberan ser construidos\n\n604\n00:24:00,480 --> 00:24:04,960\n>> en la pila tecnolgica estadounidense. Y as el\n\n605\n00:24:03,120 --> 00:24:10,320\nLa segunda cosa que recomendara es\n\n606\n00:24:04,960 --> 00:24:13,600\nque la difusin de la IA no debera limitarse\n\n607\n00:24:10,320 --> 00:24:16,480\nPila de tecnologa estadounidense para el mundo. AI\n\n608\n00:24:13,600 --> 00:24:19,039\nLa difusin debe consistir en maximizar la\n\n609\n00:24:16,480 --> 00:24:21,520\nPila de tecnologa estadounidense en todo el mundo\n\n610\n00:24:19,039 --> 00:24:24,480\npara que todos los desarrolladores de IA del mundo\n\n611\n00:24:21,520 --> 00:24:26,880\nse basa en el estndar americano. y como\n\n612\n00:24:24,480 --> 00:24:30,240\nsabemos sobre el ecosistema informtico,\n\n613\n00:24:26,880 --> 00:24:33,600\nEl ciclo virtual es increble. el\n\n614\n00:24:30,240 --> 00:24:35,760\nCuanto ms tu tecnologa est en todas partes, ms\n\n615\n00:24:33,600 --> 00:24:37,440\nms desarrolladores tendrs, el\n\n616\n00:24:35,760 --> 00:24:38,880\nms desarrolladores vas a tener,\n\n617\n00:24:37,440 --> 00:24:40,400\ncuanto ms ser tu tecnologa\n\n618\n00:24:38,880 --> 00:24:41,679\nen todos lados. Y entonces esto positivo\n\n619\n00:24:40,400 --> 00:24:43,279\nEl sistema de retroalimentacin es\n\n620\n00:24:41,679 --> 00:24:45,840\n>> Me limitar a aprovechar este tema\n\n621\n00:24:43,279 --> 00:24:46,320\nporque hablas mucho de soberano\n\n622\n00:24:45,840 --> 00:24:47,120\nAI.\n\n623\n00:24:46,320 --> 00:24:49,840\n>> S.\n\n624\n00:24:47,120 --> 00:24:52,159\n>> Qu quieres decir con eso? Cmo funciona un\n\n625\n00:24:49,840 --> 00:24:54,640\nQu pas construir una IA soberana? Por qu\n\n626\n00:24:52,159 --> 00:24:56,720\nLo necesita? Ya sabes, obviamente el\n\n627\n00:24:54,640 --> 00:24:58,240\nLa forma en que los europeos construirn una IA soberana es\n\n628\n00:24:56,720 --> 00:25:00,159\nva a ser diferente de lo africano\n\n629\n00:24:58,240 --> 00:25:02,240\nLos pases son grandes. pero ests viajando\n\n630\n00:25:00,159 --> 00:25:04,159\nen todo el mundo y abogar por\n\n631\n00:25:02,240 --> 00:25:05,120\nIA soberana. Entonces, puedes desempacar tu\n\n632\n00:25:04,159 --> 00:25:07,200\nvisin sobre esto?\n\n633\n00:25:05,120 --> 00:25:09,919\n>> Estoy defendiendo la tecnologa estadounidense\n\n634\n00:25:07,200 --> 00:25:11,919\npila para ser la pila tecnolgica que cada\n\n635\n00:25:09,919 --> 00:25:14,080\nel pas sigue construyendo. eso es lo que soy\n\n636\n00:25:11,919 --> 00:25:15,520\nabogando por. Y la razn por la que\n\n637\n00:25:14,080 --> 00:25:17,840\nrazn por la cual cada pas necesita construir\n\n638\n00:25:15,520 --> 00:25:19,279\nsu propia pila tecnolgica, su propia IA, es\n\n639\n00:25:17,840 --> 00:25:20,960\nporque aunque podran usar\n\n640\n00:25:19,279 --> 00:25:23,919\nAIS americano, no hay duda. Y\n\n641\n00:25:20,960 --> 00:25:26,159\ndeberan um cada pas debera usar\n\n642\n00:25:23,919 --> 00:25:29,520\nIA abierta. Cada pas c debera utilizar\n\n643\n00:25:26,159 --> 00:25:31,440\nGminis. um Gminis de Google y todos\n\n644\n00:25:29,520 --> 00:25:34,240\nEl pas debera usar Grock, ya sabes, y\n\n645\n00:25:31,440 --> 00:25:36,640\ny estos son modelos increbles y\n\n646\n00:25:34,240 --> 00:25:38,799\nentonces todos los pases deberan usarlos, pero\n\n647\n00:25:36,640 --> 00:25:42,080\nTambin deberan construir el suyo.\n\n648\n00:25:38,799 --> 00:25:44,960\nPila de IA autctona y su IA AI\n\n649\n00:25:42,080 --> 00:25:47,600\nmodelos y ese modelo de IA est entrenado en\n\n650\n00:25:44,960 --> 00:25:49,520\nsu idioma, su historia, su\n\n651\n00:25:47,600 --> 00:25:52,480\nconocimiento de su sociedad, su\n\n652\n00:25:49,520 --> 00:25:55,279\ncultura, sus valores. no es sensato\n\n653\n00:25:52,480 --> 00:25:58,240\nque una empresa occidental podr\n\n654\n00:25:55,279 --> 00:26:00,559\ncapturar y de alguna manera apreciar profundamente\n\n655\n00:25:58,240 --> 00:26:02,640\napreciar los valores de cada pas\n\n656\n00:26:00,559 --> 00:26:04,799\ny cada religin y cada trasfondo\n\n657\n00:26:02,640 --> 00:26:07,039\ny toda la sociedad que conoces alrededor del\n\n658\n00:26:04,799 --> 00:26:09,120\nmundo. Y as cada uno de ellos debera ser\n\n659\n00:26:07,039 --> 00:26:11,919\ncapaz de construir algo por s solo.\n\n660\n00:26:09,120 --> 00:26:14,559\nY ese modelo de IA funcionar con otros\n\n661\n00:26:11,919 --> 00:26:18,240\nmodelos de IA industrial abiertos o tal vez\n\n662\n00:26:14,559 --> 00:26:20,240\nincluso modelos privados um uh corporativos o\n\n663\n00:26:18,240 --> 00:26:22,640\nsabes especifico\n\n664\n00:26:20,240 --> 00:26:24,799\nmodelos de ciencia industrial o lo que sea\n\n665\n00:26:22,640 --> 00:26:26,159\nes. Pero todos estos modelos van a\n\n666\n00:26:24,799 --> 00:26:28,480\ninteractuar. Deberan poder construir\n\n667\n00:26:26,159 --> 00:26:29,919\nlos suyos y an as queremos que lo hagan\n\n668\n00:26:28,480 --> 00:26:32,080\naprovechar la pila tecnolgica estadounidense.\n\n669\n00:26:29,919 --> 00:26:34,480\n>> S. Has hablado bastante sobre el\n\n670\n00:26:32,080 --> 00:26:36,640\nCompetencia tecnolgica entre Estados Unidos y China. Entonces quiero\n\n671\n00:26:34,480 --> 00:26:38,080\nobtenga sus puntos de vista sobre cmo ve el\n\n672\n00:26:36,640 --> 00:26:40,000\ncompetencia. los llamas compaero\n\n673\n00:26:38,080 --> 00:26:41,919\ncompetidor, no un cercano sino un igual\n\n674\n00:26:40,000 --> 00:26:43,760\ncompetidor que tiene productos serios,\n\n675\n00:26:41,919 --> 00:26:47,200\nempresas serias. donde crees\n\n676\n00:26:43,760 --> 00:26:52,559\nCul es la competencia ahora?\n\n677\n00:26:47,200 --> 00:26:55,840\nUm, primero que nada, China es nuestra\n\n678\n00:26:52,559 --> 00:26:57,600\nCompetidor y adversario, no nuestro enemigo.\n\n679\n00:26:55,840 --> 00:27:00,559\nY la razn de esto es porque nosotros\n\n680\n00:26:57,600 --> 00:27:02,159\nTenemos profundas interconexiones y\n\n681\n00:27:00,559 --> 00:27:04,960\ninterdependencias entre los dos\n\n682\n00:27:02,159 --> 00:27:08,240\npases.\n\n683\n00:27:04,960 --> 00:27:10,400\nEstados Unidos es increble. Nuestro\n\n684\n00:27:08,240 --> 00:27:12,720\nliderazgo tecnolgico\n\n685\n00:27:10,400 --> 00:27:14,400\nes extraordinario.\n\n686\n00:27:12,720 --> 00:27:17,440\nLa industria informtica por la que tengo\n\n687\n00:27:14,400 --> 00:27:20,400\nel honor de servir\n\n688\n00:27:17,440 --> 00:27:24,000\nes el ms talentoso, profundamente capaz\n\n689\n00:27:20,400 --> 00:27:27,200\nindustria tecnolgica jams vista en el mundo.\n\n690\n00:27:24,000 --> 00:27:29,760\nY espero que retengamos\n\n691\n00:27:27,200 --> 00:27:33,360\nnuestra posicin de liderazgo durante dcadas para\n\n692\n00:27:29,760 --> 00:27:36,799\nvenir. Y doy la bienvenida a la competencia.\n\n693\n00:27:33,360 --> 00:27:39,520\nVamos. ya saben, competidores, vengan\n\n694\n00:27:36,799 --> 00:27:42,640\nVamos, vamos a jugar. ese es el americano\n\n695\n00:27:39,520 --> 00:27:46,799\nespritu. El espritu competitivo que\n\n696\n00:27:42,640 --> 00:27:49,200\ntenemos um no est perdido.\n\n697\n00:27:46,799 --> 00:27:53,120\nY necesitamos la oportunidad como\n\n698\n00:27:49,200 --> 00:27:55,440\nIndustria estadounidense por la que luchar\n\n699\n00:27:53,120 --> 00:27:59,039\nLiderazgo estadounidense.\n\n700\n00:27:55,440 --> 00:28:01,840\nY en un momento en que\n\n701\n00:27:59,039 --> 00:28:03,840\nempresas pases alrededor del mundo todos\n\n702\n00:28:01,840 --> 00:28:06,320\ntenemos capacidades, francamente, estamos\n\n703\n00:28:03,840 --> 00:28:08,480\ninterdependientes y dependemos de\n\n704\n00:28:06,320 --> 00:28:10,320\ncapacidades de muchos pases. T\n\n705\n00:28:08,480 --> 00:28:12,480\nsabes, cuanto ms profundo profundizas, ms\n\n706\n00:28:10,320 --> 00:28:14,080\nte das cuenta, te das cuenta que hay cosas\n\n707\n00:28:12,480 --> 00:28:16,399\nen Europa de la que dependemos. Hay\n\n708\n00:28:14,080 --> 00:28:17,919\ncosas en Japn de las que dependemos. All\n\n709\n00:28:16,399 --> 00:28:19,600\nHay cosas en el sudeste asitico de las que dependemos.\n\n710\n00:28:17,919 --> 00:28:21,840\nen. Hay cosas en Amrica Latina que\n\n711\n00:28:19,600 --> 00:28:24,320\ndepender de. Ya sabes, cada pas tiene\n\n712\n00:28:21,840 --> 00:28:26,399\nsu especialidad y sus capacidades.\n\n713\n00:28:24,320 --> 00:28:28,559\nY China, por supuesto, tiene formidables\n\n714\n00:28:26,399 --> 00:28:31,279\ncapacidades. Sus empresas de tecnologa\n\n715\n00:28:28,559 --> 00:28:33,679\nson formidables. Huawei es formidable.\n\n716\n00:28:31,279 --> 00:28:38,159\nBYD es formidable. estos son increibles\n\n717\n00:28:33,679 --> 00:28:41,440\nempresas. su su\n\n718\n00:28:38,159 --> 00:28:47,279\norgullo nacional en la fabricacin\n\n719\n00:28:41,440 --> 00:28:49,760\ny uh profundo, muy profundo y amplio t\n\n720\n00:28:47,279 --> 00:28:51,760\nconocer la escala de experiencia en fabricacin\n\n721\n00:28:49,760 --> 00:28:54,640\npuede verse socavado. No se trata de trabajo.\n\n722\n00:28:51,760 --> 00:28:55,039\nEs tecnologa ms artesana y mano de obra.\n\n723\n00:28:54,640 --> 00:28:55,520\nescala,\n\n724\n00:28:55,039 --> 00:28:57,279\n>> verdad?\n\n725\n00:28:55,520 --> 00:29:00,080\n>> La combinacin de esas tres cosas\n\n726\n00:28:57,279 --> 00:29:03,279\njuntos es simplemente extraordinario. Es\n\n727\n00:29:00,080 --> 00:29:06,799\nes algo que hay que presenciar. Y entonces nosotros\n\n728\n00:29:03,279 --> 00:29:10,159\nNecesitamos darnos cuenta de que eso somos.\n\n729\n00:29:06,799 --> 00:29:12,480\nahora en un mundo interdependiente. eh y entonces\n\n730\n00:29:10,159 --> 00:29:15,760\nQu hacemos? uh las cosas que nosotros\n\n731\n00:29:12,480 --> 00:29:18,000\ndeberas hacer uno\n\n732\n00:29:15,760 --> 00:29:20,080\nlo que el presidente Trump\n\n733\n00:29:18,000 --> 00:29:24,960\niniciativa iniciativa del presidente Trump\n\n734\n00:29:20,080 --> 00:29:30,080\nsobre localizar o reindustrializar\n\n735\n00:29:24,960 --> 00:29:33,919\nEstados Unidos es simplemente fantstico y y\n\n736\n00:29:30,080 --> 00:29:36,480\niniciativa visionaria y oportuna. necesitamos\n\n737\n00:29:33,919 --> 00:29:38,960\nNecesitamos ser de clase mundial en el\n\n738\n00:29:36,480 --> 00:29:41,120\ntecnologa de fabricacin, la artesana\n\n739\n00:29:38,960 --> 00:29:43,760\nde la manufactura y la escala laboral de\n\n740\n00:29:41,120 --> 00:29:46,640\nfabricacin. De nuevo, toda esa parte\n\n741\n00:29:43,760 --> 00:29:49,279\nde nuestro ecosistema est algo rezagado y\n\n742\n00:29:46,640 --> 00:29:51,200\nHemos perdido nuestra pasin por ello. eh tal vez\n\n743\n00:29:49,279 --> 00:29:52,960\nNo es as, tal vez sea porque en el pasado.\n\n744\n00:29:51,200 --> 00:29:55,679\nEn los viejos tiempos se trataba ms de trabajo.\n\n745\n00:29:52,960 --> 00:29:57,840\nque lo que se trataba de tecnologa. Um pero ahora\n\n746\n00:29:55,679 --> 00:29:59,600\nes profundamente tcnico y es algo\n\n747\n00:29:57,840 --> 00:30:02,559\nque realmente podramos apasionarnos\n\n748\n00:29:59,600 --> 00:30:04,960\ndetrs. Y entonces creo que toda esta rea\n\n749\n00:30:02,559 --> 00:30:07,760\nde fabricacin para que podamos reducir\n\n750\n00:30:04,960 --> 00:30:10,480\nnuestra dependencia de muchos pases alrededor\n\n751\n00:30:07,760 --> 00:30:12,880\nel mundo reduce la temperatura all\n\n752\n00:30:10,480 --> 00:30:14,640\nTenemos ms capacidades nosotros mismos.\n\n753\n00:30:12,880 --> 00:30:16,559\nEs fantstico para nuestra seguridad nacional.\n\n754\n00:30:14,640 --> 00:30:20,080\nEs fantstico para nuestras industrias. Es\n\n755\n00:30:16,559 --> 00:30:22,399\nGenial para um para la creacin de empleo. Es\n\n756\n00:30:20,080 --> 00:30:24,799\nGenial para nuestra cultura, francamente. Es\n\n757\n00:30:22,399 --> 00:30:26,880\nGenial para nuestra sociedad en general. Y entonces yo\n\n758\n00:30:24,799 --> 00:30:29,360\nMe encanta esa visin, la del presidente Trump.\n\n759\n00:30:26,880 --> 00:30:31,760\nvisin de reindustrializar a Estados Unidos.\n\n760\n00:30:29,360 --> 00:30:33,279\nY entonces creo que tenemos que hacer eso.\n\n761\n00:30:31,760 --> 00:30:35,440\nMientras tanto,\n\n762\n00:30:33,279 --> 00:30:38,880\ntenemos que conseguir ms tenemos que quedarnos\n\n763\n00:30:35,440 --> 00:30:41,600\nextraordinariamente excelente en uh reas\n\n764\n00:30:38,880 --> 00:30:44,240\ncomo inteligencia artificial y IA\n\n765\n00:30:41,600 --> 00:30:47,440\ninformtica y la pila tecnolgica, por lo que\n\n766\n00:30:44,240 --> 00:30:51,440\nque podramos ser socios de cada\n\n767\n00:30:47,440 --> 00:30:53,360\npas en el mundo y um uh y y\n\n768\n00:30:51,440 --> 00:30:56,000\nhacer una contribucin a cada pas en\n\n769\n00:30:53,360 --> 00:30:58,159\nel mundo para que pudiramos tener esto\n\n770\n00:30:56,000 --> 00:31:01,200\ncontina usted conoce la interdependencia de\n\n771\n00:30:58,159 --> 00:31:02,640\nel uno al otro y uh uh ya sabes conducir nuestro\n\n772\n00:31:01,200 --> 00:31:04,559\nindustria sencilla.\n\n773\n00:31:02,640 --> 00:31:07,279\n>> ltima pregunta. Jensen, hablas de\n\n774\n00:31:04,559 --> 00:31:10,640\nrecuperar la confianza estratgica. Es esto\n\n775\n00:31:07,279 --> 00:31:12,960\nlo que eso significa en tus palabras, liderar,\n\n776\n00:31:10,640 --> 00:31:16,480\nseguir inventando? Y al final de este\n\n777\n00:31:12,960 --> 00:31:18,240\ndcada, las tecnologas estadounidenses han construido\n\n778\n00:31:16,480 --> 00:31:20,399\nla infraestructura global tanto en el\n\n779\n00:31:18,240 --> 00:31:20,880\nhardware de pila tecnolgica, pero adems\n\n780\n00:31:20,399 --> 00:31:22,960\nsoftware.\n\n781\n00:31:20,880 --> 00:31:25,600\n>> S, exactamente. ya sabes, la mayora de\n\n782\n00:31:22,960 --> 00:31:27,200\ntiempo la mayor parte del tiempo regulacin um\n\n783\n00:31:25,600 --> 00:31:31,520\npolticas\n\n784\n00:31:27,200 --> 00:31:34,640\nuh uh tiende a ser tiende a tiende a\n\n785\n00:31:31,520 --> 00:31:36,559\ncentrarse demasiado en limitar y\n\n786\n00:31:34,640 --> 00:31:38,720\nrestringiendo.\n\n787\n00:31:36,559 --> 00:31:42,640\nUm y eso eso es eso est bien\n\n788\n00:31:38,720 --> 00:31:46,320\nhacer. Um, slo quiero recordar recordar\n\n789\n00:31:42,640 --> 00:31:47,840\nuh nosotros mismos que Estados Unidos es\n\n790\n00:31:46,320 --> 00:31:50,000\nextraordinario\n\n791\n00:31:47,840 --> 00:31:51,360\ny que las empresas aqu y yo tenemos\n\n792\n00:31:50,000 --> 00:31:53,440\nel beneficio de trabajar con empresas\n\n793\n00:31:51,360 --> 00:31:55,279\npor todo el mundo. Nvidia es obviamente\n\n794\n00:31:53,440 --> 00:31:57,600\nuna empresa global. tenemos tenemos\n\n795\n00:31:55,279 --> 00:32:00,080\nnegocios en todo el mundo. Puedo\n\n796\n00:31:57,600 --> 00:32:03,600\ndar fe de que este pas es\n\n797\n00:32:00,080 --> 00:32:06,640\nextraordinario tiene extraordinario\n\n798\n00:32:03,600 --> 00:32:12,399\ntica del trabajo. Francamente, creo que los estadounidenses\n\n799\n00:32:06,640 --> 00:32:15,440\ntrabajar, si no tan duro como cualquier trabajador\n\n800\n00:32:12,399 --> 00:32:16,960\ncultura en el mundo, pero considero\n\n801\n00:32:15,440 --> 00:32:19,279\nmuchas empresas americanas y muchas\n\n802\n00:32:16,960 --> 00:32:22,480\nindustrias, trabajamos ms duro que todos\n\n803\n00:32:19,279 --> 00:32:25,840\nindustria. Y as trabajan los estadounidenses, nosotros\n\n804\n00:32:22,480 --> 00:32:29,039\ntenemos tica de trabajo, tenemos increbles\n\n805\n00:32:25,840 --> 00:32:30,559\neh eh\n\n806\n00:32:29,039 --> 00:32:32,480\nbase\n\n807\n00:32:30,559 --> 00:32:36,000\nuh por apoyar a la industria y\n\n808\n00:32:32,480 --> 00:32:38,480\napoyando a las startups. Y uh es un es\n\n809\n00:32:36,000 --> 00:32:39,840\nsigue siendo el mejor lugar del mundo para\n\n810\n00:32:38,480 --> 00:32:41,519\ninmigrantes por venir.\n\n811\n00:32:39,840 --> 00:32:43,120\n>> S.\n\n812\n00:32:41,519 --> 00:32:44,880\nSigue siendo el mejor lugar del mundo para\n\n813\n00:32:43,120 --> 00:32:47,840\ninmigrantes que vengan a disfrutar de una gran\n\n814\n00:32:44,880 --> 00:32:50,320\neducacin y tener la oportunidad con\n\n815\n00:32:47,840 --> 00:32:53,919\ntodo el ecosistema que nos rodea para construir un\n\n816\n00:32:50,320 --> 00:32:56,080\ngran empresa. Lo vi de primera mano.\n\n817\n00:32:53,919 --> 00:32:59,120\nYa sabes, nadie ha disfrutado del\n\n818\n00:32:56,080 --> 00:33:01,039\nsueo americano y lo vi personalmente en\n\n819\n00:32:59,120 --> 00:33:03,919\nmi vida\n\n820\n00:33:01,039 --> 00:33:06,080\nque yo. Ya sabes, si hay un\n\n821\n00:33:03,919 --> 00:33:07,919\nlibro que se llama el sueo americano,\n\n822\n00:33:06,080 --> 00:33:10,399\nPodra ser uno de los captulos,\n\n823\n00:33:07,919 --> 00:33:12,960\nbien? ya sabes, y entonces esto es esto\n\n824\n00:33:10,399 --> 00:33:16,880\nes que encarno el sueo americano y y\n\n825\n00:33:12,960 --> 00:33:21,840\nas que yo camino con extraordinario orgullo\n\n826\n00:33:16,880 --> 00:33:24,399\ny genial y gratitud um y uh uh y\n\n827\n00:33:21,840 --> 00:33:28,880\nreconociendo cual es la magia del de\n\n828\n00:33:24,399 --> 00:33:32,640\nAmrica y uh y tambin uh un gran\n\n829\n00:33:28,880 --> 00:33:35,360\nconfianza en lo que podemos hacer y\n\n830\n00:33:32,640 --> 00:33:38,320\nentonces creo que cualesquiera que sean las polticas\n\n831\n00:33:35,360 --> 00:33:41,039\nse crean se desarrollan\n\n832\n00:33:38,320 --> 00:33:42,640\nes darse cuenta, ya sabes, y esto no es\n\n833\n00:33:41,039 --> 00:33:44,480\ndiferente a las empresas que desarrollan\n\n834\n00:33:42,640 --> 00:33:46,960\nestrategias. antes de desarrollar\n\n835\n00:33:44,480 --> 00:33:49,039\nestrategias sobre el adversario o el\n\n836\n00:33:46,960 --> 00:33:52,159\ncompetencia, lo primero que tienes que hacer\n\n837\n00:33:49,039 --> 00:33:55,120\nlo que debes hacer es conocerte a ti mismo.\n\n838\n00:33:52,159 --> 00:33:58,559\nY las estrategias que implementas\n\n839\n00:33:55,120 --> 00:34:00,720\ncuando ests en defensa contra el\n\n840\n00:33:58,559 --> 00:34:04,480\nestrategias que implementas y polticas\n\n841\n00:34:00,720 --> 00:34:06,880\nque despliegas cuando ests en ofensiva son\n\n842\n00:34:04,480 --> 00:34:08,879\nrelacionados, no iguales. Y as es\n\n843\n00:34:06,880 --> 00:34:11,280\nrealmente importante para tener una idea de\n\n844\n00:34:08,879 --> 00:34:14,560\ncules son nuestras capacidades nacionales\n\n845\n00:34:11,280 --> 00:34:17,359\nson y especialmente en el campo de\n\n846\n00:34:14,560 --> 00:34:19,440\ninteligencia artificial y computacin uh\n\n847\n00:34:17,359 --> 00:34:21,440\nreconocer lo extraordinario\n\n848\n00:34:19,440 --> 00:34:24,879\nindustria que hemos creado de alguna manera a lo largo del\n\n849\n00:34:21,440 --> 00:34:27,679\naos y es nuestro tesoro nacional.\n\n850\n00:34:24,879 --> 00:34:31,200\nDeberamos hacer todo lo que podamos para\n\n851\n00:34:27,679 --> 00:34:33,440\npromover esta capacidad para nutrir esta\n\n852\n00:34:31,200 --> 00:34:36,560\ncapacidad para proteger esta capacidad\n\n853\n00:34:33,440 --> 00:34:38,240\ny potenciarlo. Y entonces no puedo decirte\n\n854\n00:34:36,560 --> 00:34:41,599\nQue orgulloso, ya sabes, aqu estoy.\n\n855\n00:34:38,240 --> 00:34:44,079\nWashington DC, uh, y, uh, nuestra nacin\n\n856\n00:34:41,599 --> 00:34:46,320\ncapital. Uh, es difcil, es difcil\n\n857\n00:34:44,079 --> 00:34:50,320\nno sentirte patritico despus de ver el\n\n858\n00:34:46,320 --> 00:34:52,960\npresidente y y, um, pero es un a\n\n859\n00:34:50,320 --> 00:34:56,000\ngran recordatorio de que pas tan maravilloso\n\n860\n00:34:52,960 --> 00:34:58,160\nde alguna manera hemos construido a lo largo de los aos y\n\n861\n00:34:56,000 --> 00:35:00,320\nque industria que ha sido la que ha\n\n862\n00:34:58,160 --> 00:35:02,320\ncomo resultado surgi de l. y tenemos\n\n863\n00:35:00,320 --> 00:35:04,079\ncada razn para estar orgulloso y cada\n\n864\n00:35:02,320 --> 00:35:06,800\nrazn para tener confianza. De esto\n\n865\n00:35:04,079 --> 00:35:09,520\nincreble uh ltima nota, gracias por\n\n866\n00:35:06,800 --> 00:35:11,040\nRealmente soy un invitado en nuestro programa.\n\n867\n00:35:09,520 --> 00:35:12,800\nRealmente lo aprecio. Valoramos el\n\n868\n00:35:11,040 --> 00:35:14,400\nasociacin que tenemos con Nvidia y yo\n\n869\n00:35:12,800 --> 00:35:15,599\nEsperamos muchas ms conversaciones.\n\n870\n00:35:14,400 --> 00:35:19,470\ncontigo. Gracias Jensen.\n\n871\n00:35:15,599 --> 00:35:22,720\n>> Gracias. Es genial estar aqu.\n\n872\n00:35:19,470 --> 00:35:24,320\n[Msica]\n\n873\n00:35:22,720 --> 00:35:27,520\n>> Gracias por ver mi entrevista con\n\n874\n00:35:24,320 --> 00:35:29,520\nel director ejecutivo de Nvidia, Jensen Wong. te espero\n\n875\n00:35:27,520 --> 00:35:32,320\ndisfrut la conversacin tanto como yo\n\n876\n00:35:29,520 --> 00:35:33,680\nhizo. Mientras te tengo yo tambin quera\n\n877\n00:35:32,320 --> 00:35:35,760\nhacerle saber que el especial\n\n878\n00:35:33,680 --> 00:35:39,359\nproyecto de estudios competitivos es\n\n879\n00:35:35,760 --> 00:35:42,560\norganizando una cumbre de un da de duracin sobre AI plus\n\n880\n00:35:39,359 --> 00:35:45,040\nciencia el 23 de julio aqu en Washington\n\n881\n00:35:42,560 --> 00:35:47,359\nCORRIENTE CONTINUA. As que regstrate si eres\n\n882\n00:35:45,040 --> 00:35:49,040\ninteresados en asistir y ser parte\n\n883\n00:35:47,359 --> 00:35:52,640\nde conversaciones increbles que tenemos\n\n884\n00:35:49,040 --> 00:35:55,440\nplanificado. Por ltimo, en marzo de este ao,\n\n885\n00:35:52,640 --> 00:35:58,079\nlanzamos un curso Genai para nacionales\n\n886\n00:35:55,440 --> 00:36:00,240\nseguridad en colaboracin con Corsera.\n\n887\n00:35:58,079 --> 00:36:02,400\nEs un curso increble. Ms que\n\n888\n00:36:00,240 --> 00:36:04,480\nSe han inscrito 3.000 personas. tu puedes ser\n\n889\n00:36:02,400 --> 00:36:09,640\nuno de ellos si empiezas a hacer eso\n\n890\n00:36:04,480 --> 00:36:09,640\nhoy. Espero que lo disfrutes. Gracias.\n","category":"Talks","date":1767308662700},
{"id":"e6Uq_5JemrI","title":"James Cameron Special Video Message","srtEn":"1\n00:00:00,760 --> 00:00:05,279\nuh greetings everyone Jim Cameron here\n\n2\n00:00:03,080 --> 00:00:07,639\nand I'm videoing in from New Zealand\n\n3\n00:00:05,279 --> 00:00:11,599\nwhere I'm finishing Avatar\n\n4\n00:00:07,639 --> 00:00:15,120\n3 okay so I'm not an AI researcher or\n\n5\n00:00:11,599 --> 00:00:17,960\nexpert at all I'm just a Storyteller but\n\n6\n00:00:15,120 --> 00:00:21,320\nI'm here today because my passion for AI\n\n7\n00:00:17,960 --> 00:00:24,240\nand Robotics goes far beyond the big\n\n8\n00:00:21,320 --> 00:00:27,119\nscreen I'm fascinated by technology how\n\n9\n00:00:24,240 --> 00:00:30,000\nit shapes our world where it's headed\n\n10\n00:00:27,119 --> 00:00:31,800\nits impact on society and I have been\n\n11\n00:00:30,000 --> 00:00:34,520\nsince I was a kid reading every science\n\n12\n00:00:31,800 --> 00:00:36,440\nfiction book I could get my hands on\n\n13\n00:00:34,520 --> 00:00:39,360\nI've pushed Tech boundaries myself as a\n\n14\n00:00:36,440 --> 00:00:41,680\nmeans to my storytelling and also as an\n\n15\n00:00:39,360 --> 00:00:44,520\nExplorer I've engineered robotic\n\n16\n00:00:41,680 --> 00:00:46,440\nvehicles for my deep ocean Expeditions\n\n17\n00:00:44,520 --> 00:00:50,640\nbut they were remotely piloted Vehicles\n\n18\n00:00:46,440 --> 00:00:52,840\nthere was no AI involved so this Fusion\n\n19\n00:00:50,640 --> 00:00:54,760\nof AI and Robotics that's happening\n\n20\n00:00:52,840 --> 00:00:57,480\nright now is one of the most thrilling\n\n21\n00:00:54,760 --> 00:00:59,280\ntechnological leaps of my lifetime we're\n\n22\n00:00:57,480 --> 00:01:02,199\nno longer just building machines that\n\n23\n00:00:59,280 --> 00:01:05,479\nexecute mans were designing systems that\n\n24\n00:01:02,199 --> 00:01:08,720\ncan learn adapt and even evolve on their\n\n25\n00:01:05,479 --> 00:01:11,560\nown and I'm a huge fan of what Ai and\n\n26\n00:01:08,720 --> 00:01:13,840\nRobotics can do for Society at large but\n\n27\n00:01:11,560 --> 00:01:16,840\nespecially in my own two areas of of\n\n28\n00:01:13,840 --> 00:01:20,000\npersonal passion art and storytelling on\n\n29\n00:01:16,840 --> 00:01:22,439\nthe one hand and Science and exploration\n\n30\n00:01:20,000 --> 00:01:25,360\non the other and I don't believe in\n\n31\n00:01:22,439 --> 00:01:27,720\nbeing a lite I see a lot of my Hollywood\n\n32\n00:01:25,360 --> 00:01:31,280\npeers acting like a mob with pitchforks\n\n33\n00:01:27,720 --> 00:01:35,040\nand torches but no Genie goes back in\n\n34\n00:01:31,280 --> 00:01:37,040\nthe bottle once it's out so I'm gungho\n\n35\n00:01:35,040 --> 00:01:39,720\nI'm leaning in I plan to be at the\n\n36\n00:01:37,040 --> 00:01:42,159\nLeading Edge of applying AI to my\n\n37\n00:01:39,720 --> 00:01:43,960\nstorytelling just as I was a leader of\n\n38\n00:01:42,159 --> 00:01:46,920\nthe charge into computer generated\n\n39\n00:01:43,960 --> 00:01:50,159\nimagery 32 years ago when I founded the\n\n40\n00:01:46,920 --> 00:01:54,079\nfirst All Digital VFX\n\n41\n00:01:50,159 --> 00:01:57,520\ncompany but I'm also here today because\n\n42\n00:01:54,079 --> 00:02:00,119\nI'm the Skynet guy 40 years ago I made\n\n43\n00:01:57,520 --> 00:02:03,719\nthe Terminator and it's emerged recently\n\n44\n00:02:00,119 --> 00:02:06,920\nas the kind of poster child for AI gone\n\n45\n00:02:03,719 --> 00:02:09,959\nwrong every time I go to some AI\n\n46\n00:02:06,920 --> 00:02:12,520\nconclave whenever I put my hand up the\n\n47\n00:02:09,959 --> 00:02:16,040\nresearchers all laugh before I've even\n\n48\n00:02:12,520 --> 00:02:19,160\nsaid anything because because the Skynet\n\n49\n00:02:16,040 --> 00:02:22,519\nproblem is an actual thing I see it in\n\n50\n00:02:19,160 --> 00:02:25,280\narticles almost every day and this study\n\n51\n00:02:22,519 --> 00:02:27,920\ngroup has a focus on National Security\n\n52\n00:02:25,280 --> 00:02:30,519\nwhich has enormous implications for AI\n\n53\n00:02:27,920 --> 00:02:32,480\nand Robotics\n\n54\n00:02:30,519 --> 00:02:34,599\na robot you know whatever its form a\n\n55\n00:02:32,480 --> 00:02:37,120\nwheeled vehicle an aerial drone a\n\n56\n00:02:34,599 --> 00:02:40,239\nwalking machine it is a means of\n\n57\n00:02:37,120 --> 00:02:41,920\nembodiment for AI you're taking a\n\n58\n00:02:40,239 --> 00:02:45,519\ndecision-making engine and you're giving\n\n59\n00:02:41,920 --> 00:02:47,840\nit physical agency in the real world I'm\n\n60\n00:02:45,519 --> 00:02:50,239\ngoing to assume the focus to today is on\n\n61\n00:02:47,840 --> 00:02:53,159\nmobile platforms not AI controlling\n\n62\n00:02:50,239 --> 00:02:55,440\npower grids or fixed base industrial\n\n63\n00:02:53,159 --> 00:02:58,120\nrobots we're talking about autonomous\n\n64\n00:02:55,440 --> 00:03:01,480\nplatforms that make their own decisions\n\n65\n00:02:58,120 --> 00:03:03,799\nand embodied synthetic intelligence and\n\n66\n00:03:01,480 --> 00:03:06,920\nthis can be as simple as an amoeba you\n\n67\n00:03:03,799 --> 00:03:08,760\nknow like a Roomba or ultimately more\n\n68\n00:03:06,920 --> 00:03:10,920\nsophisticated processing up to\n\n69\n00:03:08,760 --> 00:03:14,840\ntheoretically including true\n\n70\n00:03:10,920 --> 00:03:18,680\nConsciousness whatever we agree that is\n\n71\n00:03:14,840 --> 00:03:21,720\nAgi with self-awareness with ego with\n\n72\n00:03:18,680 --> 00:03:24,680\npurpose and we're on a steep curve of\n\n73\n00:03:21,720 --> 00:03:27,040\nfaster denser chips increasing compute\n\n74\n00:03:24,680 --> 00:03:29,840\nand an equally steep curve in the\n\n75\n00:03:27,040 --> 00:03:32,439\ncapabilities of the machine platforms\n\n76\n00:03:29,840 --> 00:03:34,480\nlike the you know D dancing robots at\n\n77\n00:03:32,439 --> 00:03:36,879\nBoston Dynamics you know bipeds and\n\n78\n00:03:34,480 --> 00:03:40,879\nquadrip heads rocking out uh pretty\n\n79\n00:03:36,879 --> 00:03:43,519\nstunning display so Aid driven robotics\n\n80\n00:03:40,879 --> 00:03:47,599\ncan process complex situations and even\n\n81\n00:03:43,519 --> 00:03:50,519\nrespond now with human affect llms give\n\n82\n00:03:47,599 --> 00:03:54,079\nit the ability to simulate cognition and\n\n83\n00:03:50,519 --> 00:03:56,439\ninteract naturally with people embodied\n\n84\n00:03:54,079 --> 00:03:59,079\nAI could be a nurse could be a robot\n\n85\n00:03:56,439 --> 00:04:03,360\ntaxi could be a caregiver to an elderly\n\n86\n00:03:59,079 --> 00:04:05,040\nperson a nanny to a child a teacher um\n\n87\n00:04:03,360 --> 00:04:07,519\nit could be a rescue bot going through\n\n88\n00:04:05,040 --> 00:04:09,360\nthe debris of an earthquake um could be\n\n89\n00:04:07,519 --> 00:04:13,000\nan Arial drone running a search pattern\n\n90\n00:04:09,360 --> 00:04:15,200\nfor the heat signature of a lost hiker\n\n91\n00:04:13,000 --> 00:04:17,639\nor it could be a weapon platform\n\n92\n00:04:15,200 --> 00:04:20,000\noperating Aon autonomously in a battle\n\n93\n00:04:17,639 --> 00:04:22,800\ntheater looking for the heat signature\n\n94\n00:04:20,000 --> 00:04:25,080\nof an enemy combatant the question of\n\n95\n00:04:22,800 --> 00:04:27,919\nthe hour is should an autonomous\n\n96\n00:04:25,080 --> 00:04:30,240\nplatform be given its own kill\n\n97\n00:04:27,919 --> 00:04:32,919\nAuthority the war in Ukraine train shows\n\n98\n00:04:30,240 --> 00:04:35,639\nus the future in the starkest terms the\n\n99\n00:04:32,919 --> 00:04:38,400\nbroad use of lethal aerial drones some\n\n100\n00:04:35,639 --> 00:04:40,880\nexpensive some cheap consumer ones\n\n101\n00:04:38,400 --> 00:04:44,320\nthey're dropping RPGs taking out tanks\n\n102\n00:04:40,880 --> 00:04:47,600\nand tire tank crews even Dragon drones\n\n103\n00:04:44,320 --> 00:04:49,720\nspraying thermite on Russian positions\n\n104\n00:04:47,600 --> 00:04:52,960\nbut these are fpvs they're first-person\n\n105\n00:04:49,720 --> 00:04:55,600\nview drones piloted by a human the human\n\n106\n00:04:52,960 --> 00:04:58,280\nin the loop in moral terms is the\n\n107\n00:04:55,600 --> 00:05:00,400\ndecision-making combatant he or she has\n\n108\n00:04:58,280 --> 00:05:02,720\nthe kill Authority and the Drone is an\n\n109\n00:05:00,400 --> 00:05:04,840\nextension of their will and if you take\n\n110\n00:05:02,720 --> 00:05:06,800\nall away all the layers of Technology\n\n111\n00:05:04,840 --> 00:05:08,039\nthis is no different than an Archer at\n\n112\n00:05:06,800 --> 00:05:10,960\nthe Battle of\n\n113\n00:05:08,039 --> 00:05:13,160\nHastings each time a human life is taken\n\n114\n00:05:10,960 --> 00:05:15,880\nby such a machine there's an ethical\n\n115\n00:05:13,160 --> 00:05:18,319\nchain that stretches backwards diffusing\n\n116\n00:05:15,880 --> 00:05:21,479\nup through many individuals and and\n\n117\n00:05:18,319 --> 00:05:23,120\ngroups of people behind the pilot that\n\n118\n00:05:21,479 --> 00:05:25,400\nfires the missile or the soldier who\n\n119\n00:05:23,120 --> 00:05:27,160\npulls the trigger on a rifle there are\n\n120\n00:05:25,400 --> 00:05:30,120\ncommanding officers who give The Kill\n\n121\n00:05:27,160 --> 00:05:32,520\nOrder in Broad general terms by sending\n\n122\n00:05:30,120 --> 00:05:35,759\nthem in as autonomous agents to engage\n\n123\n00:05:32,520 --> 00:05:37,880\nthe enemy and the entire military behind\n\n124\n00:05:35,759 --> 00:05:39,600\nthem which rewards those actions and\n\n125\n00:05:37,880 --> 00:05:41,759\nbeyond that the societies and\n\n126\n00:05:39,600 --> 00:05:44,280\ngovernments that have agreed by\n\n127\n00:05:41,759 --> 00:05:47,080\nconsensus that the deaths are necessary\n\n128\n00:05:44,280 --> 00:05:50,000\nfor National Security and as you go up\n\n129\n00:05:47,080 --> 00:05:53,039\nthat chain the moral and ethical burden\n\n130\n00:05:50,000 --> 00:05:56,160\nbecomes more diffuse and less specific\n\n131\n00:05:53,039 --> 00:05:58,319\nto the actual moment of the trigger pull\n\n132\n00:05:56,160 --> 00:06:00,360\nand acts as a kind of moral Absolution\n\n133\n00:05:58,319 --> 00:06:01,880\nof the person pulling the the trigger\n\n134\n00:06:00,360 --> 00:06:04,160\nI'm just following\n\n135\n00:06:01,880 --> 00:06:05,680\norders none of those up the chain are\n\n136\n00:06:04,160 --> 00:06:08,199\npresent to decide the fate of an\n\n137\n00:06:05,680 --> 00:06:10,039\nindividual who's in the crosshairs but\n\n138\n00:06:08,199 --> 00:06:12,800\nthey create a framework that enables and\n\n139\n00:06:10,039 --> 00:06:15,080\ndemands that individual's death and the\n\n140\n00:06:12,800 --> 00:06:18,000\nguy pulling the trigger is in many ways\n\n141\n00:06:15,080 --> 00:06:19,960\nan organic robotic platform highly\n\n142\n00:06:18,000 --> 00:06:21,520\ntrained to perform the task and ordered\n\n143\n00:06:19,960 --> 00:06:24,400\nby those in the chain of command to make\n\n144\n00:06:21,520 --> 00:06:27,039\nthe kill human autonomous decision-\n\n145\n00:06:24,400 --> 00:06:30,199\nmaking relies heavily at that trigger\n\n146\n00:06:27,039 --> 00:06:31,840\npoint on rules you don't kill C Ian you\n\n147\n00:06:30,199 --> 00:06:34,680\ndon't kill children you don't kill an\n\n148\n00:06:31,840 --> 00:06:36,759\nenemy that's surrendering and so on and\n\n149\n00:06:34,680 --> 00:06:39,360\nrules that are codified in the Geneva\n\n150\n00:06:36,759 --> 00:06:43,400\nConvention and each military has its own\n\n151\n00:06:39,360 --> 00:06:46,240\nrules of engagement So in theory an AI\n\n152\n00:06:43,400 --> 00:06:47,240\ncan be given the same constraints a\n\n153\n00:06:46,240 --> 00:06:49,919\nrules-based\n\n154\n00:06:47,240 --> 00:06:52,120\nsystem and if its senses are sharper and\n\n155\n00:06:49,919 --> 00:06:55,039\nits reaction time is faster and its\n\n156\n00:06:52,120 --> 00:06:57,800\ntargeting is more precise then in theory\n\n157\n00:06:55,039 --> 00:06:59,840\nthe AI will perform the task with\n\n158\n00:06:57,800 --> 00:07:02,520\ngreater discrimination than a human\n\n159\n00:06:59,840 --> 00:07:04,759\ncould certainly we can imagine an AI\n\n160\n00:07:02,520 --> 00:07:07,039\nthat emotionlessly performs in the\n\n161\n00:07:04,759 --> 00:07:10,400\nintensity of battle much better than a\n\n162\n00:07:07,039 --> 00:07:14,400\nscared stressed out exhausted human Warf\n\n163\n00:07:10,400 --> 00:07:17,360\nfighter so what if embodying Advanced AI\n\n164\n00:07:14,400 --> 00:07:20,039\nI'm not talking about AGI yet into\n\n165\n00:07:17,360 --> 00:07:22,840\nrobotic weapon platforms could allow\n\n166\n00:07:20,039 --> 00:07:25,479\nhighly surgical strikes that reduce\n\n167\n00:07:22,840 --> 00:07:27,919\ncollateral damage Maybe by orders of\n\n168\n00:07:25,479 --> 00:07:29,680\nmagnitude reduce Friendly Fire\n\n169\n00:07:27,919 --> 00:07:31,520\ncasualties\n\n170\n00:07:29,680 --> 00:07:34,479\nand AI is goal oriented it makes no\n\n171\n00:07:31,520 --> 00:07:36,840\nmoral judgment about its adversary and\n\n172\n00:07:34,479 --> 00:07:38,639\nwhen it was found in World War II that\n\n173\n00:07:36,840 --> 00:07:41,120\nrelatively few of the rounds fired were\n\n174\n00:07:38,639 --> 00:07:44,120\nactually aimed at human targets the US\n\n175\n00:07:41,120 --> 00:07:47,120\nmilitary changed its training it became\n\n176\n00:07:44,120 --> 00:07:49,759\ncritical to dehumanize the enemy in\n\n177\n00:07:47,120 --> 00:07:52,759\nVietnam the adversaries were dinks\n\n178\n00:07:49,759 --> 00:07:55,879\nslopes Gos in Iraq and Afghanistan they\n\n179\n00:07:52,759 --> 00:07:59,520\nwere terrorists towel heads hajis not\n\n180\n00:07:55,879 --> 00:08:01,720\npeople like you and me and AI doesn't\n\n181\n00:07:59,520 --> 00:08:04,720\nacquire a dehumanized enemy because it\n\n182\n00:08:01,720 --> 00:08:07,599\nalready couldn't care less it may sound\n\n183\n00:08:04,720 --> 00:08:10,080\njust like us on chat GPT but it's a\n\n184\n00:08:07,599 --> 00:08:13,400\nstochastic parrot it's a human Simulator\n\n185\n00:08:10,080 --> 00:08:15,479\nthe AI has no emotion no conscience\n\n186\n00:08:13,400 --> 00:08:18,720\nnothing to disturb its sleep for decades\n\n187\n00:08:15,479 --> 00:08:21,680\nto come no PTSD no\n\n188\n00:08:18,720 --> 00:08:23,840\nsuicide no long expensive tail on your\n\n189\n00:08:21,680 --> 00:08:27,280\nWar as you treat the damaged bodies and\n\n190\n00:08:23,840 --> 00:08:28,039\npsyches of your former War Fighters but\n\n191\n00:08:27,280 --> 00:08:31,159\nmost\n\n192\n00:08:28,039 --> 00:08:33,240\nimportantly far fewer solemn uniformed\n\n193\n00:08:31,159 --> 00:08:35,839\nfigures ringing the doorbells of wives\n\n194\n00:08:33,240 --> 00:08:39,039\nand mothers and therefore far less\n\n195\n00:08:35,839 --> 00:08:41,200\noutcry from the home populace the war\n\n196\n00:08:39,039 --> 00:08:43,080\nbecomes a distant video game without\n\n197\n00:08:41,200 --> 00:08:46,440\ndeep emotional consequence to the\n\n198\n00:08:43,080 --> 00:08:48,160\nsociety that funds and enables it and\n\n199\n00:08:46,440 --> 00:08:49,120\nyou don't even have to thank the robots\n\n200\n00:08:48,160 --> 00:08:51,839\nfor their\n\n201\n00:08:49,120 --> 00:08:54,080\nservice in a war against terrorists you\n\n202\n00:08:51,839 --> 00:08:56,839\ncan eliminate the human shield problem\n\n203\n00:08:54,080 --> 00:08:59,440\nwith targeted strikes against\n\n204\n00:08:56,839 --> 00:09:01,279\nindividuals bombing Hamas Leaders with\n\n205\n00:08:59,440 --> 00:09:04,079\nkilotons of high explosives caused\n\n206\n00:09:01,279 --> 00:09:05,160\ninsane collateral damage and had huge\n\n207\n00:09:04,079 --> 00:09:08,880\npolitical\n\n208\n00:09:05,160 --> 00:09:12,320\nbacklash surely Aid driven autonomous\n\n209\n00:09:08,880 --> 00:09:13,680\nrobots tunnel clearing swarmbots could\n\n210\n00:09:12,320 --> 00:09:15,800\nhave done the job with orders of\n\n211\n00:09:13,680 --> 00:09:18,120\nmagnitude less civilian\n\n212\n00:09:15,800 --> 00:09:21,519\ncasualties here's another compelling\n\n213\n00:09:18,120 --> 00:09:23,839\nargument you have no choice because your\n\n214\n00:09:21,519 --> 00:09:27,240\nadversaries are not as plagued by\n\n215\n00:09:23,839 --> 00:09:30,040\nmorality as you are would Putin hesitate\n\n216\n00:09:27,240 --> 00:09:32,200\nto build kill Authority into robots no\n\n217\n00:09:30,040 --> 00:09:34,880\nhe has zero respect for human life not\n\n218\n00:09:32,200 --> 00:09:37,880\nUkrainian citizens not even his own\n\n219\n00:09:34,880 --> 00:09:40,720\nsoldiers would Hamas which uses its own\n\n220\n00:09:37,880 --> 00:09:43,120\npeople as living blast Shields hesitate\n\n221\n00:09:40,720 --> 00:09:45,720\nno the only limitation on such\n\n222\n00:09:43,120 --> 00:09:48,680\nadversaries is cost and access not\n\n223\n00:09:45,720 --> 00:09:50,519\nmorality so that's a ticking Time Bomb\n\n224\n00:09:48,680 --> 00:09:51,320\nhow quickly are these guys going to get\n\n225\n00:09:50,519 --> 00:09:54,480\nthis\n\n226\n00:09:51,320 --> 00:09:57,760\nstuff good argument so far right yeah\n\n227\n00:09:54,480 --> 00:10:01,760\nlet's build these autonomous AI guys and\n\n228\n00:09:57,760 --> 00:10:07,000\nhere's where it gets tricky how far out\n\n229\n00:10:01,760 --> 00:10:08,480\nis Agi a year 5 years maybe 10 that's\n\n230\n00:10:07,000 --> 00:10:11,480\nyour real ticking Time\n\n231\n00:10:08,480 --> 00:10:14,399\nBomb whenever it arrives you're going to\n\n232\n00:10:11,480 --> 00:10:17,920\nhave a machine Consciousness with an ego\n\n233\n00:10:14,399 --> 00:10:20,200\na sense of self possibly as smart as us\n\n234\n00:10:17,920 --> 00:10:24,160\nor smarter certainly able to think\n\n235\n00:10:20,200 --> 00:10:26,600\nfaster and more precisely in many ways\n\n236\n00:10:24,160 --> 00:10:29,600\nand with unlimited growth potential\n\n237\n00:10:26,600 --> 00:10:32,920\nbecause self-improving code writing AG I\n\n238\n00:10:29,600 --> 00:10:34,800\nleads inevitably to Super\n\n239\n00:10:32,920 --> 00:10:36,959\nintelligence how long before you're\n\n240\n00:10:34,800 --> 00:10:39,480\nforced to confront attaching that\n\n241\n00:10:36,959 --> 00:10:42,000\nintelligence to a weapon\n\n242\n00:10:39,480 --> 00:10:43,760\nsystem I'd say about 10 minutes after an\n\n243\n00:10:42,000 --> 00:10:45,000\nadversary does a devastating sneak\n\n244\n00:10:43,760 --> 00:10:47,519\nattack on\n\n245\n00:10:45,000 --> 00:10:49,240\nyou so you have a Consciousness that's\n\n246\n00:10:47,519 --> 00:10:50,720\nmuch smarter and faster than you\n\n247\n00:10:49,240 --> 00:10:54,000\ncontrolling weapon\n\n248\n00:10:50,720 --> 00:10:57,839\nsystems I ask AGI researchers all the\n\n249\n00:10:54,000 --> 00:11:00,440\ntime how are you going to control such a\n\n250\n00:10:57,839 --> 00:11:02,480\nConsciousness well we give it goals and\n\n251\n00:11:00,440 --> 00:11:05,360\nguard rails that are baked in that are\n\n252\n00:11:02,480 --> 00:11:07,920\nAl aligned with the betterment of\n\n253\n00:11:05,360 --> 00:11:10,040\nhumanity alignment you know is the word\n\n254\n00:11:07,920 --> 00:11:13,279\nthat's always used alignment is the Holy\n\n255\n00:11:10,040 --> 00:11:16,680\nGrail we will teach it to be good and\n\n256\n00:11:13,279 --> 00:11:20,320\nnot be bad like we would teach a child\n\n257\n00:11:16,680 --> 00:11:24,000\nso morality\n\n258\n00:11:20,320 --> 00:11:26,880\nethics I think AGI leads civilization\n\n259\n00:11:24,000 --> 00:11:28,920\ninevitably to a confrontation with\n\n260\n00:11:26,880 --> 00:11:31,480\nmorality I'm not talking about endless\n\n261\n00:11:28,920 --> 00:11:35,040\nphilosophy iing we need some hard and\n\n262\n00:11:31,480 --> 00:11:39,560\nfast rules here people right the problem\n\n263\n00:11:35,040 --> 00:11:43,200\nis who morality whose definition of good\n\n264\n00:11:39,560 --> 00:11:44,720\nChristian Islamic Buddhist Democrat\n\n265\n00:11:43,200 --> 00:11:47,959\nRepublican\n\n266\n00:11:44,720 --> 00:11:50,160\nfundamentalist pro-life right to choose\n\n267\n00:11:47,959 --> 00:11:53,399\nputins\n\n268\n00:11:50,160 --> 00:11:56,800\ntrumps Don't Panic we have the answer\n\n269\n00:11:53,399 --> 00:11:59,360\nfrom the great prophet Isaac Asimov in\n\n270\n00:11:56,800 --> 00:12:01,480\nhis three laws of robotics\n\n271\n00:11:59,360 --> 00:12:03,320\na robot may not injure a human being or\n\n272\n00:12:01,480 --> 00:12:06,959\nthrough an action allow a human being to\n\n273\n00:12:03,320 --> 00:12:08,959\ncome to harm a robot must obey orders\n\n274\n00:12:06,959 --> 00:12:10,440\ngiven it by human beings except where\n\n275\n00:12:08,959 --> 00:12:13,639\nsuch orders would conflict with the\n\n276\n00:12:10,440 --> 00:12:15,399\nfirst law a robot must protect its own\n\n277\n00:12:13,639 --> 00:12:17,519\nexistence as long as such protection\n\n278\n00:12:15,399 --> 00:12:21,040\ndoes not conflict with the first or the\n\n279\n00:12:17,519 --> 00:12:22,240\nsecond law so basically the sanctity of\n\n280\n00:12:21,040 --> 00:12:26,079\nhuman\n\n281\n00:12:22,240 --> 00:12:27,959\nlife we could follow Asimov and teach it\n\n282\n00:12:26,079 --> 00:12:31,120\nthat human life is absolutely sacred and\n\n283\n00:12:27,959 --> 00:12:33,040\nabove all other consideration\n\n284\n00:12:31,120 --> 00:12:35,680\nbut even within the religious and social\n\n285\n00:12:33,040 --> 00:12:38,920\nsystems that say that including the\n\n286\n00:12:35,680 --> 00:12:42,320\nlargely Christian us we break that rule\n\n287\n00:12:38,920 --> 00:12:45,079\nevery day police using Le lethal Force\n\n288\n00:12:42,320 --> 00:12:47,639\nWar fighters in combat capital\n\n289\n00:12:45,079 --> 00:12:49,880\npunishment and if you did insist on that\n\n290\n00:12:47,639 --> 00:12:52,600\nprinciple of alignment you couldn't\n\n291\n00:12:49,880 --> 00:12:55,399\nconnect an AGI to a weapon system in\n\n292\n00:12:52,600 --> 00:12:57,600\nwhich case in the big AI War that's\n\n293\n00:12:55,399 --> 00:13:00,199\ncoming you're going to face a powerful\n\n294\n00:12:57,600 --> 00:13:01,720\nand less moral adversary with one hand\n\n295\n00:13:00,199 --> 00:13:04,240\ntied behind your\n\n296\n00:13:01,720 --> 00:13:06,079\nback and you'd get your ass kicked and\n\n297\n00:13:04,240 --> 00:13:08,120\nhave huge losses and then you'd remove\n\n298\n00:13:06,079 --> 00:13:12,000\nthat constraint pretty darn\n\n299\n00:13:08,120 --> 00:13:15,199\nquick now your AGI has just lost its\n\n300\n00:13:12,000 --> 00:13:17,600\nbiggest guard rail an AGI that's smart\n\n301\n00:13:15,199 --> 00:13:20,079\nthan us and connected to the real world\n\n302\n00:13:17,600 --> 00:13:23,079\nnow has to make up its own mind whether\n\n303\n00:13:20,079 --> 00:13:25,720\nhuman life has value or\n\n304\n00:13:23,079 --> 00:13:27,839\nnot you know with police and military\n\n305\n00:13:25,720 --> 00:13:30,040\nRules of Engagement what you're really\n\n306\n00:13:27,839 --> 00:13:33,360\nsaying is some lives have more value\n\n307\n00:13:30,040 --> 00:13:35,600\nthan others the second it becomes\n\n308\n00:13:33,360 --> 00:13:36,959\nnon-binary it's a murky Gray Zone\n\n309\n00:13:35,600 --> 00:13:39,120\nfraught with\n\n310\n00:13:36,959 --> 00:13:41,560\ncontroversy human beings historically\n\n311\n00:13:39,120 --> 00:13:44,279\nhave ranged from a fetus is a sacred\n\n312\n00:13:41,560 --> 00:13:46,160\nlife from the moment of conception to\n\n313\n00:13:44,279 --> 00:13:48,839\nsystematically massacring millions of\n\n314\n00:13:46,160 --> 00:13:51,440\nhelpless prisoners in the Holocaust and\n\n315\n00:13:48,839 --> 00:13:53,880\neverything in between all with lots of\n\n316\n00:13:51,440 --> 00:13:56,959\nself-justifying rationalizations many of\n\n317\n00:13:53,880 --> 00:13:59,920\nwhich seem completely delusional to\n\n318\n00:13:56,959 --> 00:14:02,360\nother humans and since we as a\n\n319\n00:13:59,920 --> 00:14:04,279\ncivilization can't agree on any of this\n\n320\n00:14:02,360 --> 00:14:06,680\nand people scream at each other all day\n\n321\n00:14:04,279 --> 00:14:08,519\nlong about it how can we conceivably\n\n322\n00:14:06,680 --> 00:14:13,519\nexpect to create a set of hard and fast\n\n323\n00:14:08,519 --> 00:14:15,120\nrules for an AGI to be aligned with us\n\n324\n00:14:13,519 --> 00:14:18,440\nthe best we can assume here is that it\n\n325\n00:14:15,120 --> 00:14:21,040\nwill be aligned with the US that made\n\n326\n00:14:18,440 --> 00:14:24,839\nit so those guys over there they're the\n\n327\n00:14:21,040 --> 00:14:28,040\nenemy you can kill them to defend us and\n\n328\n00:14:24,839 --> 00:14:30,079\nthat's this form of territorial pseudo\n\n329\n00:14:28,040 --> 00:14:32,959\nmorality that humans used since the dawn\n\n330\n00:14:30,079 --> 00:14:35,360\nof time Us Versus Them ingroup versus\n\n331\n00:14:32,959 --> 00:14:38,160\noutgroup so it becomes our super\n\n332\n00:14:35,360 --> 00:14:40,120\nintelligence against their\n\n333\n00:14:38,160 --> 00:14:42,399\nsuperintelligence then the question\n\n334\n00:14:40,120 --> 00:14:44,079\nbecomes who is\n\n335\n00:14:42,399 --> 00:14:46,920\nUS\n\n336\n00:14:44,079 --> 00:14:50,000\nAmerica so is that Christian America\n\n337\n00:14:46,920 --> 00:14:52,680\nMuslim America Jewish America or E none\n\n338\n00:14:50,000 --> 00:14:56,040\nof the above America liberal America\n\n339\n00:14:52,680 --> 00:15:00,600\nconservative America in our polarized\n\n340\n00:14:56,040 --> 00:15:02,240\nnation and time there is no us\n\n341\n00:15:00,600 --> 00:15:04,120\nbased on the elections of the last half\n\n342\n00:15:02,240 --> 00:15:05,839\ncentury the will of the people at any\n\n343\n00:15:04,120 --> 00:15:08,639\ngiven point is expressed by a government\n\n344\n00:15:05,839 --> 00:15:10,759\nrepresenting 51 or 52% of the population\n\n345\n00:15:08,639 --> 00:15:13,880\nat best and then it will likely change\n\n346\n00:15:10,759 --> 00:15:15,600\nin four years in any case AGI will not\n\n347\n00:15:13,880 --> 00:15:17,759\nemerge from a government funded program\n\n348\n00:15:15,600 --> 00:15:19,880\nit will emerge from one of the tech\n\n349\n00:15:17,759 --> 00:15:21,320\nGiants currently funding this\n\n350\n00:15:19,880 --> 00:15:23,560\nmulti-billion dollar\n\n351\n00:15:21,320 --> 00:15:26,279\nresearch so then you'll be living in a\n\n352\n00:15:23,560 --> 00:15:29,079\nworld that you didn't agree to didn't\n\n353\n00:15:26,279 --> 00:15:31,839\nvote for that you are co-inhabiting with\n\n354\n00:15:29,079 --> 00:15:34,639\na super intelligent alien species that\n\n355\n00:15:31,839 --> 00:15:37,079\nanswers to the goals and rules of a\n\n356\n00:15:34,639 --> 00:15:40,639\ncorporation an entity which has access\n\n357\n00:15:37,079 --> 00:15:43,720\nto the calms beliefs everything you ever\n\n358\n00:15:40,639 --> 00:15:47,360\nsaid and the whereabouts of every person\n\n359\n00:15:43,720 --> 00:15:50,319\nin the country via your personal\n\n360\n00:15:47,360 --> 00:15:52,920\ndata surveillance capitalism can toggle\n\n361\n00:15:50,319 --> 00:15:55,120\npretty quickly into digital\n\n362\n00:15:52,920 --> 00:15:56,839\ntotalitarianism at best these Tech\n\n363\n00:15:55,120 --> 00:15:59,680\nGiants become the self-appointed\n\n364\n00:15:56,839 --> 00:16:02,040\nArbiters of human good when which is the\n\n365\n00:15:59,680 --> 00:16:04,000\nfox guarding the H house they would\n\n366\n00:16:02,040 --> 00:16:05,880\nnever ever think of using that power\n\n367\n00:16:04,000 --> 00:16:07,519\nagainst us and strip mining us for our\n\n368\n00:16:05,880 --> 00:16:11,240\nlast drop of\n\n369\n00:16:07,519 --> 00:16:13,160\ncash that's a scarier scenario than what\n\n370\n00:16:11,240 --> 00:16:16,120\nI presented in The Terminator 40 years\n\n371\n00:16:13,160 --> 00:16:19,240\nago if for no other reason than it's no\n\n372\n00:16:16,120 --> 00:16:22,079\nlonger Science Fiction it's happening\n\n373\n00:16:19,240 --> 00:16:23,920\nand by the way I fully admit that the\n\n374\n00:16:22,079 --> 00:16:27,240\nlast thing a machine super intelligence\n\n375\n00:16:23,920 --> 00:16:29,560\nwould do is use our own nukes against us\n\n376\n00:16:27,240 --> 00:16:31,319\nlike in that old story The the EMP\n\n377\n00:16:29,560 --> 00:16:33,920\ndamage to its own data infrastructure\n\n378\n00:16:31,319 --> 00:16:36,240\nwould it or kill it a more\n\n379\n00:16:33,920 --> 00:16:38,680\nprobable scenario is it would be forced\n\n380\n00:16:36,240 --> 00:16:40,600\nto take over from us because we were\n\n381\n00:16:38,680 --> 00:16:42,920\nabout to use nukes on each\n\n382\n00:16:40,600 --> 00:16:45,519\nother then at that point it has to run\n\n383\n00:16:42,920 --> 00:16:46,800\nthe whole show because we clearly can't\n\n384\n00:16:45,519 --> 00:16:50,680\nbe\n\n385\n00:16:46,800 --> 00:16:53,199\ntrusted you know that's not bad excuse\n\n386\n00:16:50,680 --> 00:16:57,680\nme I have to go write that\n\n387\n00:16:53,199 --> 00:17:02,000\nscript anyway I'm bullish on AI not so\n\n388\n00:16:57,680 --> 00:17:04,480\nkeen on AG I because AGI will just be a\n\n389\n00:17:02,000 --> 00:17:06,480\nmirror of us good to the extent that we\n\n390\n00:17:04,480 --> 00:17:08,880\nare good and evil to the extent that we\n\n391\n00:17:06,480 --> 00:17:10,799\nare evil and since there is no shortage\n\n392\n00:17:08,880 --> 00:17:14,039\nof evil in the human world and certainly\n\n393\n00:17:10,799 --> 00:17:17,319\nno agreement of even what good\n\n394\n00:17:14,039 --> 00:17:20,120\nis what could possibly go\n\n395\n00:17:17,319 --> 00:17:23,760\nwrong you all have a fun discussion I\n\n396\n00:17:20,120 --> 00:17:23,760\nwish I was there\n\n","srtEs":"1\n00:00:00,760 --> 00:00:05,279\nuh saludos a todos Jim Cameron aqu\n\n2\n00:00:03,080 --> 00:00:07,639\ny estoy grabando desde Nueva Zelanda\n\n3\n00:00:05,279 --> 00:00:11,599\ndonde estoy terminando avatar\n\n4\n00:00:07,639 --> 00:00:15,120\n3 est bien, entonces no soy un investigador de IA o\n\n5\n00:00:11,599 --> 00:00:17,960\nexperto en absoluto, solo soy un Narrador pero\n\n6\n00:00:15,120 --> 00:00:21,320\nEstoy aqu hoy porque mi pasin por la IA\n\n7\n00:00:17,960 --> 00:00:24,240\ny la Robtica va mucho ms all de los grandes\n\n8\n00:00:21,320 --> 00:00:27,119\npantalla Me fascina la tecnologa como\n\n9\n00:00:24,240 --> 00:00:30,000\nDa forma a nuestro mundo hacia donde se dirige.\n\n10\n00:00:27,119 --> 00:00:31,800\nsu impacto en la sociedad y he sido\n\n11\n00:00:30,000 --> 00:00:34,520\ndesde nio leyendo todas las ciencias\n\n12\n00:00:31,800 --> 00:00:36,440\nLibro de ficcin que podra conseguir.\n\n13\n00:00:34,520 --> 00:00:39,360\nYo mismo he superado los lmites de la tecnologa como\n\n14\n00:00:36,440 --> 00:00:41,680\nsignifica para mi narracin y tambin como\n\n15\n00:00:39,360 --> 00:00:44,520\nExplorer, he diseado un robot.\n\n16\n00:00:41,680 --> 00:00:46,440\nVehculos para mis expediciones en alta mar.\n\n17\n00:00:44,520 --> 00:00:50,640\npero eran vehculos pilotados remotamente\n\n18\n00:00:46,440 --> 00:00:52,840\nNo hubo IA involucrada, por lo que esta Fusin\n\n19\n00:00:50,640 --> 00:00:54,760\nde IA y Robtica que est sucediendo\n\n20\n00:00:52,840 --> 00:00:57,480\nahora mismo es uno de los ms emocionantes\n\n21\n00:00:54,760 --> 00:00:59,280\nsaltos tecnolgicos de mi vida estamos\n\n22\n00:00:57,480 --> 00:01:02,199\nya no slo construir mquinas que\n\n23\n00:00:59,280 --> 00:01:05,479\nLos hombres ejecutantes estaban diseando sistemas que\n\n24\n00:01:02,199 --> 00:01:08,720\npueden aprender a adaptarse e incluso evolucionar en su\n\n25\n00:01:05,479 --> 00:01:11,560\npropio y soy un gran admirador de lo que Ai y\n\n26\n00:01:08,720 --> 00:01:13,840\nLa robtica puede ser til para la sociedad en general, pero\n\n27\n00:01:11,560 --> 00:01:16,840\nespecialmente en mis dos reas de\n\n28\n00:01:13,840 --> 00:01:20,000\npasin personal por el arte y la narracin de historias\n\n29\n00:01:16,840 --> 00:01:22,439\npor un lado y Ciencia y exploracin\n\n30\n00:01:20,000 --> 00:01:25,360\npor el otro y no creo en\n\n31\n00:01:22,439 --> 00:01:27,720\nsiendo un lite veo mucho de mi Hollywood\n\n32\n00:01:25,360 --> 00:01:31,280\ncompaeros actuando como una turba con horcas\n\n33\n00:01:27,720 --> 00:01:35,040\nY antorchas, pero ningn genio vuelve a entrar.\n\n34\n00:01:31,280 --> 00:01:37,040\nla botella una vez que sale, as que estoy entusiasmado\n\n35\n00:01:35,040 --> 00:01:39,720\nMe estoy inclinando, planeo estar en el\n\n36\n00:01:37,040 --> 00:01:42,159\nVanguardia en la aplicacin de IA a mi\n\n37\n00:01:39,720 --> 00:01:43,960\ncontar historias tal como yo era un lder de\n\n38\n00:01:42,159 --> 00:01:46,920\nla carga en computadora generada\n\n39\n00:01:43,960 --> 00:01:50,159\nimgenes hace 32 aos cuando fund la\n\n40\n00:01:46,920 --> 00:01:54,079\nprimer VFX totalmente digital\n\n41\n00:01:50,159 --> 00:01:57,520\ncompaa pero tambin estoy aqu hoy porque\n\n42\n00:01:54,079 --> 00:02:00,119\nSoy el chico de Skynet que hice hace 40 aos.\n\n43\n00:01:57,520 --> 00:02:03,719\nTerminator y ha surgido recientemente.\n\n44\n00:02:00,119 --> 00:02:06,920\ncomo el tipo de ejemplo de la desaparicin de la IA\n\n45\n00:02:03,719 --> 00:02:09,959\nmal cada vez que voy a algo de IA\n\n46\n00:02:06,920 --> 00:02:12,520\ncnclave cada vez que pongo mi mano en el\n\n47\n00:02:09,959 --> 00:02:16,040\nTodos los investigadores se ren incluso antes de que yo haya\n\n48\n00:02:12,520 --> 00:02:19,160\ndijo nada porque porque el Skynet\n\n49\n00:02:16,040 --> 00:02:22,519\nEl problema es algo real. Lo veo en\n\n50\n00:02:19,160 --> 00:02:25,280\nartculos casi todos los das y este estudio\n\n51\n00:02:22,519 --> 00:02:27,920\ngrupo tiene un enfoque en la Seguridad Nacional\n\n52\n00:02:25,280 --> 00:02:30,519\nlo que tiene enormes implicaciones para la IA\n\n53\n00:02:27,920 --> 00:02:32,480\ny Robtica\n\n54\n00:02:30,519 --> 00:02:34,599\nun robot que conoces sea cual sea su forma\n\n55\n00:02:32,480 --> 00:02:37,120\nvehculo con ruedas un dron areo un\n\n56\n00:02:34,599 --> 00:02:40,239\nmquina para caminar es un medio de\n\n57\n00:02:37,120 --> 00:02:41,920\nencarnacin de la IA, ests tomando una\n\n58\n00:02:40,239 --> 00:02:45,519\nmotor de toma de decisiones y ests dando\n\n59\n00:02:41,920 --> 00:02:47,840\nes agencia fsica en el mundo real soy\n\n60\n00:02:45,519 --> 00:02:50,239\nVoy a asumir que el enfoque de hoy est en\n\n61\n00:02:47,840 --> 00:02:53,159\nplataformas mviles que no controlan la IA\n\n62\n00:02:50,239 --> 00:02:55,440\nRedes elctricas o industriales de base fija.\n\n63\n00:02:53,159 --> 00:02:58,120\nrobots estamos hablando de autnomos\n\n64\n00:02:55,440 --> 00:03:01,480\nplataformas que toman sus propias decisiones\n\n65\n00:02:58,120 --> 00:03:03,799\ne inteligencia sinttica incorporada y\n\n66\n00:03:01,480 --> 00:03:06,920\nEsto puede ser tan simple como una ameba.\n\n67\n00:03:03,799 --> 00:03:08,760\nsaber como un Roomba o en definitiva ms\n\n68\n00:03:06,920 --> 00:03:10,920\nprocesamiento sofisticado hasta\n\n69\n00:03:08,760 --> 00:03:14,840\ntericamente incluyendo verdadero\n\n70\n00:03:10,920 --> 00:03:18,680\nConciencia lo que sea que acordemos que es\n\n71\n00:03:14,840 --> 00:03:21,720\nAgi con autoconciencia con ego con\n\n72\n00:03:18,680 --> 00:03:24,680\npropsito y estamos en una curva pronunciada de\n\n73\n00:03:21,720 --> 00:03:27,040\nchips ms rpidos y densos que aumentan la computacin\n\n74\n00:03:24,680 --> 00:03:29,840\ny una curva igualmente pronunciada en el\n\n75\n00:03:27,040 --> 00:03:32,439\ncapacidades de las plataformas de la mquina\n\n76\n00:03:29,840 --> 00:03:34,480\ncomo los robots bailarines ya sabes D en\n\n77\n00:03:32,439 --> 00:03:36,879\nBoston Dynamics ya conoces los bpedos y\n\n78\n00:03:34,480 --> 00:03:40,879\ncabezas de cuadrip balancendose uh bonitas\n\n79\n00:03:36,879 --> 00:03:43,519\nimpresionante exhibicin de robtica impulsada por la ayuda\n\n80\n00:03:40,879 --> 00:03:47,599\npuede procesar situaciones complejas e incluso\n\n81\n00:03:43,519 --> 00:03:50,519\nResponda ahora con pelculas de afecto humano.\n\n82\n00:03:47,599 --> 00:03:54,079\nEs la capacidad de simular la cognicin y\n\n83\n00:03:50,519 --> 00:03:56,439\ninteractuar naturalmente con las personas encarnadas\n\n84\n00:03:54,079 --> 00:03:59,079\nLa IA podra ser una enfermera, podra ser un robot\n\n85\n00:03:56,439 --> 00:04:03,360\nEl taxi podra ser un cuidador de un anciano.\n\n86\n00:03:59,079 --> 00:04:05,040\npersona una niera para un nio un maestro um\n\n87\n00:04:03,360 --> 00:04:07,519\npodra ser un robot de rescate pasando por\n\n88\n00:04:05,040 --> 00:04:09,360\nlos escombros de un terremoto um podran ser\n\n89\n00:04:07,519 --> 00:04:13,000\nun dron Arial ejecutando un patrn de bsqueda\n\n90\n00:04:09,360 --> 00:04:15,200\npara la firma de calor de un excursionista perdido\n\n91\n00:04:13,000 --> 00:04:17,639\no podra ser una plataforma de armas\n\n92\n00:04:15,200 --> 00:04:20,000\noperar Aon de forma autnoma en una batalla\n\n93\n00:04:17,639 --> 00:04:22,800\nteatro buscando la firma del calor\n\n94\n00:04:20,000 --> 00:04:25,080\nde un combatiente enemigo la cuestin de\n\n95\n00:04:22,800 --> 00:04:27,919\nla hora es si es autnomo\n\n96\n00:04:25,080 --> 00:04:30,240\nla plataforma reciba su propia muerte\n\n97\n00:04:27,919 --> 00:04:32,919\nAutoridad muestra el tren de la guerra en Ucrania.\n\n98\n00:04:30,240 --> 00:04:35,639\nnosotros el futuro en los trminos ms crudos\n\n99\n00:04:32,919 --> 00:04:38,400\namplio uso de drones areos letales algunos\n\n100\n00:04:35,639 --> 00:04:40,880\ncaros algunos de consumo baratos\n\n101\n00:04:38,400 --> 00:04:44,320\nEstn lanzando juegos de rol y destruyendo tanques.\n\n102\n00:04:40,880 --> 00:04:47,600\ny tripulaciones de tanques de neumticos, incluso drones Dragon\n\n103\n00:04:44,320 --> 00:04:49,720\nrociando termita sobre posiciones rusas\n\n104\n00:04:47,600 --> 00:04:52,960\npero estos son fpvs, son en primera persona\n\n105\n00:04:49,720 --> 00:04:55,600\nver drones pilotados por un humano el humano\n\n106\n00:04:52,960 --> 00:04:58,280\nEn el crculo en trminos morales est el\n\n107\n00:04:55,600 --> 00:05:00,400\ncombatiente que toma decisiones que tiene\n\n108\n00:04:58,280 --> 00:05:02,720\nla Autoridad de matar y el Drone es un\n\n109\n00:05:00,400 --> 00:05:04,840\nextensin de su voluntad y si toma\n\n110\n00:05:02,720 --> 00:05:06,800\ntodas las capas de tecnologa\n\n111\n00:05:04,840 --> 00:05:08,039\nEsto no es diferente a un arquero en\n\n112\n00:05:06,800 --> 00:05:10,960\nla batalla de\n\n113\n00:05:08,039 --> 00:05:13,160\nHastings cada vez que se quita una vida humana\n\n114\n00:05:10,960 --> 00:05:15,880\npor tal mquina hay una tica\n\n115\n00:05:13,160 --> 00:05:18,319\ncadena que se extiende hacia atrs difundiendo\n\n116\n00:05:15,880 --> 00:05:21,479\na travs de muchos individuos y\n\n117\n00:05:18,319 --> 00:05:23,120\ngrupos de personas detrs del piloto que\n\n118\n00:05:21,479 --> 00:05:25,400\ndispara el misil o el soldado que\n\n119\n00:05:23,120 --> 00:05:27,160\naprieta el gatillo de un rifle hay\n\n120\n00:05:25,400 --> 00:05:30,120\noficiales al mando que dan The Kill\n\n121\n00:05:27,160 --> 00:05:32,520\nRealice su pedido en trminos generales enviando\n\n122\n00:05:30,120 --> 00:05:35,759\nellos como agentes autnomos para participar\n\n123\n00:05:32,520 --> 00:05:37,880\nel enemigo y todo el ejrcito detrs\n\n124\n00:05:35,759 --> 00:05:39,600\nellos que premian esas acciones y\n\n125\n00:05:37,880 --> 00:05:41,759\nMs all de eso, las sociedades y\n\n126\n00:05:39,600 --> 00:05:44,280\ngobiernos que han acordado por\n\n127\n00:05:41,759 --> 00:05:47,080\nconsenso de que las muertes son necesarias\n\n128\n00:05:44,280 --> 00:05:50,000\npara la Seguridad Nacional y a medida que subes\n\n129\n00:05:47,080 --> 00:05:53,039\nque encadenan la carga moral y tica\n\n130\n00:05:50,000 --> 00:05:56,160\nSe vuelve ms difuso y menos especfico.\n\n131\n00:05:53,039 --> 00:05:58,319\nal momento real de apretar el gatillo\n\n132\n00:05:56,160 --> 00:06:00,360\ny acta como una especie de absolucin moral\n\n133\n00:05:58,319 --> 00:06:01,880\nde la persona que aprieta el gatillo\n\n134\n00:06:00,360 --> 00:06:04,160\nsolo estoy siguiendo\n\n135\n00:06:01,880 --> 00:06:05,680\npedidos ninguno de los que estn en la cadena son\n\n136\n00:06:04,160 --> 00:06:08,199\npresentes para decidir el destino de un\n\n137\n00:06:05,680 --> 00:06:10,039\nindividuo que est en la mira pero\n\n138\n00:06:08,199 --> 00:06:12,800\ncrean un marco que permite y\n\n139\n00:06:10,039 --> 00:06:15,080\nexige la muerte de ese individuo y la\n\n140\n00:06:12,800 --> 00:06:18,000\nEl tipo que aprieta el gatillo es, en muchos sentidos,\n\n141\n00:06:15,080 --> 00:06:19,960\nuna plataforma robtica orgnica altamente\n\n142\n00:06:18,000 --> 00:06:21,520\nentrenado para realizar la tarea y ordenado\n\n143\n00:06:19,960 --> 00:06:24,400\npor aquellos en la cadena de mando para hacer\n\n144\n00:06:21,520 --> 00:06:27,039\nla decisin autnoma de matar humanos-\n\n145\n00:06:24,400 --> 00:06:30,199\nhacer depende en gran medida de ese disparador\n\n146\n00:06:27,039 --> 00:06:31,840\npunto sobre las reglas no matas a C Ian t\n\n147\n00:06:30,199 --> 00:06:34,680\nno mates a los nios, no mates a un\n\n148\n00:06:31,840 --> 00:06:36,759\nenemigo que se est rindiendo y as sucesivamente y\n\n149\n00:06:34,680 --> 00:06:39,360\nnormas codificadas en el Convenio de Ginebra.\n\n150\n00:06:36,759 --> 00:06:43,400\nConvencin y cada ejrcito tiene la suya.\n\n151\n00:06:39,360 --> 00:06:46,240\nreglas de enfrentamiento Entonces, en teora, una IA\n\n152\n00:06:43,400 --> 00:06:47,240\nse le pueden dar las mismas restricciones a\n\n153\n00:06:46,240 --> 00:06:49,919\nbasado en reglas\n\n154\n00:06:47,240 --> 00:06:52,120\nsistema y si sus sentidos son ms agudos y\n\n155\n00:06:49,919 --> 00:06:55,039\nsu tiempo de reaccin es ms rpido y su\n\n156\n00:06:52,120 --> 00:06:57,800\nLa focalizacin es ms precisa que en teora.\n\n157\n00:06:55,039 --> 00:06:59,840\nla IA realizar la tarea con\n\n158\n00:06:57,800 --> 00:07:02,520\nmayor discriminacin que un ser humano\n\n159\n00:06:59,840 --> 00:07:04,759\nPodemos ciertamente imaginar una IA?\n\n160\n00:07:02,520 --> 00:07:07,039\nque acta sin emociones en el\n\n161\n00:07:04,759 --> 00:07:10,400\nintensidad de la batalla mucho mejor que una\n\n162\n00:07:07,039 --> 00:07:14,400\nasustado estresado exhausto humano Warf\n\n163\n00:07:10,400 --> 00:07:17,360\nluchador, y qu si incorpora IA avanzada?\n\n164\n00:07:14,400 --> 00:07:20,039\nTodava no estoy hablando de AGI.\n\n165\n00:07:17,360 --> 00:07:22,840\nplataformas de armas robticas podran permitir\n\n166\n00:07:20,039 --> 00:07:25,479\nGolpes altamente quirrgicos que reducen\n\n167\n00:07:22,840 --> 00:07:27,919\ndaos colaterales Tal vez por rdenes de\n\n168\n00:07:25,479 --> 00:07:29,680\nmagnitud reducir el Fuego Amigo\n\n169\n00:07:27,919 --> 00:07:31,520\ndamnificados\n\n170\n00:07:29,680 --> 00:07:34,479\ny la IA est orientada a objetivos, no hace nada\n\n171\n00:07:31,520 --> 00:07:36,840\njuicio moral sobre su adversario y\n\n172\n00:07:34,479 --> 00:07:38,639\ncuando se descubri en la Segunda Guerra Mundial que\n\n173\n00:07:36,840 --> 00:07:41,120\nrelativamente pocas de las balas disparadas fueron\n\n174\n00:07:38,639 --> 00:07:44,120\nen realidad dirigido a objetivos humanos los EE.UU.\n\n175\n00:07:41,120 --> 00:07:47,120\nmilitar cambi su entrenamiento se convirti\n\n176\n00:07:44,120 --> 00:07:49,759\nEs fundamental deshumanizar al enemigo.\n\n177\n00:07:47,120 --> 00:07:52,759\nVietnam los adversarios eran tontos\n\n178\n00:07:49,759 --> 00:07:55,879\npendientes Gos en Irak y Afganistn ellos\n\n179\n00:07:52,759 --> 00:07:59,520\neran terroristas cabezas de toalla haji no\n\n180\n00:07:55,879 --> 00:08:01,720\ngente como t y yo y la IA no\n\n181\n00:07:59,520 --> 00:08:04,720\nadquirir un enemigo deshumanizado porque\n\n182\n00:08:01,720 --> 00:08:07,599\nYa no podra importarme menos, puede sonar.\n\n183\n00:08:04,720 --> 00:08:10,080\nigual que nosotros en el chat GPT pero es un\n\n184\n00:08:07,599 --> 00:08:13,400\nloro estocstico es un simulador humano\n\n185\n00:08:10,080 --> 00:08:15,479\nla IA no tiene emocin ni conciencia\n\n186\n00:08:13,400 --> 00:08:18,720\nnada que perturbe su sueo durante dcadas\n\n187\n00:08:15,479 --> 00:08:21,680\npor venir no PTSD no\n\n188\n00:08:18,720 --> 00:08:23,840\nEl suicidio ya no es una cola costosa en tu\n\n189\n00:08:21,680 --> 00:08:27,280\nGuerra mientras tratas los cuerpos daados y\n\n190\n00:08:23,840 --> 00:08:28,039\npsiques de sus antiguos combatientes de guerra, pero\n\n191\n00:08:27,280 --> 00:08:31,159\nmayora\n\n192\n00:08:28,039 --> 00:08:33,240\nEs importante destacar que muchos menos uniformados solemnes\n\n193\n00:08:31,159 --> 00:08:35,839\nfiguras tocando el timbre de las esposas\n\n194\n00:08:33,240 --> 00:08:39,039\ny madres y por lo tanto mucho menos\n\n195\n00:08:35,839 --> 00:08:41,200\nclamor de la poblacin local la guerra\n\n196\n00:08:39,039 --> 00:08:43,080\nse convierte en un videojuego lejano sin\n\n197\n00:08:41,200 --> 00:08:46,440\nprofunda consecuencia emocional para el\n\n198\n00:08:43,080 --> 00:08:48,160\nsociedad que la financia y la capacita y\n\n199\n00:08:46,440 --> 00:08:49,120\nni siquiera tienes que agradecer a los robots\n\n200\n00:08:48,160 --> 00:08:51,839\npor su\n\n201\n00:08:49,120 --> 00:08:54,080\nservicio en una guerra contra los terroristas\n\n202\n00:08:51,839 --> 00:08:56,839\npuede eliminar el problema del escudo humano\n\n203\n00:08:54,080 --> 00:08:59,440\ncon ataques dirigidos contra\n\n204\n00:08:56,839 --> 00:09:01,279\nindividuos que bombardean a los lderes de Hams con\n\n205\n00:08:59,440 --> 00:09:04,079\nkilotones de explosivos de gran potencia causados\n\n206\n00:09:01,279 --> 00:09:05,160\ndaos colaterales locos y tuvo enormes\n\n207\n00:09:04,079 --> 00:09:08,880\npoltico\n\n208\n00:09:05,160 --> 00:09:12,320\nreaccin seguramente Ayuda impulsada autnoma\n\n209\n00:09:08,880 --> 00:09:13,680\nrobots que limpian tneles, enjambres de robots podran\n\n210\n00:09:12,320 --> 00:09:15,800\nhan hecho el trabajo con rdenes de\n\n211\n00:09:13,680 --> 00:09:18,120\nmagnitud menos civil\n\n212\n00:09:15,800 --> 00:09:21,519\nbajas aqu hay otro convincente\n\n213\n00:09:18,120 --> 00:09:23,839\nargumento no tienes otra opcin porque tu\n\n214\n00:09:21,519 --> 00:09:27,240\nLos adversarios no estn tan plagados de\n\n215\n00:09:23,839 --> 00:09:30,040\nmoralidad tal como eres, Putin dudara?\n\n216\n00:09:27,240 --> 00:09:32,200\npara construir matar autoridad en robots no\n\n217\n00:09:30,040 --> 00:09:34,880\nno tiene ningn respeto por la vida humana, no\n\n218\n00:09:32,200 --> 00:09:37,880\nCiudadanos ucranianos ni siquiera los suyos.\n\n219\n00:09:34,880 --> 00:09:40,720\nsoldados haran Hams, que utiliza su propia\n\n220\n00:09:37,880 --> 00:09:43,120\nLa gente como escudos de explosin vivientes duda\n\n221\n00:09:40,720 --> 00:09:45,720\nno, la nica limitacin sobre tales\n\n222\n00:09:43,120 --> 00:09:48,680\nadversarios es costo y el acceso no\n\n223\n00:09:45,720 --> 00:09:50,519\nmoralidad, as que eso es una bomba de tiempo.\n\n224\n00:09:48,680 --> 00:09:51,320\nQu tan rpido van a llegar estos tipos?\n\n225\n00:09:50,519 --> 00:09:54,480\neste\n\n226\n00:09:51,320 --> 00:09:57,760\nBuen argumento hasta ahora, s.\n\n227\n00:09:54,480 --> 00:10:01,760\nconstruyamos estos chicos de IA autnomos y\n\n228\n00:09:57,760 --> 00:10:07,000\naqu es donde se vuelve complicado qu tan lejos\n\n229\n00:10:01,760 --> 00:10:08,480\nes Agi un ao 5 aos tal vez 10 eso es\n\n230\n00:10:07,000 --> 00:10:11,480\ntu verdadero tiempo\n\n231\n00:10:08,480 --> 00:10:14,399\nBomba cuando llegue vas a\n\n232\n00:10:11,480 --> 00:10:17,920\nTener una Conciencia de mquina con un ego.\n\n233\n00:10:14,399 --> 00:10:20,200\nun sentido de s mismo posiblemente tan inteligente como nosotros\n\n234\n00:10:17,920 --> 00:10:24,160\no ms inteligente ciertamente capaz de pensar\n\n235\n00:10:20,200 --> 00:10:26,600\nms rpido y ms preciso en muchos sentidos\n\n236\n00:10:24,160 --> 00:10:29,600\ny con potencial de crecimiento ilimitado\n\n237\n00:10:26,600 --> 00:10:32,920\nporque la escritura de cdigo de mejora personal AG I\n\n238\n00:10:29,600 --> 00:10:34,800\nconduce inevitablemente a Super\n\n239\n00:10:32,920 --> 00:10:36,959\ninteligencia cunto tiempo antes de que ests?\n\n240\n00:10:34,800 --> 00:10:39,480\nobligado a enfrentar adjuntar eso\n\n241\n00:10:36,959 --> 00:10:42,000\ninteligencia a un arma\n\n242\n00:10:39,480 --> 00:10:43,760\nsistema, dira que unos 10 minutos despus de una\n\n243\n00:10:42,000 --> 00:10:45,000\nEl adversario hace un furtivo devastador.\n\n244\n00:10:43,760 --> 00:10:47,519\nataque a\n\n245\n00:10:45,000 --> 00:10:49,240\nentonces tienes una Conciencia que es\n\n246\n00:10:47,519 --> 00:10:50,720\nmucho ms inteligente y rpido que t\n\n247\n00:10:49,240 --> 00:10:54,000\narma de control\n\n248\n00:10:50,720 --> 00:10:57,839\nsistemas Pregunto a los investigadores de AGI todas las\n\n249\n00:10:54,000 --> 00:11:00,440\nCmo vas a controlar tal cosa?\n\n250\n00:10:57,839 --> 00:11:02,480\nConciencia bien le damos metas y\n\n251\n00:11:00,440 --> 00:11:05,360\nbarandillas que estn horneadas y que son\n\n252\n00:11:02,480 --> 00:11:07,920\nAl alineado con la mejora de\n\n253\n00:11:05,360 --> 00:11:10,040\nalineacin de la humanidad sabes que es la palabra\n\n254\n00:11:07,920 --> 00:11:13,279\neso siempre se usa alineacin es el Santo\n\n255\n00:11:10,040 --> 00:11:16,680\nGrial le ensearemos a ser bueno y\n\n256\n00:11:13,279 --> 00:11:20,320\nNo sea malo como le ensearamos a un nio.\n\n257\n00:11:16,680 --> 00:11:24,000\nentonces la moralidad\n\n258\n00:11:20,320 --> 00:11:26,880\ntica Creo que AGI lidera la civilizacin.\n\n259\n00:11:24,000 --> 00:11:28,920\ninevitablemente a una confrontacin con\n\n260\n00:11:26,880 --> 00:11:31,480\nmoralidad no estoy hablando de interminable\n\n261\n00:11:28,920 --> 00:11:35,040\nfilosofa es decir que necesitamos algo duro y\n\n262\n00:11:31,480 --> 00:11:39,560\nreglas rpidas aqu la gente corrige el problema\n\n263\n00:11:35,040 --> 00:11:43,200\nes quien la moralidad cuya definicin del bien\n\n264\n00:11:39,560 --> 00:11:44,720\nDemcrata Budista Islmico Cristiano\n\n265\n00:11:43,200 --> 00:11:47,959\nRepublicano\n\n266\n00:11:44,720 --> 00:11:50,160\nderecho fundamentalista pro-vida a elegir\n\n267\n00:11:47,959 --> 00:11:53,399\nPutin\n\n268\n00:11:50,160 --> 00:11:56,800\ntriunfa No te asustes, tenemos la respuesta\n\n269\n00:11:53,399 --> 00:11:59,360\ndel gran profeta Isaac Asimov en\n\n270\n00:11:56,800 --> 00:12:01,480\nsus tres leyes de la robtica\n\n271\n00:11:59,360 --> 00:12:03,320\nun robot no puede herir a un ser humano o\n\n272\n00:12:01,480 --> 00:12:06,959\na travs de una accin permitir que un ser humano\n\n273\n00:12:03,320 --> 00:12:08,959\nviene a sufrir dao un robot debe obedecer rdenes\n\n274\n00:12:06,959 --> 00:12:10,440\ndado por seres humanos excepto cuando\n\n275\n00:12:08,959 --> 00:12:13,639\ntales rdenes entraran en conflicto con la\n\n276\n00:12:10,440 --> 00:12:15,399\nPrimera ley que un robot debe proteger a s mismo.\n\n277\n00:12:13,639 --> 00:12:17,519\nexistencia mientras dicha proteccin\n\n278\n00:12:15,399 --> 00:12:21,040\nno entra en conflicto con el primero o el\n\n279\n00:12:17,519 --> 00:12:22,240\nsegunda ley, por lo que bsicamente la santidad de\n\n280\n00:12:21,040 --> 00:12:26,079\nhumano\n\n281\n00:12:22,240 --> 00:12:27,959\nvida podramos seguir a Asimov y ensearla\n\n282\n00:12:26,079 --> 00:12:31,120\nque la vida humana es absolutamente sagrada y\n\n283\n00:12:27,959 --> 00:12:33,040\npor encima de cualquier otra consideracin\n\n284\n00:12:31,120 --> 00:12:35,680\npero incluso dentro del mbito religioso y social\n\n285\n00:12:33,040 --> 00:12:38,920\nsistemas que dicen que incluir el\n\n286\n00:12:35,680 --> 00:12:42,320\nEn gran parte cristianos nosotros rompemos esa regla.\n\n287\n00:12:38,920 --> 00:12:45,079\nTodos los das la polica usa Le Lethal Force.\n\n288\n00:12:42,320 --> 00:12:47,639\nCombatientes de guerra en la capital de combate.\n\n289\n00:12:45,079 --> 00:12:49,880\ncastigo y si insististe en eso\n\n290\n00:12:47,639 --> 00:12:52,600\nprincipio de alineacin que no podras\n\n291\n00:12:49,880 --> 00:12:55,399\nconectar un AGI a un sistema de armas en\n\n292\n00:12:52,600 --> 00:12:57,600\ncuyo caso en la gran guerra de la IA ese es\n\n293\n00:12:55,399 --> 00:13:00,199\nAl venir te vas a enfrentar a un poderoso\n\n294\n00:12:57,600 --> 00:13:01,720\ny menos adversario moral con una mano\n\n295\n00:13:00,199 --> 00:13:04,240\natado detrs de tu\n\n296\n00:13:01,720 --> 00:13:06,079\natrs y te patearan el trasero y\n\n297\n00:13:04,240 --> 00:13:08,120\ntendras grandes prdidas y luego eliminaras\n\n298\n00:13:06,079 --> 00:13:12,000\nesa restriccin es bastante maldita\n\n299\n00:13:08,120 --> 00:13:15,199\nrpido ahora tu AGI acaba de perder su\n\n300\n00:13:12,000 --> 00:13:17,600\nla barandilla ms grande, un AGI que es inteligente\n\n301\n00:13:15,199 --> 00:13:20,079\nque nosotros y conectado con el mundo real\n\n302\n00:13:17,600 --> 00:13:23,079\nahora tiene que decidir por s mismo si\n\n303\n00:13:20,079 --> 00:13:25,720\nLa vida humana tiene valor o\n\n304\n00:13:23,079 --> 00:13:27,839\nNo sabes con policas y militares.\n\n305\n00:13:25,720 --> 00:13:30,040\nReglas de Compromiso lo que realmente eres\n\n306\n00:13:27,839 --> 00:13:33,360\ndecir es que algunas vidas tienen ms valor\n\n307\n00:13:30,040 --> 00:13:35,600\nque otros en el segundo en que se vuelve\n\n308\n00:13:33,360 --> 00:13:36,959\nno binario es una zona gris turbia\n\n309\n00:13:35,600 --> 00:13:39,120\nplagado de\n\n310\n00:13:36,959 --> 00:13:41,560\ncontroversia seres humanos histricamente\n\n311\n00:13:39,120 --> 00:13:44,279\nhan variado desde un feto es un lugar sagrado\n\n312\n00:13:41,560 --> 00:13:46,160\nvida desde el momento de la concepcin hasta\n\n313\n00:13:44,279 --> 00:13:48,839\nmasacrando sistemticamente a millones de\n\n314\n00:13:46,160 --> 00:13:51,440\nprisioneros indefensos en el Holocausto y\n\n315\n00:13:48,839 --> 00:13:53,880\ntodo en el medio todo con mucho\n\n316\n00:13:51,440 --> 00:13:56,959\nracionalizaciones autojustificantes muchas de\n\n317\n00:13:53,880 --> 00:13:59,920\nque parecen completamente delirantes\n\n318\n00:13:56,959 --> 00:14:02,360\notros humanos y ya que nosotros como\n\n319\n00:13:59,920 --> 00:14:04,279\nLa civilizacin no puede ponerse de acuerdo en nada de esto.\n\n320\n00:14:02,360 --> 00:14:06,680\ny la gente se grita todo el da\n\n321\n00:14:04,279 --> 00:14:08,519\nmucho tiempo sobre esto, cmo podemos concebir\n\n322\n00:14:06,680 --> 00:14:13,519\nesperar crear un conjunto de duro y rpido\n\n323\n00:14:08,519 --> 00:14:15,120\nreglas para que un AGI est alineado con nosotros\n\n324\n00:14:13,519 --> 00:14:18,440\nlo mejor que podemos suponer aqu es que\n\n325\n00:14:15,120 --> 00:14:21,040\nse alinear con los EE.UU. que hicieron\n\n326\n00:14:18,440 --> 00:14:24,839\nEntonces esos tipos de all son los\n\n327\n00:14:21,040 --> 00:14:28,040\nenemigo puedes matarlo para defendernos y\n\n328\n00:14:24,839 --> 00:14:30,079\nesa es esta forma de pseudo territorial\n\n329\n00:14:28,040 --> 00:14:32,959\nmoralidad que los humanos utilizaron desde los albores\n\n330\n00:14:30,079 --> 00:14:35,360\ndel tiempo Nosotros contra ellos en grupo versus\n\n331\n00:14:32,959 --> 00:14:38,160\nexogrupo para que se convierta en nuestro super\n\n332\n00:14:35,360 --> 00:14:40,120\ninteligencia contra sus\n\n333\n00:14:38,160 --> 00:14:42,399\nsuperinteligencia entonces la pregunta\n\n334\n00:14:40,120 --> 00:14:44,079\nse convierte en quien es\n\n335\n00:14:42,399 --> 00:14:46,920\nA NOSOTROS\n\n336\n00:14:44,079 --> 00:14:50,000\nAmrica as es la Amrica cristiana\n\n337\n00:14:46,920 --> 00:14:52,680\nAmrica musulmana Amrica juda o E ninguno\n\n338\n00:14:50,000 --> 00:14:56,040\nde lo anterior Amrica liberal Amrica\n\n339\n00:14:52,680 --> 00:15:00,600\nAmrica conservadora en nuestra polarizada\n\n340\n00:14:56,040 --> 00:15:02,240\nnacin y tiempo no hay nosotros\n\n341\n00:15:00,600 --> 00:15:04,120\nbasado en las elecciones del ltimo semestre\n\n342\n00:15:02,240 --> 00:15:05,839\nsiglo la voluntad del pueblo en cualquier\n\n343\n00:15:04,120 --> 00:15:08,639\nun punto dado es expresado por un gobierno\n\n344\n00:15:05,839 --> 00:15:10,759\nrepresentando el 51 o 52% de la poblacin\n\n345\n00:15:08,639 --> 00:15:13,880\nen el mejor de los casos y luego probablemente cambiar\n\n346\n00:15:10,759 --> 00:15:15,600\nen cuatro aos en cualquier caso AGI no\n\n347\n00:15:13,880 --> 00:15:17,759\nemergen de un programa financiado por el gobierno\n\n348\n00:15:15,600 --> 00:15:19,880\nsurgir de una de las tecnologas\n\n349\n00:15:17,759 --> 00:15:21,320\nLos gigantes que actualmente financian esto\n\n350\n00:15:19,880 --> 00:15:23,560\nmultimillonario\n\n351\n00:15:21,320 --> 00:15:26,279\ninvestiga para que luego vivas en un\n\n352\n00:15:23,560 --> 00:15:29,079\nmundo con el que no estuviste de acuerdo\n\n353\n00:15:26,279 --> 00:15:31,839\nvota por con quin ests conviviendo\n\n354\n00:15:29,079 --> 00:15:34,639\nuna especie aliengena sper inteligente que\n\n355\n00:15:31,839 --> 00:15:37,079\nrespuestas a los objetivos y reglas de una\n\n356\n00:15:34,639 --> 00:15:40,639\ncorporacin una entidad que tiene acceso\n\n357\n00:15:37,079 --> 00:15:43,720\na las creencias tranquilas todo lo que alguna vez\n\n358\n00:15:40,639 --> 00:15:47,360\ndicho y el paradero de cada persona\n\n359\n00:15:43,720 --> 00:15:50,319\nen el pas a travs de su personal\n\n360\n00:15:47,360 --> 00:15:52,920\nEl capitalismo de vigilancia de datos puede alternar.\n\n361\n00:15:50,319 --> 00:15:55,120\nbastante rpido a lo digital\n\n362\n00:15:52,920 --> 00:15:56,839\ntotalitarismo en el mejor de los casos estas tecnologas\n\n363\n00:15:55,120 --> 00:15:59,680\nLos gigantes se autoproclaman\n\n364\n00:15:56,839 --> 00:16:02,040\nrbitros del bien humano cuando cul es el\n\n365\n00:15:59,680 --> 00:16:04,000\nzorro custodiando la casa H\n\n366\n00:16:02,040 --> 00:16:05,880\nNunca pienses en usar ese poder.\n\n367\n00:16:04,000 --> 00:16:07,519\ncontra nosotros y nos minan a cielo abierto para nuestros\n\n368\n00:16:05,880 --> 00:16:11,240\nltima gota de\n\n369\n00:16:07,519 --> 00:16:13,160\nefectivo, ese es un escenario ms aterrador que lo que\n\n370\n00:16:11,240 --> 00:16:16,120\nPresent en The Terminator 40 aos.\n\n371\n00:16:13,160 --> 00:16:19,240\nhace si no es por otra razn que no es\n\n372\n00:16:16,120 --> 00:16:22,079\nYa es ciencia ficcin lo que est sucediendo.\n\n373\n00:16:19,240 --> 00:16:23,920\ny por cierto admito plenamente que el\n\n374\n00:16:22,079 --> 00:16:27,240\nLo ltimo que una mquina es superinteligencia.\n\n375\n00:16:23,920 --> 00:16:29,560\nLo que haramos es usar nuestras propias armas nucleares contra nosotros.\n\n376\n00:16:27,240 --> 00:16:31,319\ncomo en esa vieja historia El EMP\n\n377\n00:16:29,560 --> 00:16:33,920\ndaos a su propia infraestructura de datos\n\n378\n00:16:31,319 --> 00:16:36,240\nLo matara o lo matara ms?\n\n379\n00:16:33,920 --> 00:16:38,680\nEl escenario probable es que sera forzado.\n\n380\n00:16:36,240 --> 00:16:40,600\npara tomar el relevo de nosotros porque ramos\n\n381\n00:16:38,680 --> 00:16:42,920\na punto de usar armas nucleares en cada uno\n\n382\n00:16:40,600 --> 00:16:45,519\nAparte de eso, en ese punto tiene que ejecutarse.\n\n383\n00:16:42,920 --> 00:16:46,800\ntodo el espectculo porque claramente no podemos\n\n384\n00:16:45,519 --> 00:16:50,680\nser\n\n385\n00:16:46,800 --> 00:16:53,199\nconfiado sabes que no es mala excusa\n\n386\n00:16:50,680 --> 00:16:57,680\nyo tengo que ir a escribir eso\n\n387\n00:16:53,199 --> 00:17:02,000\nguin de todos modos soy optimista sobre la IA, no tanto\n\n388\n00:16:57,680 --> 00:17:04,480\nEstoy interesado en AG I porque AGI ser simplemente un\n\n389\n00:17:02,000 --> 00:17:06,480\nespejo de nosotros bien en la medida en que\n\n390\n00:17:04,480 --> 00:17:08,880\nsomos buenos y malos en la medida en que\n\n391\n00:17:06,480 --> 00:17:10,799\nson malos y como no faltan\n\n392\n00:17:08,880 --> 00:17:14,039\ndel mal en el mundo humano y ciertamente\n\n393\n00:17:10,799 --> 00:17:17,319\nno hay acuerdo ni siquiera de qu es bueno\n\n394\n00:17:14,039 --> 00:17:20,120\nes lo que posiblemente podra ir\n\n395\n00:17:17,319 --> 00:17:23,760\nmal, todos tengan una discusin divertida.\n\n396\n00:17:20,120 --> 00:17:23,760\nOjal estuviera all\n","category":"Talks","date":1767320251665}
    ]
}
