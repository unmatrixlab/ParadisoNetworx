{
    "videos": [
        {"id":"8Pfa8kPjUio","title":"A conversation with NVIDIAâ€™s Jensen Huang","srtEn":"1\n00:00:11,909 --> 00:00:15,512\nWelcome back to the stage,\nPatrick Collison.\n\n2\n00:00:28,792 --> 00:00:30,494\n-[Patrick] All right.\nGood afternoon, folks.\n\n3\n00:00:30,594 --> 00:00:33,697\nI hope you've enjoyed\nthe Sessions between now\n\n4\n00:00:33,830 --> 00:00:36,233\nand when we last saw you this morning.\n\n5\n00:00:36,800 --> 00:00:40,737\nFor this afternoon's keynote,\nor fireside chat, I suppose,\n\n6\n00:00:40,871 --> 00:00:44,074\nI'm about to introduce somebody\nwho needs little introduction.\n\n7\n00:00:44,207 --> 00:00:48,211\nAlthough a fun fact that you may not know\nabout Jensen Huang is that\n\n8\n00:00:48,312 --> 00:00:51,582\nhe's been a CEO of NVIDIA\nfor 31 years this month,\n\n9\n00:00:51,682 --> 00:00:55,586\nmaking him the longest-serving CEO\nin the technology industry\n\n10\n00:00:55,686 --> 00:00:58,755\nand therefore logically--\n\n11\n00:01:03,327 --> 00:01:06,563\nJohn and I have only been doing it \nfor a mere 14 years.\n\n12\n00:01:06,697 --> 00:01:11,168\nSo, even if we double that,\nwe'll still be second to him.\n\n13\n00:01:12,836 --> 00:01:15,572\nJensen, well, we'll talk\nabout this on stage,\n\n14\n00:01:15,906 --> 00:01:20,310\nattended the Oneida Baptist Institute\nin Kentucky.\n\n15\n00:01:20,911 --> 00:01:22,379\nWe'll definitely be asking him about it.\n\n16\n00:01:23,880 --> 00:01:30,020\nOregon State, worked as \na waiter at Denny's.\n\n17\n00:01:30,320 --> 00:01:32,623\nThere's a Denny's close to here, actually.\n\n18\n00:01:32,723 --> 00:01:35,892\nLSI Logic and then AMD,\nwhich is of course\n\n19\n00:01:35,993 --> 00:01:39,963\nnow run by his first cousin once removed.\n\n20\n00:01:40,130 --> 00:01:41,932\nWe'll definitely be asking about that.\n\n21\n00:01:42,265 --> 00:01:45,435\nBefore he founded NVIDIA in 1993...\n\n22\n00:01:45,602 --> 00:01:53,343\nand NVIDIA's market cap was $8 billion\nwhen Stripe launched in 2011.\n\n23\n00:01:53,710 --> 00:01:59,016\nAnd it is now, of course, more than \n200 times that. So he's been busy since.\n\n24\n00:01:59,282 --> 00:02:01,451\nPlease welcome to the stage,\nJensen Huang.\n\n25\n00:02:16,867 --> 00:02:18,235\n-[Jensen] Hey, everybody!\n\n26\n00:02:20,037 --> 00:02:22,773\n-[Patrick] So you watched \nthe keynote earlier?\n\n27\n00:02:22,873 --> 00:02:26,910\n-I did. I've never seen a duet before.\n-[Patrick laughing] So, well--\n\n28\n00:02:27,010 --> 00:02:30,881\n-I've never seen a duet before. \nYou were so synchronized.\n\n29\n00:02:31,081 --> 00:02:35,252\nIt seemed like the two of you knew\neach other. It's incredible.\n\n30\n00:02:35,686 --> 00:02:38,088\n-Some acquaintance.\n\n31\n00:02:38,188 --> 00:02:40,891\nYou've been doing keynotes a long time.\n\n32\n00:02:40,991 --> 00:02:44,628\nYou are the keynote goat.\nSo give us--\n\n33\n00:02:44,995 --> 00:02:46,163\n-Stop it.\n\n34\n00:02:46,263 --> 00:02:49,633\n-Give us your, like...\nwe don't have even a signature outfit yet.\n\n35\n00:02:49,766 --> 00:02:53,270\nWe're just amateurs here. \nSo give us--\n\n36\n00:02:53,837 --> 00:02:55,505\n-It's because you're still young.\n\n37\n00:02:56,873 --> 00:03:00,110\n-Well, give us your keynote performance \nreview. What did you think?\n\n38\n00:03:00,210 --> 00:03:05,282\n-I thought it was A+.\nI thought it was A+. I was... really!\n\n39\n00:03:06,850 --> 00:03:10,921\nYou explained perfectly\n\n40\n00:03:11,021 --> 00:03:15,759\nthe purpose of the company. \nWhat inspires you guys.\n\n41\n00:03:15,892 --> 00:03:18,695\nWhat keeps you guys up.\nWhat makes you work so hard.\n\n42\n00:03:18,962 --> 00:03:22,833\nThe ecosystem that you serve, \nthe incredible platform you built,\n\n43\n00:03:22,999 --> 00:03:26,436\nthe amazing contribution you make \nto the world's economy.\n\n44\n00:03:26,536 --> 00:03:28,238\nIt's incredible. I thought it was great!\n\n45\n00:03:28,338 --> 00:03:31,908\nAnd there was a whole bunch of technology \nstuff, feature stuff, money stuff.\n\n46\n00:03:32,142 --> 00:03:35,512\nI didn't understand any of that, but...\n\n47\n00:03:35,612 --> 00:03:39,616\nsomething about a CYK or something,\nwhat was that?\n\n48\n00:03:39,716 --> 00:03:43,053\n-A KYC.\n-KYC, yeah.\n\n49\n00:03:43,186 --> 00:03:44,187\nI thought it was--\n\n50\n00:03:44,321 --> 00:03:45,489\n-It's a big deal in our world.\n\n51\n00:03:45,589 --> 00:03:47,457\n-Is that right?\nKentucky Fried Chicken?\n\n52\n00:03:48,191 --> 00:03:52,395\n-We take care of KYC so that you can\nassociate us with Kentucky Fried Chicken.\n\n53\n00:03:52,529 --> 00:03:53,730\n-Okay. Got it.\n\n54\n00:03:54,131 --> 00:03:57,667\n-Software-defined financial services,\n\n55\n00:03:57,768 --> 00:04:00,537\nthis idea, did that make sense to you?\n\n56\n00:04:00,637 --> 00:04:02,873\n-Well, first of all,\nI think it's a giant idea.\n\n57\n00:04:03,640 --> 00:04:05,308\n-Do you know where it came from?\n\n58\n00:04:07,410 --> 00:04:09,112\n-You're going to tell me?\n\n59\n00:04:09,980 --> 00:04:12,115\n-So Jensen and I were catching up...\n\n60\n00:04:12,215 --> 00:04:16,219\n-The part that\nI loved was how you realized\n\n61\n00:04:16,319 --> 00:04:22,959\nin the very beginning that financial\npayments  was about code, not finance.\n\n62\n00:04:23,059 --> 00:04:24,661\nI thought that was incredible.\n\n63\n00:04:24,761 --> 00:04:27,097\nAnd you explained that \nthe first time we met.\n\n64\n00:04:27,197 --> 00:04:29,766\nSo, Jensen and I were catching up\n18 months ago or so,\n\n65\n00:04:29,900 --> 00:04:32,702\nand I guess it was a couple\nof years since we had last spoken.\n\n66\n00:04:32,836 --> 00:04:36,506\nAnd so he was asking for the update\non Stripe, and I was explaining.\n\n67\n00:04:36,606 --> 00:04:43,013\nAnd you said, \"Oh, so it's like software-\ndefined networking, but for money.\"\n\n68\n00:04:43,113 --> 00:04:46,683\nAnd that was still ricocheting around\nin my mind.\n\n69\n00:04:46,817 --> 00:04:50,821\nThat's where we got to this idea for\nsoftware-defined financial services.\n\n70\n00:04:50,954 --> 00:04:52,756\nSo I hope we don't have to pay\na licensing fee for that.\n\n71\n00:04:52,889 --> 00:04:54,691\n-I got zero equity for that good idea.\n\n72\n00:04:54,825 --> 00:04:56,726\n-All right.\n\n73\n00:04:56,827 --> 00:04:58,562\nYou guys are doing okay.\nI was thinking about this.\n\n74\n00:04:58,695 --> 00:05:01,131\nTesla's earnings were reported yesterday,\n\n75\n00:05:01,231 --> 00:05:06,269\nand Elon announced that Tesla\nis going to have 85,000\n\n76\n00:05:06,570 --> 00:05:09,272\nA100s by the end of this year.\n\n77\n00:05:09,406 --> 00:05:12,142\nAnd I was just reflecting on--\n\n78\n00:05:12,242 --> 00:05:14,110\nit's quite a success to sort\nof build a business\n\n79\n00:05:14,211 --> 00:05:17,747\nwhere CEOs kind of compete with\neach other to announce\n\n80\n00:05:17,848 --> 00:05:20,050\nwho has spent more buying your product.\n\n81\n00:05:20,183 --> 00:05:22,652\nSo I think you've done something\nquite impressive.\n\n82\n00:05:22,786 --> 00:05:24,754\nBut anyway, I actually want to start out\ntalking a little bit about--\n\n83\n00:05:24,888 --> 00:05:28,859\n-All of my CEO friends,\nthey all have the most.\n\n84\n00:05:32,229 --> 00:05:35,866\n-So I want to start out talking\na little bit about,\n\n85\n00:05:36,733 --> 00:05:41,771\na remark you made at a Stanford event\nrecently, at the GSB, I think.\n\n86\n00:05:41,938 --> 00:05:49,379\nAnd you said, \"I wish upon you\nample doses of pain and suffering.\"\n\n87\n00:05:50,614 --> 00:05:51,615\nElaborate.\n\n88\n00:05:53,917 --> 00:06:00,357\n-Well. Let's see. \nThere is a misunderstanding.\n\n89\n00:06:01,224 --> 00:06:02,559\nThere's a phrase that said,\n\n90\n00:06:02,659 --> 00:06:08,632\n\"You should choose your career \nbased on your passion.\"\n\n91\n00:06:08,732 --> 00:06:13,536\nUsually people connect passion\nwith happiness.\n\n92\n00:06:14,604 --> 00:06:17,641\nAnd I think there's something\nmissing in that.\n\n93\n00:06:17,741 --> 00:06:20,911\nNothing there is wrong,\nbut there's something missing.\n\n94\n00:06:21,044 --> 00:06:23,847\nAnd the reason for that is because\nif you want to do great things,\n\n95\n00:06:23,947 --> 00:06:27,684\nand I know this to be true about\nyou creating Stripe--\n\n96\n00:06:27,784 --> 00:06:32,522\nAnd by the way,\nthis is one of the world's finest CEOs,\n\n97\n00:06:32,622 --> 00:06:34,457\nyoung as he may be, yep.\n\n98\n00:06:37,894 --> 00:06:41,665\nYou guys know, I've met a lot of CEOs,\nI've heard about a lot of companies,\n\n99\n00:06:41,798 --> 00:06:45,402\nand this is genuinely one of the world's\ngreat visionary companies.\n\n100\n00:06:45,502 --> 00:06:48,038\nAnd so anyways,\nI just want to say that.\n\n101\n00:06:48,204 --> 00:06:50,707\nIt's the reason why I just love what...\n\n102\n00:06:50,807 --> 00:06:53,209\n-No more compliments allowed,\nit makes us terribly uncomfortable.\n\n103\n00:06:53,343 --> 00:06:56,546\n-I know, I could tell.\nI could see him, he's starting to sweat.\n\n104\n00:06:57,981 --> 00:07:03,253\nAnd so the thing is, when you want\nto build something great,\n\n105\n00:07:03,353 --> 00:07:05,722\nit's not easy to do.\n\n106\n00:07:05,822 --> 00:07:07,590\nAnd when you're doing something \nthat's not easy to do,\n\n107\n00:07:07,724 --> 00:07:10,460\nyou're not always enjoying it.\n\n108\n00:07:10,560 --> 00:07:13,229\nI don't love every day of my job.\n\n109\n00:07:13,363 --> 00:07:15,432\nI don't think every day brings me joy,\n\n110\n00:07:15,532 --> 00:07:19,269\nnor does joy have to be\nthe definition of a good day.\n\n111\n00:07:19,369 --> 00:07:21,237\nAnd every day I'm not happy.\n\n112\n00:07:21,338 --> 00:07:23,573\nEvery year I'm not happy\nabout the company.\n\n113\n00:07:23,773 --> 00:07:26,876\nBut I love the company\nevery single second.\n\n114\n00:07:27,677 --> 00:07:31,247\nAnd so I think that what\npeople misunderstand is\n\n115\n00:07:31,381 --> 00:07:35,986\nsomehow the best jobs are the ones that\nbrings you happiness all the time.\n\n116\n00:07:36,119 --> 00:07:41,324\nI don't think that that's right. \nYou have to  suffer.\n\n117\n00:07:41,458 --> 00:07:44,961\nYou have to struggle. \nYou have to endeavor.\n\n118\n00:07:45,095 --> 00:07:46,796\nYou have to do those hard things\n\n119\n00:07:46,930 --> 00:07:51,568\nand work through it in order to really\nappreciate what you've done.\n\n120\n00:07:52,102 --> 00:07:56,006\nAnd there are no such things that\nare great that were easy to do.\n\n121\n00:07:56,473 --> 00:08:01,211\nAnd so by definition,\nI would say therefore,\n\n122\n00:08:01,311 --> 00:08:05,749\nI wish upon you greatness, which,\nby my way of saying it,\n\n123\n00:08:05,882 --> 00:08:09,419\nI wish upon you plenty of pain \nand suffering.\n\n124\n00:08:16,192 --> 00:08:19,629\n-Anything in your upbringing\nthat taught you that idea,\n\n125\n00:08:19,763 --> 00:08:22,465\nor is it just somehow innate\nto your makeup?\n\n126\n00:08:29,305 --> 00:08:31,441\n-I didn't realize I had\nto lay down for this.\n\n127\n00:08:36,146 --> 00:08:39,315\nI'm about to tell you things I've never\ntold anyone. Not even my family.\n\n128\n00:08:43,053 --> 00:08:48,324\nI was an immigrant. \nAnd when I came in 1973, I was nine.\n\n129\n00:08:48,425 --> 00:08:50,760\nMy older brother was almost 11.\n\n130\n00:08:50,894 --> 00:08:57,600\nAnd this was a foreign country \nand there was nothing easy about that.\n\n131\n00:08:57,734 --> 00:09:04,674\nAnd we also grew up with really,\nreally terrific parents.\n\n132\n00:09:04,774 --> 00:09:09,179\nBut we weren't wealthy. And so they\nworked hard, they work hard today.\n\n133\n00:09:09,512 --> 00:09:13,917\nAnd so they passed along \na lot of life lessons by working hard.\n\n134\n00:09:14,184 --> 00:09:16,820\nI had all kinds of jobs\n\n135\n00:09:16,920 --> 00:09:23,359\nand we went to a school \nthat included a lot of chores.\n\n136\n00:09:23,460 --> 00:09:24,461\n-It's in Kentucky?\n\n137\n00:09:24,561 --> 00:09:28,098\n-Yeah, Kentucky. \nOneida Baptist Institute.\n\n138\n00:09:28,198 --> 00:09:33,970\nI don't think it's the same as 'MIT', \nthat 'I' is not the same.\n\n139\n00:09:34,070 --> 00:09:39,075\nIt's the same word, but it's different.\nIt's a different type of institute.\n\n140\n00:09:39,476 --> 00:09:44,047\nBut my institute required you to go\nto school, and it was a dormitory,\n\n141\n00:09:44,180 --> 00:09:45,548\nand so there were a lot of chores.\n\n142\n00:09:45,782 --> 00:09:47,383\nI was the youngest kid in school,\n\n143\n00:09:47,517 --> 00:09:49,886\nand so all of the other kids\ngot the hard work.\n\n144\n00:09:49,986 --> 00:09:53,690\nThey had to work in the tobacco farm,\nand I got the easy job.\n\n145\n00:09:53,823 --> 00:09:57,660\nI was nine years old, and so after they\nleft, I had to clean all the bathrooms.\n\n146\n00:09:58,628 --> 00:10:02,332\nI never felt that I got the easy job,\n\n147\n00:10:02,465 --> 00:10:07,070\nbecause what they left behind was... \nYou can't unsee that kind of stuff.\n\n148\n00:10:09,072 --> 00:10:12,008\nBut that was my job and\nso I did it delightfully.\n\n149\n00:10:12,108 --> 00:10:17,046\nAnd then I had plenty of other jobs,\nand Denny's was one of them.\n\n150\n00:10:17,147 --> 00:10:21,050\nAnd I started out as a dishwasher and\nbecame a busboy and became a waiter.\n\n151\n00:10:21,151 --> 00:10:23,887\nAnd I loved every one of them.\nI loved every one of them.\n\n152\n00:10:23,987 --> 00:10:28,825\nSomehow, I've always found...\n\n153\n00:10:30,160 --> 00:10:31,961\nI want to say joy, \nbut that's not quite right.\n\n154\n00:10:32,095 --> 00:10:37,600\nJust everything that I was doing, \nI wanted to do the best I could.\n\n155\n00:10:37,700 --> 00:10:40,136\nAnd maybe that was kind of ingrained\nfrom the very beginning,\n\n156\n00:10:40,236 --> 00:10:45,475\nbut I was definitely the best\nbathroom cleaner the world's ever seen.\n\n157\n00:10:45,575 --> 00:10:47,243\nI'm sure of it. Yeah.\n\n158\n00:10:47,343 --> 00:10:53,516\n-So if we fast forward just a little bit, \nto the NVIDIA of today,\n\n159\n00:10:53,616 --> 00:10:55,285\nhow large is your leadership team?\n\n160\n00:10:55,618 --> 00:10:58,354\n-How large is...?\n-Your leadership team.\n\n161\n00:10:59,722 --> 00:11:04,260\n-NVIDIA's leadership team is 60+ people.\n\n162\n00:11:04,394 --> 00:11:05,962\n-And they all report to you?\n\n163\n00:11:06,062 --> 00:11:08,364\n-Yeah, they all report to me.\n-60 direct reports?\n\n164\n00:11:08,464 --> 00:11:09,832\n-60 direct reports. Yeah.\n\n165\n00:11:09,933 --> 00:11:13,436\n-Which is not conventionally considered\na best practice.\n\n166\n00:11:15,505 --> 00:11:17,740\nI agree that the best practice...\n\n167\n00:11:18,041 --> 00:11:19,609\n-I'm certain that's the best practice.\n\n168\n00:11:19,742 --> 00:11:23,546\nIt's not conventional, but I am certain\nit's the best practice.\n\n169\n00:11:27,450 --> 00:11:31,254\nAnd by the end of this,\nI'm going to convince all of you\n\n170\n00:11:31,354 --> 00:11:33,456\nto have 60 people on your direct reports.\n\n171\n00:11:34,290 --> 00:11:35,491\n-The floor is yours.\n\n172\n00:11:36,626 --> 00:11:38,661\n-The reason... first of all,\n\n173\n00:11:38,761 --> 00:11:43,833\nthe reason is because the layer of\nhierarchy in your company really matters.\n\n174\n00:11:43,933 --> 00:11:46,202\nInformation really matters.\n\n175\n00:11:46,302 --> 00:11:51,107\nI believe that your contribution to...\n\n176\n00:11:51,241 --> 00:11:56,179\nthe work should not be based on the\nprivileged access to information.\n\n177\n00:11:57,280 --> 00:12:02,352\nI don't do one-on-ones and I don't...\nMy staff is quite large,\n\n178\n00:12:02,952 --> 00:12:06,022\nand almost everything that I say,\nI say to everybody all at the same time.\n\n179\n00:12:06,589 --> 00:12:08,324\nAnd the reason for that is because\n\n180\n00:12:08,591 --> 00:12:11,661\nI don't really believe there's any\ninformation that I operate on,\n\n181\n00:12:11,794 --> 00:12:15,131\nthat somehow only one or two people\nshould hear about.\n\n182\n00:12:15,231 --> 00:12:17,900\nAnd these are the challenges\nof the company\n\n183\n00:12:18,001 --> 00:12:19,068\nor this is the problem\nI'm trying to solve,\n\n184\n00:12:19,168 --> 00:12:21,237\nor this is the direction\nwe're trying to go into.\n\n185\n00:12:21,337 --> 00:12:23,039\nThese are the new endeavors.\n\n186\n00:12:23,139 --> 00:12:24,907\nThis isn't working. That's working well.\n\n187\n00:12:25,008 --> 00:12:29,946\nAnd so all of this type of information\neverybody should be able to hear it.\n\n188\n00:12:30,046 --> 00:12:33,650\nI love that everybody is\nworking off of the same song sheet.\n\n189\n00:12:33,750 --> 00:12:37,253\nI love that there is no privileged access\nto information.\n\n190\n00:12:37,353 --> 00:12:40,890\nI love that we're able to all contribute\nto solving a problem.\n\n191\n00:12:41,024 --> 00:12:44,560\nAnd when you have 60 people in a room\nand oftentimes...\n\n192\n00:12:44,661 --> 00:12:47,830\nWell my staff meetings are once\nevery other week,\n\n193\n00:12:47,930 --> 00:12:51,701\nand it's all based on issues,\nwhatever issues we have.\n\n194\n00:12:51,801 --> 00:12:54,404\nEverybody's there working\non it at the same time,\n\n195\n00:12:54,504 --> 00:12:58,041\neverybody heard\nthe reasoning of the problem.\n\n196\n00:12:58,174 --> 00:13:01,911\nEverybody heard the reasoning of the\nsolution. Everybody heard everything.\n\n197\n00:13:02,011 --> 00:13:04,847\nAnd so that empowers people.\n\n198\n00:13:04,947 --> 00:13:07,317\nI believe that when you give\neverybody equal access to information,\n\n199\n00:13:07,417 --> 00:13:11,287\nit empowers people.\nAnd so that's number one, empowering.\n\n200\n00:13:11,387 --> 00:13:19,028\nNumber two,\nif the CEO's direct staff is 60 people,\n\n201\n00:13:19,162 --> 00:13:21,230\nthe number of layers you've\nremoved in a company\n\n202\n00:13:21,331 --> 00:13:24,167\nis probably something like seven, \ndepending on how it is.\n\n203\n00:13:24,267 --> 00:13:26,869\n-60 at every layer, or only 60?\n\n204\n00:13:26,969 --> 00:13:33,509\nAs in, if I'm one of the fortunate 60,\ndo I also have 60 direct reports?\n\n205\n00:13:33,609 --> 00:13:37,013\n-No. I also don't think\nthat that's scalable downward.\n\n206\n00:13:37,113 --> 00:13:42,185\nAnd the reason for that is because\nyou need more and more supervision,\n\n207\n00:13:42,318 --> 00:13:45,421\ndepending on certain levels.\n\n208\n00:13:45,521 --> 00:13:48,658\nAnd at the E-Staff level,\n\n209\n00:13:48,758 --> 00:13:52,095\nif you're so unfortunate to be serving\non NVIDIA's E-Staff,\n\n210\n00:13:52,195 --> 00:13:57,600\nit's very unlikely\nyou need a lot of managerial.\n\n211\n00:13:57,700 --> 00:14:02,839\n-And so I rarely find myself having \nto stand up for conventional wisdom.\n\n212\n00:14:02,939 --> 00:14:05,842\nBut if I were to kind of steel mine\nthe other side, I'd say, well,\n\n213\n00:14:05,942 --> 00:14:08,411\none-on-ones are where you\nprovide coaching,\n\n214\n00:14:08,511 --> 00:14:11,614\nwhere you maybe talk through goals\ntogether, personal goals,\n\n215\n00:14:11,748 --> 00:14:13,883\ncareer advancement, what have you.\n\n216\n00:14:14,183 --> 00:14:17,220\nWhere maybe you give feedback\non something that you see somebody\n\n217\n00:14:17,320 --> 00:14:18,888\nsystematically not doing\nso well and so forth.\n\n218\n00:14:18,988 --> 00:14:20,990\nSo there's all these things\nthat kind of one is, again,\n\n219\n00:14:21,124 --> 00:14:23,826\nconventionally supposed to do\nin the one-on-one.\n\n220\n00:14:23,960 --> 00:14:26,629\nDo you not do those things or \ndo you do them in a different way?\n\n221\n00:14:26,796 --> 00:14:29,932\n-Really good question. \nI do it right there.\n\n222\n00:14:31,167 --> 00:14:32,335\nI do it right there.\n\n223\n00:14:32,802 --> 00:14:35,505\nI give you feedback right there\nin front of everybody.\n\n224\n00:14:36,439 --> 00:14:39,375\nAnd in fact, this is really a big deal.\n\n225\n00:14:39,709 --> 00:14:43,713\nFirst of all, feedback is learning.\nFeedback is learning.\n\n226\n00:14:43,813 --> 00:14:47,283\nFor what reason are you \nthe only person who should learn this?\n\n227\n00:14:47,950 --> 00:14:50,620\nNow you created the conditions\n\n228\n00:14:51,387 --> 00:14:54,991\nbecause of some mistake that you made\n\n229\n00:14:55,158 --> 00:15:01,230\nor silliness that you brought\nupon yourself.\n\n230\n00:15:04,734 --> 00:15:07,570\nWe should all learn from that opportunity.\n\n231\n00:15:07,703 --> 00:15:10,473\nSo you created the conditions,\nbut we should all learn from it.\n\n232\n00:15:10,606 --> 00:15:12,375\nDoes that make sense?\n\n233\n00:15:12,475 --> 00:15:15,211\nAnd so for me to explain to you\nwhy that doesn't make sense\n\n234\n00:15:15,311 --> 00:15:18,581\nor how I differ from it--\nhalf the time I'm not right--\n\n235\n00:15:18,714 --> 00:15:21,818\nbut for me to reason through it\nin front of everybody\n\n236\n00:15:21,918 --> 00:15:24,353\nhelps everybody learn how\nto reason through it.\n\n237\n00:15:24,454 --> 00:15:29,592\nAnd so the issue-- the problem I have with\none-on-ones and taking feedback aside\n\n238\n00:15:29,692 --> 00:15:32,929\nis you deprive a whole bunch\nof people that same learning.\n\n239\n00:15:33,029 --> 00:15:36,499\nLearning from mistakes,\nother people's mistakes,\n\n240\n00:15:36,599 --> 00:15:37,767\nis the best way to learn.\n\n241\n00:15:37,867 --> 00:15:40,937\nWhy learn from your own mistakes?\n\n242\n00:15:41,037 --> 00:15:43,139\nWhy learn from your own embarrassment?\n\n243\n00:15:43,239 --> 00:15:44,807\nYou got to learn from other\npeople's embarrassment.\n\n244\n00:15:44,907 --> 00:15:48,244\nThat's why we have case studies.\nIsn't that right?\n\n245\n00:15:48,344 --> 00:15:52,215\nWe're trying to read from other people's\ndisasters, other people's tragedies.\n\n246\n00:15:52,315 --> 00:15:54,383\nNothing makes us happier than that.\n\n247\n00:15:56,352 --> 00:16:00,723\n-Have you succeeded in getting other\nleaders at NVIDIA to adopt this practice,\n\n248\n00:16:00,857 --> 00:16:02,258\nor is that difficult?\n\n249\n00:16:02,391 --> 00:16:05,061\n-I give people the opportunity\nto decide for themselves,\n\n250\n00:16:05,161 --> 00:16:07,396\nbut I really discourage one-on-ones.\n\n251\n00:16:07,497 --> 00:16:09,031\nI really discourage one-on-ones.\n\n252\n00:16:09,131 --> 00:16:13,302\nNothing is worse than\nthe idea that somebody says,\n\n253\n00:16:13,402 --> 00:16:16,672\n\"Oh, Jensen wants us to do this.\"\n\n254\n00:16:16,772 --> 00:16:20,109\nWhy does that have to be said to anybody?\nEverybody should know.\n\n255\n00:16:20,209 --> 00:16:25,281\nOr somebody said,\n\"That E-Staff said that.\"\n\n256\n00:16:25,381 --> 00:16:27,483\nNothing drives me nuttier than that.\n\n257\n00:16:28,718 --> 00:16:34,023\n-You once told me that you really didn't \nlike firing people and very seldom did it.\n\n258\n00:16:34,123 --> 00:16:35,791\nCan you elaborate on that?\n\n259\n00:16:36,959 --> 00:16:40,930\n-Well, I'd rather improve you\nthan give up on you.\n\n260\n00:16:41,030 --> 00:16:44,700\nWhen you fire somebody,\nyou're kind of saying--\n\n261\n00:16:45,334 --> 00:16:47,436\nWell, a lot of people say,\n\n262\n00:16:47,570 --> 00:16:51,974\n\"It wasn't your fault\" or\n\"I made the wrong choice\" or--\n\n263\n00:16:52,108 --> 00:16:55,111\nThere are very few jobs--\nLook, I used to clean bathrooms,\n\n264\n00:16:55,244 --> 00:16:59,148\nand now I'm a CEO of a company,\nI think you could learn it.\n\n265\n00:16:59,248 --> 00:17:01,417\nI'm pretty certain you can learn this.\n\n266\n00:17:01,517 --> 00:17:04,287\nAnd there are a lot of things in life\nthat I believe you can learn.\n\n267\n00:17:04,387 --> 00:17:06,455\nAnd you just have to be given\nthe opportunity to learn it.\n\n268\n00:17:06,556 --> 00:17:09,425\nI had the benefit of watching a lot of\nsmart people do a lot of things.\n\n269\n00:17:09,559 --> 00:17:12,328\nI'm surrounded by 60 people. \nThey're doing smart things all the time.\n\n270\n00:17:12,428 --> 00:17:14,397\nAnd they probably don't realize it,\n\n271\n00:17:14,497 --> 00:17:17,066\nbut I'm learning constantly from\nevery single one of them.\n\n272\n00:17:17,166 --> 00:17:21,904\nAnd so, I don't like giving up on people\nbecause I think they could improve.\n\n273\n00:17:22,004 --> 00:17:24,774\nAnd so there's a--\nit's kind of tongue-in-cheek--\n\n274\n00:17:24,874 --> 00:17:29,545\nbut people know that I'd rather\ntorture them into greatness.\n\n275\n00:17:29,712 --> 00:17:32,281\n-That was the phrase that\nI was hoping to uncover.\n\n276\n00:17:32,381 --> 00:17:33,783\nYeah, I remember you mentioned that.\n\n277\n00:17:33,916 --> 00:17:37,286\n-Yeah. So I'd rather torture you into\ngreatness because I believe in you.\n\n278\n00:17:37,954 --> 00:17:43,326\nAnd I think coaches that\nreally believe in their team\n\n279\n00:17:43,426 --> 00:17:45,127\ntorture them into greatness.\n\n280\n00:17:45,261 --> 00:17:49,198\nAnd oftentimes they're so close. \nDon't give up, they're so close.\n\n281\n00:17:49,298 --> 00:17:52,868\nGreatness kind of comes all of a sudden,\none day he's like, \"I got it\".\n\n282\n00:17:53,002 --> 00:17:54,203\nDo you know what I'm saying?\n\n283\n00:17:54,303 --> 00:17:55,805\nThat feeling that you didn't\nget it yesterday\n\n284\n00:17:55,905 --> 00:17:59,208\nand all of a sudden one day\nsomething clicked. \"Oh, I got it.\"\n\n285\n00:17:59,308 --> 00:18:03,212\nCould you imagine you gave up just\nthat moment right before you got it?\n\n286\n00:18:03,512 --> 00:18:04,981\nI don't want you to give up on that,\n\n287\n00:18:05,081 --> 00:18:07,116\nso I'll just keep torturing you.\n\n288\n00:18:09,118 --> 00:18:13,990\n-How's your work-life balance?\n\n289\n00:18:21,764 --> 00:18:23,733\n-Well, it depends on who you ask.\n\n290\n00:18:24,934 --> 00:18:27,436\nI think my work-life balance\nis really great.\n\n291\n00:18:27,870 --> 00:18:34,644\nIt's really great. \nI work as much as I can.\n\n292\n00:18:38,147 --> 00:18:39,815\nI feel like he's judging me.\n\n293\n00:18:42,551 --> 00:18:46,022\nI'm older than you.\nI have more wisdom than you.\n\n294\n00:18:46,122 --> 00:18:47,390\nSo what I...\n\n295\n00:18:48,658 --> 00:18:50,693\n-These are all the highlights\nfrom our conversations\n\n296\n00:18:50,793 --> 00:18:53,262\nthat I think more people\nshould get to hear, so...\n\n297\n00:18:54,664 --> 00:18:59,402\n-Well, I work from the moment\nI wake up to the moment I go to bed,\n\n298\n00:18:59,502 --> 00:19:01,871\nand I work seven days a week.\n\n299\n00:19:02,004 --> 00:19:04,306\nWhen I'm not working, \nI'm thinking about working.\n\n300\n00:19:05,107 --> 00:19:09,011\nAnd when I'm working,\nI'm working, and so...\n\n301\n00:19:10,179 --> 00:19:13,783\nAnd I sit through movies,\nbut I don't remember  them\n\n302\n00:19:13,883 --> 00:19:15,418\nbecause I'm thinking about work.\n\n303\n00:19:17,386 --> 00:19:21,724\nAnd so that's--\n\n304\n00:19:22,091 --> 00:19:28,297\nBut my work is not, as you know,\nit's not working as in,\n\n305\n00:19:28,431 --> 00:19:31,367\nthere's this problem and you're\ntrying to solve this problem.\n\n306\n00:19:31,467 --> 00:19:34,203\nYou're thinking about what\nthe company can be,\n\n307\n00:19:34,336 --> 00:19:37,173\nand are there things that\nwe could do even better?\n\n308\n00:19:37,306 --> 00:19:40,342\nOr sometimes it's just trying to\nsolve a problem, you know?\n\n309\n00:19:40,476 --> 00:19:42,611\nBut sometimes you're imagining the future.\n\n310\n00:19:42,712 --> 00:19:44,880\nAnd, boy, if we did this and that.\n\n311\n00:19:44,980 --> 00:19:49,552\nIt's working, you're fantasizing,\nyou're dreaming, right.\n\n312\n00:19:49,652 --> 00:19:51,053\nI mean, that's incredible.\n\n313\n00:19:51,187 --> 00:19:53,289\n-Well, so yeah, \nto concretize this a little bit\n\n314\n00:19:53,389 --> 00:19:57,993\nand we will get to talking about AI,\nwhich I hear is a thing these days.\n\n315\n00:19:58,094 --> 00:20:01,363\n-It's a thing.\n-Yeah, officially a thing.\n\n316\n00:20:01,464 --> 00:20:04,233\nBut to concrete concretize\nthis a bit like,\n\n317\n00:20:04,366 --> 00:20:07,503\nwhat does a day in\nJensen's life look like?\n\n318\n00:20:08,237 --> 00:20:10,306\n-Well, I used to wake up at five.\n\n319\n00:20:10,406 --> 00:20:13,375\nThese days, I wake up at six\nbecause of my dogs.\n\n320\n00:20:13,476 --> 00:20:16,879\nAnd the reason why six,\nis somehow we decided\n\n321\n00:20:16,979 --> 00:20:19,248\nthat six o'clock is when\nthey should wake up.\n\n322\n00:20:19,348 --> 00:20:24,754\nAnd I don't know what it is.\nI don't mind waking anybody up\n\n323\n00:20:24,854 --> 00:20:27,022\nbut I feel guilty when I\nwake the puppies up.\n\n324\n00:20:29,091 --> 00:20:33,696\nAnd it actually burdens me.\nSo, I don't want to move.\n\n325\n00:20:33,829 --> 00:20:38,367\nThey pick up on any vibration in the house\n\n326\n00:20:38,701 --> 00:20:42,872\nand it wakes them up and\nso we kind of stay in bed.\n\n327\n00:20:43,005 --> 00:20:46,976\nAnd I just read in bed until\nsix o'clock and it's time--\n\n328\n00:20:47,109 --> 00:20:48,978\n-But you're thinking about GPUs?\n\n329\n00:20:49,145 --> 00:20:54,250\n-Oh yeah. Yeah, yeah, sure. I'm obsessed\nabout GPUs. I mean, what can you do?\n\n330\n00:20:54,383 --> 00:20:57,319\nI'm constantly... \nNo. I'm just...\n\n331\n00:20:57,586 --> 00:21:00,322\n-And then the day is all, I guess,\ngroup meetings.\n\n332\n00:21:00,422 --> 00:21:02,224\nBecause it can't be\none-on-one meetings.\n\n333\n00:21:02,324 --> 00:21:06,695\n-Yeah. I get my work done before I go\nto work and then when I get to work...\n\n334\n00:21:08,731 --> 00:21:11,066\n-And how many meetings in a typical day?\n\n335\n00:21:12,434 --> 00:21:14,403\n-Pretty much all day long.\n\n336\n00:21:14,537 --> 00:21:17,306\nAnd so, I select the meetings that\nare really important to me.\n\n337\n00:21:17,439 --> 00:21:21,710\nI try not to have regular meetings,\n\n338\n00:21:21,811 --> 00:21:24,079\nregular operational meetings.\n\n339\n00:21:24,180 --> 00:21:26,482\nBecause I've got amazing people in\nthe company who are\n\n340\n00:21:26,582 --> 00:21:28,184\ndoing regular operational meetings.\n\n341\n00:21:28,284 --> 00:21:32,221\nAnd so we're pinch hitters,\nCEOs are pinch hitters.\n\n342\n00:21:32,354 --> 00:21:35,591\nWe should be working on the things\nthat nobody else can or nobody else is.\n\n343\n00:21:35,691 --> 00:21:38,794\nSo you're jumping in to projects\nthat are stuck or off track.\n\n344\n00:21:38,894 --> 00:21:40,362\n-That's right.\n-Or new ideas.\n\n345\n00:21:40,462 --> 00:21:44,266\n-Wherever we can move the needle.\nNo reporting meetings.\n\n346\n00:21:44,366 --> 00:21:46,735\nI hate reporting meetings. \nThey don't have to report to me.\n\n347\n00:21:46,869 --> 00:21:48,103\nI just have problem meetings.\n\n348\n00:21:48,204 --> 00:21:51,140\nAnd so problem meetings, or idea meetings,\nor brainstorming meetings,\n\n349\n00:21:51,240 --> 00:21:53,843\nor creation meetings, or whatever it is,\n\n350\n00:21:53,943 --> 00:21:55,311\nthose are the meetings I go to.\n\n351\n00:21:55,444 --> 00:21:58,080\nAnd so usually I call them,\n\n352\n00:21:58,214 --> 00:22:01,517\nI try really hard not to have Outlook\nmanage my life.\n\n353\n00:22:01,617 --> 00:22:03,752\nAnd so we purposefully decide\n\n354\n00:22:03,853 --> 00:22:06,255\nwhat kind of things we want to do,\nwe want to work on.\n\n355\n00:22:06,355 --> 00:22:08,190\nAnd so I try to\nlive a life of purpose,\n\n356\n00:22:08,290 --> 00:22:12,661\nand I manage my time accordingly. Yeah.\n\n357\n00:22:13,262 --> 00:22:17,867\n-You used a phrase, once, \n$0 billion markets,\n\n358\n00:22:17,967 --> 00:22:20,536\nthat $0 billion markets are\nyour favorite markets.\n\n359\n00:22:20,636 --> 00:22:22,071\n-Yeah.\n-What do you mean?\n\n360\n00:22:22,938 --> 00:22:27,209\n-If you take a step back, our purpose,\n\n361\n00:22:27,343 --> 00:22:30,479\nalmost all of our purposes should be\n\n362\n00:22:30,579 --> 00:22:36,252\nto go and do something\nthat has never been done before.\n\n363\n00:22:37,019 --> 00:22:39,388\nThat is insanely hard to do.\n\n364\n00:22:39,488 --> 00:22:43,626\nThat if you achieve it,\ncould make a real contribution.\n\n365\n00:22:43,726 --> 00:22:46,762\nI know your company does that.\nI try to do that.\n\n366\n00:22:47,429 --> 00:22:51,967\nAnd if that's the case, it hasn't been\ndone before, it's incredibly hard to do,\n\n367\n00:22:52,301 --> 00:22:54,737\nIt's probably-- and it's never\nbeen done before--\n\n368\n00:22:55,137 --> 00:22:58,474\nthat market is probably\n$0 billion in size.\n\n369\n00:22:59,074 --> 00:23:01,010\nBecause it has never been done before.\n\n370\n00:23:01,143 --> 00:23:06,181\nI'd rather be a market maker,\nmarket creator, than a market taker.\n\n371\n00:23:07,983 --> 00:23:13,055\nTo create something new that\nnever existed before versus thinking share\n\n372\n00:23:13,155 --> 00:23:16,558\nI don't love thinking about share.\nI don't like the concept of share.\n\n373\n00:23:17,293 --> 00:23:23,599\nAnd the reason for that is because\nif you think about it in the big picture,\n\n374\n00:23:23,699 --> 00:23:27,303\nStripe existed out of thin air,\nyou vaporized.\n\n375\n00:23:27,436 --> 00:23:29,371\nYou created something out of vapor.\n\n376\n00:23:29,471 --> 00:23:34,710\nIt wasn't as if there was another-- \nsomething else.\n\n377\n00:23:34,810 --> 00:23:41,784\nAnd so I'd like to think that we can\ncome up with something that is $0 billion.\n\n378\n00:23:41,884 --> 00:23:47,389\nA $0 billion market is a good way\nto cause the company to think about\n\n379\n00:23:47,523 --> 00:23:49,892\nhow to go create something\nfor the first time.\n\n380\n00:23:50,159 --> 00:23:52,728\n-So our mission is to grow\nthe GDP of the internet,\n\n381\n00:23:52,861 --> 00:23:57,066\nand the GDP of the internet--\n\n382\n00:23:57,199 --> 00:23:59,668\nthe clause in that usually gets\nmost of the attention.\n\n383\n00:23:59,768 --> 00:24:03,706\nBut I think the most important\npart is just the verb \"grow\".\n\n384\n00:24:04,173 --> 00:24:08,711\nBecause, to your point,\nwe shouldn't be thinking about, well,\n\n385\n00:24:08,911 --> 00:24:11,146\nwhich are the transactions\nthat are already happening\n\n386\n00:24:11,246 --> 00:24:13,248\nor which are the businesses\nthat already exist,\n\n387\n00:24:13,349 --> 00:24:15,651\nwe should be thinking about, which are\nthe transactions that don't exist\n\n388\n00:24:15,751 --> 00:24:17,786\nand which are the businesses\nthat don't exist.\n\n389\n00:24:17,886 --> 00:24:21,490\nThe GDP of the world\nis around $100 trillion,\n\n390\n00:24:21,623 --> 00:24:22,891\nbut it doesn't have to be $100 trillion.\n\n391\n00:24:23,025 --> 00:24:26,161\nIt could be $200 trillion\nor $1000 trillion.\n\n392\n00:24:26,295 --> 00:24:28,263\n-That's exactly right.\n\n393\n00:24:28,364 --> 00:24:33,235\nAnd most of the value we're going\nto create over the next several decades\n\n394\n00:24:33,335 --> 00:24:39,108\nare likely not limited by physical things.\n\n395\n00:24:39,241 --> 00:24:41,610\nAnd so this is a pretty\nextraordinary time.\n\n396\n00:24:41,710 --> 00:24:45,047\n-And so with this concept\nof $0 billion markets,\n\n397\n00:24:45,180 --> 00:24:49,385\nif I'm at NVIDIA, am I coming to you\nwith some proposal for some project\n\n398\n00:24:49,518 --> 00:24:53,422\nand maybe there's several billion dollars\nof CapEx involved or\n\n399\n00:24:53,522 --> 00:24:56,458\nit's a many-year pursuit or something.\n\n400\n00:24:56,558 --> 00:25:00,195\nAnd there are no customers for it today,\n\n401\n00:25:00,329 --> 00:25:02,431\nthere's no demand\nthat I can demonstrate for it.\n\n402\n00:25:02,531 --> 00:25:06,935\nAnd you guys are just making a gut call\nto say that,\n\n403\n00:25:07,036 --> 00:25:10,973\n\"Yes nobody is doing this today.\nWe think they could, we think they should.\n\n404\n00:25:11,073 --> 00:25:12,975\nAnd therefore we're going to pursue it.\"\n\n405\n00:25:13,075 --> 00:25:14,743\n-Really close. Yeah.\nIt's kind of like that.\n\n406\n00:25:14,843 --> 00:25:18,781\nAnd it's a gut call in the sense that\n\n407\n00:25:18,881 --> 00:25:22,384\nyour intuition says something\nas a starting thesis,\n\n408\n00:25:22,484 --> 00:25:24,153\nbut then you have to reason through it.\n\n409\n00:25:24,753 --> 00:25:28,757\nAnd the reasoning of it is much, much more\nimportant to me than a spreadsheet.\n\n410\n00:25:29,391 --> 00:25:32,795\nI hate spreadsheets because you can make\nspreadsheets do whatever you want.\n\n411\n00:25:32,928 --> 00:25:35,330\nYou can make any chart you want\nout of a spreadsheet.\n\n412\n00:25:35,431 --> 00:25:37,032\nYou just got to type in some numbers.\n\n413\n00:25:37,199 --> 00:25:39,535\nAnd so I don't love spreadsheets\nfor that reason.\n\n414\n00:25:39,635 --> 00:25:42,671\nI love words for that reason,\nwords are reasoning.\n\n415\n00:25:42,805 --> 00:25:45,574\nTell me, how did you reason through this?\nWhat's our intuition?\n\n416\n00:25:45,707 --> 00:25:48,844\nWhy do we believe that matters?\nWhy do we think it's hard?\n\n417\n00:25:49,711 --> 00:25:52,781\nI like hard things because it\ntakes a long time to do,\n\n418\n00:25:53,082 --> 00:25:55,017\nand if it takes a long time to do...\n\n419\n00:25:55,884 --> 00:26:00,189\na lot of people who are less committed\nprobably won't do it.\n\n420\n00:26:00,756 --> 00:26:04,226\nIf it's really, really hard to do,\nit takes a long time to do,\n\n421\n00:26:04,326 --> 00:26:07,563\nit takes a really resilient\nand a really dedicated,\n\n422\n00:26:07,663 --> 00:26:09,965\nreally committed person to go after it.\n\n423\n00:26:10,099 --> 00:26:13,102\nAnd if it also takes a long time to do,\n\n424\n00:26:13,202 --> 00:26:17,339\nyou can kind of flounder around\nfor a couple of years and nobody notices.\n\n425\n00:26:17,439 --> 00:26:21,577\nAnd so I could be incompetent\nfor several years\n\n426\n00:26:21,710 --> 00:26:24,746\nand everybody goes,\n\"Well, who saw it?\"\n\n427\n00:26:25,047 --> 00:26:27,816\n-And where did Cuda come from?\n\n428\n00:26:29,051 --> 00:26:35,524\n-Cuda came originally from two ideas.\nOne is called...\n\n429\n00:26:36,758 --> 00:26:38,227\nI hate to get technical,\n\n430\n00:26:38,360 --> 00:26:42,431\nbut we created, we pioneered, this idea\ncalled accelerated computing.\n\n431\n00:26:42,531 --> 00:26:48,203\nAccelerated computing is\nlike an IO device,\n\n432\n00:26:48,303 --> 00:26:50,272\nsomething that you sit on PCI Express,\n\n433\n00:26:50,372 --> 00:26:51,940\nif anybody's in the computer business,\n\n434\n00:26:52,040 --> 00:26:58,847\nan IO device that allows the application\nto interact with that IO device\n\n435\n00:26:58,947 --> 00:27:02,518\nin such a way as to accelerate parts\nof the application.\n\n436\n00:27:03,085 --> 00:27:08,457\nAnd UDA was an invention in 1993,\nand it's really a profound invention,\n\n437\n00:27:08,590 --> 00:27:16,031\nallows the software programmer\nto directly program an IO device,\n\n438\n00:27:16,165 --> 00:27:18,700\nwrite an application directly\nto the IO device,\n\n439\n00:27:18,834 --> 00:27:22,571\nbecause the IO device \nis virtualized and...\n\n440\n00:27:22,704 --> 00:27:27,009\nit's architecturally compatible\nacross multiple generations.\n\n441\n00:27:27,109 --> 00:27:31,280\nIt's, anyways, we invented this idea \ncalled accelerated computing,\n\n442\n00:27:31,413 --> 00:27:35,884\nand that was-- we call it unified driver\narchitecture for whatever reason.\n\n443\n00:27:36,485 --> 00:27:39,221\nAnd then several years later,\n\n444\n00:27:39,354 --> 00:27:42,658\nwe thought we could make our\nGPUs more programable\n\n445\n00:27:42,758 --> 00:27:44,159\nto high level programing languages.\n\n446\n00:27:44,259 --> 00:27:47,429\nAnd we invented this idea called CG,\n\n447\n00:27:47,529 --> 00:27:51,567\nC for graphics, okay.\nC for graphics processors.\n\n448\n00:27:51,733 --> 00:27:57,372\nAnd that opened up\nsome really exciting opportunities.\n\n449\n00:27:57,506 --> 00:27:59,741\nAnd we thought, you know what\nthis is going to work.\n\n450\n00:27:59,875 --> 00:28:03,178\nBut CG, the programing model, \nwasn't exactly right.\n\n451\n00:28:03,278 --> 00:28:08,817\nAnd so we invented Cuda which is compute,\nwith you know...\n\n452\n00:28:08,917 --> 00:28:12,087\nSo anyways that's how.\nIt's a horrible story frankly.\n\n453\n00:28:12,554 --> 00:28:15,224\nAnyways, we invented this idea\ncalled accelerated computing.\n\n454\n00:28:15,357 --> 00:28:16,692\nWe pioneered this approach.\n\n455\n00:28:16,825 --> 00:28:20,429\nI guess the real question is\nwas it a smash hit overnight?\n\n456\n00:28:21,196 --> 00:28:27,069\n-No, it was an incredible\ndisaster overnight.\n\n457\n00:28:27,169 --> 00:28:29,571\nAnd it kind of went like this.\n\n458\n00:28:29,671 --> 00:28:32,674\n-This is one of your\n$0 billion markets you went after.\n\n459\n00:28:32,808 --> 00:28:35,010\n-Yeah.\n-And it was a disaster.\n\n460\n00:28:35,110 --> 00:28:38,080\n-Yeah. Because it was a\n$0 billion we went after.\n\n461\n00:28:38,213 --> 00:28:41,750\nBut it cost so much to go after\nthat $0 billion market,\n\n462\n00:28:41,850 --> 00:28:45,921\nit actually crushed\nthe $1 billion market we were enjoying.\n\n463\n00:28:46,021 --> 00:28:50,592\nAnd the reason for that is because\n\n464\n00:28:50,726 --> 00:28:57,299\nCuda added a ton of cost into our chips,\nbut there were no applications.\n\n465\n00:28:57,399 --> 00:28:58,800\nAnd if there are no applications,\n\n466\n00:28:58,934 --> 00:29:02,971\ncustomers don't value the product and\nthey won't pay you a premium for it.\n\n467\n00:29:03,238 --> 00:29:06,141\nAnd if people aren't willing to pay\nyou for it, but your cost went up,\n\n468\n00:29:06,241 --> 00:29:09,211\nthen your gross margins get crushed.\n\n469\n00:29:09,311 --> 00:29:15,050\nOur market cap was low and\nit went down to really low.\n\n470\n00:29:15,150 --> 00:29:19,187\nI think our market cap went\ndown to $1 billion or something like that.\n\n471\n00:29:19,288 --> 00:29:21,089\nI wish I had bought it,\nbut anyways.\n\n472\n00:29:21,189 --> 00:29:25,227\n-Okay. And so therefore you\nimmediately canceled Cuda\n\n473\n00:29:25,327 --> 00:29:26,895\nand went back to the old strategy.\n\n474\n00:29:26,995 --> 00:29:31,266\n-No, no, I believed in Cuda, because you\nreasoned about it. You reason about it.\n\n475\n00:29:31,400 --> 00:29:35,170\nLook, we really believed that\naccelerated computing\n\n476\n00:29:35,270 --> 00:29:39,775\nwas going to be able to solve problems\nthat normal computers couldn't.\n\n477\n00:29:40,309 --> 00:29:43,645\nAnd if we wanted to\nextend the architecture\n\n478\n00:29:43,779 --> 00:29:49,451\nto be much more general-purpose,\nwe had to make that sacrifice.\n\n479\n00:29:49,584 --> 00:29:55,190\nAnd so I deeply believed in\nthe mission of our company,\n\n480\n00:29:55,290 --> 00:29:58,727\nI deeply believed in its opportunities.\n-And so were analysts--\n\n481\n00:29:58,827 --> 00:30:01,797\n-I deeply, deeply believe\nthat people were wrong.\n\n482\n00:30:01,897 --> 00:30:06,168\nThey just didn't appreciate what we built.\nI deeply believed it.\n\n483\n00:30:06,268 --> 00:30:08,670\n-And so weren't analysts and\nthe board and employees--\n\n484\n00:30:08,770 --> 00:30:11,640\nyou've torpedoed this\nexisting revenue stream.\n\n485\n00:30:11,740 --> 00:30:17,312\nYou have this hyped thing that \nyou're selling a lofty dream around,\n\n486\n00:30:17,412 --> 00:30:18,847\nthat nobody seems to actually want.\n\n487\n00:30:18,980 --> 00:30:21,850\nThe business is really suffering,\n\n488\n00:30:22,984 --> 00:30:25,287\nTalk us through that. You believed.\n\n489\n00:30:26,188 --> 00:30:30,592\n-Well, it goes something like this,\n\"Oh, gosh, they're so dumb.\"\n\n490\n00:30:32,260 --> 00:30:35,797\nSomething like that-- denial.\n\n491\n00:30:35,897 --> 00:30:38,734\nNo, I'm just kidding.\nYou go back to what you believe.\n\n492\n00:30:38,834 --> 00:30:40,702\nAnd if you believe something--\n\n493\n00:30:40,802 --> 00:30:43,372\n-Did the board put pressure\non you during this?\n\n494\n00:30:44,539 --> 00:30:49,845\n-They... I start every conversation with\nwhat I deeply believed\n\n495\n00:30:49,945 --> 00:30:54,416\nand they believed it \nbecause they saw me deeply believe it.\n\n496\n00:30:54,516 --> 00:30:55,751\nAnd I reasoned about it.\n\n497\n00:30:55,851 --> 00:30:59,521\nIt wasn't like it was a spreadsheet,\n\n498\n00:30:59,621 --> 00:31:01,423\nand therefore you've\ngot to believe the spreadsheet.\n\n499\n00:31:01,556 --> 00:31:03,525\nThey had to believe the reasoning,\nthe words.\n\n500\n00:31:03,658 --> 00:31:06,962\n-How long did it take it to start working?\n\n501\n00:31:09,865 --> 00:31:12,534\n-Probably ten years, yeah.\n\n502\n00:31:13,368 --> 00:31:15,103\nIt wasn't that long.\n\n503\n00:31:16,304 --> 00:31:18,640\nTen years.\nIt comes and goes.\n\n504\n00:31:18,874 --> 00:31:21,109\n-Ten years.\n-Less than a third of your tenure.\n\n505\n00:31:21,209 --> 00:31:24,012\n-Yeah, it comes and goes.\nI barely remembered it.\n\n506\n00:31:24,112 --> 00:31:26,281\nThe suffering, I barely remembered it.\n\n507\n00:31:26,381 --> 00:31:31,486\n-Could NVIDIA be as successful\nin AI without Cuda?\n\n508\n00:31:31,586 --> 00:31:33,054\n-No, impossible.\n\n509\n00:31:33,188 --> 00:31:38,693\nIt is potentially one of the most\nimportant inventions in modern computing.\n\n510\n00:31:38,794 --> 00:31:41,430\nWe invented this idea called\naccelerated computing,\n\n511\n00:31:41,530 --> 00:31:46,968\nand the idea is so simple\nbut deeply profound.\n\n512\n00:31:47,235 --> 00:31:50,071\nIt says the vast majority...\n\n513\n00:31:50,172 --> 00:31:54,309\na small percentage\nof the code of programs\n\n514\n00:31:54,409 --> 00:31:59,648\noccupies, consumes, 99.999%\nof the runtime.\n\n515\n00:31:59,748 --> 00:32:03,118\nAnd this is true for a lot of\nvery important applications.\n\n516\n00:32:03,218 --> 00:32:07,222\nAnd that small little kernel,\n\n517\n00:32:07,322 --> 00:32:12,794\nor some several kernels,\ncan be accelerated.\n\n518\n00:32:12,894 --> 00:32:16,031\nIt's not all just parallel processing.\n\n519\n00:32:16,131 --> 00:32:17,199\nIt's not as simple as that.\n\n520\n00:32:17,332 --> 00:32:20,202\nBut the idea is that\nwe can take that kernel,\n\n521\n00:32:20,302 --> 00:32:22,103\nthat piece of software,\nthat part of the software,\n\n522\n00:32:22,204 --> 00:32:24,339\nand accelerate the living daylights\nout of it.\n\n523\n00:32:24,439 --> 00:32:27,142\nAnd today, when Moore's Law\nhas run its course\n\n524\n00:32:27,275 --> 00:32:33,482\nand CPU scaling is basically stopped,\nand if we don't accelerate every software,\n\n525\n00:32:33,582 --> 00:32:37,018\nyou're going to see extraordinary\ncomputation inflation.\n\n526\n00:32:37,118 --> 00:32:39,254\nBecause the amount of computation\nthe world does\n\n527\n00:32:39,354 --> 00:32:40,956\nis doubling every year still,\n\n528\n00:32:41,089 --> 00:32:43,658\nand yet if CPUs\nand general-purpose computers\n\n529\n00:32:43,758 --> 00:32:46,461\nare not increasing in performance\nbecause it stopped,\n\n530\n00:32:46,595 --> 00:32:47,963\nthen what's your alternative?\n\n531\n00:32:48,063 --> 00:32:50,832\nYour cost of computing is going\nto keep going up exponentially.\n\n532\n00:32:50,932 --> 00:32:53,468\nAnd so the time has come\nfor us to do that.\n\n533\n00:32:53,568 --> 00:32:56,438\n-So everyone here runs a business and...\n\n534\n00:32:56,705 --> 00:32:58,073\n-Accelerate everything!\n\n535\n00:32:58,173 --> 00:32:59,674\n-And you heard it here first.\n\n536\n00:32:59,774 --> 00:33:04,513\nAnd probably everyone has\nsome version of Cuda\n\n537\n00:33:04,613 --> 00:33:09,284\nor a thing that they think really\nmakes sense for the sector\n\n538\n00:33:09,384 --> 00:33:12,220\nor makes sense for their\ntechnology or what have you,\n\n539\n00:33:12,320 --> 00:33:15,023\nbut where the market doesn't see it yet.\n\n540\n00:33:15,323 --> 00:33:19,628\nDo you think it's possible to extract any\nkind of generalizable principles around\n\n541\n00:33:19,761 --> 00:33:23,198\nwhen you should really doggedly\ntrust that vision,\n\n542\n00:33:23,298 --> 00:33:25,834\nand when perhaps it's\nworth reconsidering\n\n543\n00:33:25,934 --> 00:33:28,103\nin a fashion that we could\nextrapolate from,\n\n544\n00:33:28,236 --> 00:33:30,939\nin the case of Cuda and other\n\"Cudas\" that have existed over\n\n545\n00:33:31,072 --> 00:33:32,140\nthe course of NVIDIA's history?\n\n546\n00:33:32,240 --> 00:33:38,079\n-Yeah. The question is determination\nand commitment versus stubbornness\n\n547\n00:33:38,179 --> 00:33:42,150\nand that line is fuzzy.\n\n548\n00:33:43,018 --> 00:33:47,689\nI gut-checked against\nmy core beliefs every day,\n\n549\n00:33:47,822 --> 00:33:50,692\nI still do.\nAnd you gut-check against it.\n\n550\n00:33:50,792 --> 00:33:55,931\nThe first principles by which you\nreason about your strategies,\n\n551\n00:33:56,031 --> 00:33:58,667\nthe first principles by which you\nreason about your strategies,\n\n552\n00:33:58,767 --> 00:34:04,973\nthose first principles are easy to \nremember, and it's not a long list.\n\n553\n00:34:05,073 --> 00:34:10,145\nAnd now the question is,\nare those principles,\n\n554\n00:34:10,245 --> 00:34:12,581\ndid they change in some\nfundamental way?\n\n555\n00:34:12,681 --> 00:34:17,886\nAre external conditions such that they\nno longer matter as much as before?\n\n556\n00:34:18,019 --> 00:34:20,121\nDid somebody else solve the problem,\n\n557\n00:34:20,221 --> 00:34:23,825\nand therefore that problem\nhas now disappeared?\n\n558\n00:34:23,925 --> 00:34:28,196\nIs it that there will never be any need?\n\n559\n00:34:28,296 --> 00:34:30,432\nYou gut-check it, right, constantly.\n\n560\n00:34:30,532 --> 00:34:34,569\nAnd to the extent that--\nthat's number one, gut check.\n\n561\n00:34:34,703 --> 00:34:38,440\nYou have to first of all be really careful\nto distill down the first principle,\n\n562\n00:34:38,540 --> 00:34:42,844\ninstead of, \"I want something\",\nthat's stubbornness.\n\n563\n00:34:42,944 --> 00:34:44,913\nYou can't reason about it.\nI just want it.\n\n564\n00:34:45,013 --> 00:34:46,648\nWe're not five-year-olds, right?\n\n565\n00:34:46,748 --> 00:34:50,485\nAnd so you've got to reason about it,\nnumber one.\n\n566\n00:34:50,585 --> 00:34:52,754\nNumber two, you have to be clever.\n\n567\n00:34:52,854 --> 00:34:56,024\nThe fact of the matter is there are a lot\nof new companies being created here.\n\n568\n00:34:56,124 --> 00:35:00,095\nIt's amazing how many great companies\nare in the audience\n\n569\n00:35:00,195 --> 00:35:02,330\nand young companies\nin the audience.\n\n570\n00:35:02,430 --> 00:35:03,498\nYou have to be clever.\n\n571\n00:35:03,598 --> 00:35:08,670\nAnd so we found ways to monetize,\n\n572\n00:35:08,770 --> 00:35:12,340\neven in a small way, Cuda.\n\n573\n00:35:12,440 --> 00:35:16,745\nAnd so we found-- we looked\neverywhere for applications.\n\n574\n00:35:16,845 --> 00:35:19,180\nWe found an application with\nCT reconstruction.\n\n575\n00:35:19,280 --> 00:35:21,650\nWe found an application with\nseismic processing.\n\n576\n00:35:21,750 --> 00:35:25,387\nWe found another application with\nmolecular dynamics,\n\n577\n00:35:25,487 --> 00:35:27,656\nand so we're constantly looking\nfor applications.\n\n578\n00:35:27,756 --> 00:35:34,162\nThey didn't make it a home run\nbut it sustained us just enough.\n\n579\n00:35:34,262 --> 00:35:38,400\nAnd bought us time\nfor it to really happen.\n\n580\n00:35:38,500 --> 00:35:42,137\n-Okay. So let's talk about about AI.\n\n581\n00:35:42,237 --> 00:35:44,639\nI'm going to just do some math\nto ground things here.\n\n582\n00:35:44,773 --> 00:35:46,808\nLet's just say that the total,\n\n583\n00:35:46,941 --> 00:35:53,281\nsort of compute capacity of all\nGPUs in the world today is X.\n\n584\n00:35:53,381 --> 00:35:56,151\nWhat do you think...\n\n585\n00:35:56,251 --> 00:36:02,624\nWhat multiple of X will\nwe be at in five years?\n\n586\n00:36:04,426 --> 00:36:08,697\n-First of all, you know that I'm going\nto regret saying this.\n\n587\n00:36:09,731 --> 00:36:10,999\nAnd this is be...\n\n588\n00:36:11,099 --> 00:36:14,402\nI'm a public company, you crazy person.\n\n589\n00:36:16,037 --> 00:36:22,010\nHow nice is it to be private?\n\n590\n00:36:29,250 --> 00:36:31,186\n-Safe to say considerably more.\n\n591\n00:36:32,053 --> 00:36:35,890\n-Well, let's reason about it, shall we?\nOkay. So let's let's reason about it.\n\n592\n00:36:35,990 --> 00:36:37,759\nLet's reason our way through okay.\n\n593\n00:36:37,859 --> 00:36:39,828\nSo first of all, it goes like this.\n\n594\n00:36:39,928 --> 00:36:43,431\nThe world has installed about\na trillion dollars worth of data centers.\n\n595\n00:36:43,665 --> 00:36:47,902\nThose trillion dollars worth of data\ncenters use general-purpose computing.\n\n596\n00:36:48,002 --> 00:36:50,705\nGeneral-purpose computing\nhas run its course.\n\n597\n00:36:51,172 --> 00:36:53,541\nWe cannot continue to process that way.\n\n598\n00:36:53,641 --> 00:36:57,846\nAnd so the world is going to accelerate\neverything, data processing, you name it.\n\n599\n00:36:57,946 --> 00:36:59,981\nAnd so we're going\nto accelerate everything.\n\n600\n00:37:00,081 --> 00:37:02,484\nWhen we accelerate everything,\nevery single data center,\n\n601\n00:37:02,617 --> 00:37:05,453\nevery single computer\nwill be an accelerated server.\n\n602\n00:37:05,687 --> 00:37:10,391\nWell, there's about a trillion dollars \nworth of computers if we don't grow at all\n\n603\n00:37:10,492 --> 00:37:13,895\nover the next four years that\nwe have to replace.\n\n604\n00:37:13,995 --> 00:37:16,498\nFour years, six years, \npick your number of years.\n\n605\n00:37:16,598 --> 00:37:20,435\nBut if the computer industry continues\nto grow at some 20% or so,\n\n606\n00:37:20,735 --> 00:37:24,272\nwe'll probably have to replace, over the \ncourse of next, pick your number of years,\n\n607\n00:37:24,405 --> 00:37:28,643\nabout $2 trillion worth of computers\nwith accelerated computing.\n\n608\n00:37:29,144 --> 00:37:32,046\nSo just make that GPUs, okay?\n\n609\n00:37:32,147 --> 00:37:34,215\nThat's number one.\nAnd this is the second part.\n\n610\n00:37:34,315 --> 00:37:38,787\nThis is the reason why,\nall of you, Stripe,\n\n611\n00:37:38,887 --> 00:37:42,123\nyou're onto something\njust absolutely monumental.\n\n612\n00:37:42,223 --> 00:37:48,696\nThis idea called, and you've heard me say\nan industrial revolution,\n\n613\n00:37:48,797 --> 00:37:50,231\nLet me tell you why.\n\n614\n00:37:50,331 --> 00:37:52,634\nWe are producing something\nfor the very first time\n\n615\n00:37:52,734 --> 00:37:55,003\nthat has never been produced before.\n\n616\n00:37:55,103 --> 00:37:57,572\nAnd we're producing it in\nextremely high volume.\n\n617\n00:37:57,705 --> 00:38:00,575\nAnd the production of this \"thing\"\n\n618\n00:38:00,675 --> 00:38:03,344\nrequires a new instrument\nthat never existed before.\n\n619\n00:38:03,444 --> 00:38:05,313\nIt's a GPU.\n\n620\n00:38:05,547 --> 00:38:08,416\nAnd the thing that we're producing\nfor the very first time,\n\n621\n00:38:08,516 --> 00:38:11,219\nfor the mathematicians and all the\ncomputer scientists in the room,\n\n622\n00:38:11,352 --> 00:38:14,589\nfor all of you, you know that\nwe're producing tokens.\n\n623\n00:38:14,756 --> 00:38:20,228\nWe're producing floating point numbers at\nhigh volume for the first time in history\n\n624\n00:38:20,361 --> 00:38:22,297\nand the floating point numbers have value.\n\n625\n00:38:22,430 --> 00:38:24,966\nThe reason why they have value\nis because it's intelligence.\n\n626\n00:38:25,099 --> 00:38:26,668\nIt's artificial intelligence.\n\n627\n00:38:26,768 --> 00:38:28,403\nYou can take these \nfloating point numbers.\n\n628\n00:38:28,536 --> 00:38:33,741\nYou reformulate it in such a way that it\nturns into English, French, proteins,\n\n629\n00:38:33,842 --> 00:38:39,981\nchemicals, graphics, images, videos,\nrobotic articulation,\n\n630\n00:38:40,081 --> 00:38:41,583\nsteering wheel articulation.\n\n631\n00:38:41,683 --> 00:38:45,153\nWe're producing tokens\nat extraordinary scale.\n\n632\n00:38:45,253 --> 00:38:48,289\nNow we've discovered a way\nthrough all of the work that we do\n\n633\n00:38:48,389 --> 00:38:52,060\nwith artificial intelligence,\nto produce tokens of almost any kind.\n\n634\n00:38:52,694 --> 00:38:58,933\nSo now, the world is going to\nproduce an enormous amount of tokens.\n\n635\n00:38:59,033 --> 00:39:02,470\nNow these tokens are going to be produced\nin new types of data centers.\n\n636\n00:39:02,570 --> 00:39:05,006\nWe call them AI factories.\n\n637\n00:39:05,106 --> 00:39:10,178\nBack in the last industrial revolution,\nwater comes into a machine,\n\n638\n00:39:10,278 --> 00:39:12,814\nyou light the water on fire, right?\n\n639\n00:39:12,947 --> 00:39:17,252\nTurn it into steam and\nthen it turns into electrons.\n\n640\n00:39:17,552 --> 00:39:19,554\nAtoms come in, electrons go out.\n\n641\n00:39:19,654 --> 00:39:21,389\nIn this new industrial revolution,\n\n642\n00:39:21,489 --> 00:39:24,959\nelectrons come in and\nfloating point numbers come out.\n\n643\n00:39:25,393 --> 00:39:27,896\nAnd just like the last\nindustrial revolution,\n\n644\n00:39:28,029 --> 00:39:32,267\nnobody understood why electricity\nis so valuable and is now sold,\n\n645\n00:39:32,400 --> 00:39:35,536\nmarketed kilowatt hours  per dollar.\n\n646\n00:39:35,870 --> 00:39:39,674\nAnd so now we have\na million tokens per dollar.\n\n647\n00:39:39,941 --> 00:39:42,844\nAnd so that same logic\n\n648\n00:39:42,944 --> 00:39:47,215\nis as incomprehensible to a lot of people\nas the last industrial revolution,\n\n649\n00:39:47,315 --> 00:39:51,853\nbut it's going to be completely normal\nin the next ten years.\n\n650\n00:39:52,353 --> 00:39:58,826\nThese tokens are going to create\nnew products, new services,\n\n651\n00:39:58,927 --> 00:40:02,330\nenhance productivity\non a whole slew of industries.\n\n652\n00:40:02,430 --> 00:40:05,533\n$100 trillion worth of industries\non top of us.\n\n653\n00:40:05,633 --> 00:40:07,669\nAnd so this industry is\ngoing to be gigantic.\n\n654\n00:40:07,769 --> 00:40:14,742\nAnd in order to monetize that, transact \nthat, you're going to need Stripe.\n\n655\n00:40:20,048 --> 00:40:23,051\nI have to tell you,\nthis is one of my favorite companies.\n\n656\n00:40:23,151 --> 00:40:26,220\nThe first time I met Patrick,\nhe had to explain Stripe to me.\n\n657\n00:40:26,321 --> 00:40:29,190\nFirst of all, \nit was so complicated.\n\n658\n00:40:31,059 --> 00:40:33,895\n-We tried to refine\nthe descriptions over time.\n\n659\n00:40:33,995 --> 00:40:37,165\n-First of all, you're in a complicated\nbusiness no matter what.\n\n660\n00:40:37,265 --> 00:40:41,235\nBut nonetheless, \nI was so inspired by it.\n\n661\n00:40:41,336 --> 00:40:42,637\nIncredible what you guys have built.\n\n662\n00:40:42,737 --> 00:40:44,372\n-We're going to get you\nmigrated to Stripe Billing\n\n663\n00:40:44,472 --> 00:40:46,174\nnow that we have\na usage-based billing?\n\n664\n00:40:46,307 --> 00:40:49,711\n-I wish I had a business\nthat required billing.\n\n665\n00:40:50,712 --> 00:40:54,415\n-I think your public filing suggests\nyou're doing a lot of billing.\n\n666\n00:40:56,918 --> 00:40:59,687\nWe'll follow up on it. All right.\n\n667\n00:41:00,521 --> 00:41:03,458\n-It's only ten transactions.\nJust so you know.\n\n668\n00:41:04,592 --> 00:41:09,731\nYour economics serving us is like nothing,\nit's like ten transactions.\n\n669\n00:41:10,198 --> 00:41:11,632\n-We'd happily take the 2.9% .\n\n670\n00:41:11,733 --> 00:41:15,670\nBut anyway,\nwe can discuss that separately.\n\n671\n00:41:18,906 --> 00:41:20,375\n-Done!\n\n672\n00:41:20,475 --> 00:41:24,412\n-You can't say that.\nYou're a public company.\n\n673\n00:41:24,512 --> 00:41:27,315\nThinking about these token factories.\n\n674\n00:41:27,448 --> 00:41:31,753\nI feel like a big question right now\nis whether the models saturate\n\n675\n00:41:31,886 --> 00:41:34,989\nin the sense that, we demoed the\nSigma assistant on stage earlier\n\n676\n00:41:35,123 --> 00:41:37,859\nand you can write some natural language,\nand we convert that to SQL.\n\n677\n00:41:37,959 --> 00:41:41,763\nAnd going from maybe\na 7 billion parameter model\n\n678\n00:41:41,896 --> 00:41:44,432\nto a 70 billion parameter model\nor something like that,\n\n679\n00:41:44,565 --> 00:41:46,401\nthere might be a significant kind of,\n\n680\n00:41:46,501 --> 00:41:49,971\nconsequential improvement in query\naccuracy for the user\n\n681\n00:41:50,071 --> 00:41:52,373\nfor the typical kinds of queries that\npeople tend to construct.\n\n682\n00:41:52,473 --> 00:41:56,144\nBut maybe going to a model that's ten x\nlarger than that is sort of unnecessary.\n\n683\n00:41:56,277 --> 00:41:57,345\nLike at some point you get to good enough,\n\n684\n00:41:57,478 --> 00:41:59,714\nyou can reliably convert the\nnatural language to SQL.\n\n685\n00:41:59,881 --> 00:42:01,682\nI think there's a question of,\n\n686\n00:42:01,783 --> 00:42:05,386\nfor the use cases for which LLM's\nare being deployed,\n\n687\n00:42:05,520 --> 00:42:07,889\nwhat does that\nsaturation curve look like,\n\n688\n00:42:07,989 --> 00:42:11,559\nand for how many use cases does one\nneed a trillion parameter model or\n\n689\n00:42:11,692 --> 00:42:12,760\na 10 trillion parameter model?\n\n690\n00:42:12,894 --> 00:42:16,030\nOr do we simply reach a point where,\nsome number that is, say,\n\n691\n00:42:16,164 --> 00:42:19,467\nless than 100 billion,\nis sufficient?\n\n692\n00:42:19,567 --> 00:42:23,304\nDo you have any point of view on that,\nor is that even\n\n693\n00:42:23,404 --> 00:42:26,541\na reasonable way to look at the\nquestion in the first place?\n\n694\n00:42:26,641 --> 00:42:28,576\n-Okay, let's break it down.\nLet's reason it out.\n\n695\n00:42:30,044 --> 00:42:31,045\n-In public.\n\n696\n00:42:31,312 --> 00:42:33,081\n-Almost everything,  every question I get,\n\n697\n00:42:33,181 --> 00:42:35,116\nokay, let's break it down,\nlet's reason it out.\n\n698\n00:42:35,216 --> 00:42:37,485\nSo, let's start with an example.\n\n699\n00:42:37,752 --> 00:42:41,856\nIn 2012, AlexNet was a computer vision.\n\n700\n00:42:41,956 --> 00:42:47,762\nImageNet, image recognition 82%,\nor something like that, accuracy.\n\n701\n00:42:47,895 --> 00:42:52,100\nOver the next...\nalmost not quite ten years,\n\n702\n00:42:52,233 --> 00:42:53,801\nI think it was like seven years,\n\n703\n00:42:54,102 --> 00:42:59,841\nevery single year, the accuracy\nerror reduced in half.\n\n704\n00:42:59,974 --> 00:43:04,445\nEvery year the error reduced in half\nor otherwise known as Moore's law.\n\n705\n00:43:04,545 --> 00:43:10,084\nSo you double the performance,\nyou double the accuracy,\n\n706\n00:43:10,518 --> 00:43:14,322\nand you double its believability\nevery single year.\n\n707\n00:43:14,422 --> 00:43:16,891\nOver the course of seven years,\nit's now superhuman.\n\n708\n00:43:17,525 --> 00:43:19,260\nSame thing with speech recognition,\n\n709\n00:43:19,360 --> 00:43:22,797\nsame thing with\nnatural language understanding.\n\n710\n00:43:22,897 --> 00:43:27,301\nWe want to know,\nwe want to believe, not know,\n\n711\n00:43:27,401 --> 00:43:31,806\nwe want to believe that the answer that's\nbeing predicted to us is accurate.\n\n712\n00:43:31,906 --> 00:43:33,174\nWe want to believe that.\n\n713\n00:43:33,274 --> 00:43:36,644\nAnd so the industry is going to chase\n\n714\n00:43:36,744 --> 00:43:39,514\nthat believability or that accuracy,\n\n715\n00:43:39,614 --> 00:43:46,454\nand double its accuracy 2x every year.\n\n716\n00:43:46,554 --> 00:43:50,258\nI believe that's going to be the same \nthing with natural language understanding.\n\n717\n00:43:50,391 --> 00:43:54,829\nAnd of course the problem\nspace is a lot more complicated,\n\n718\n00:43:54,962 --> 00:43:56,864\nbut I have every certainty\n\n719\n00:43:56,964 --> 00:44:00,067\nthat we're going to double\nits accuracy every single year\n\n720\n00:44:00,168 --> 00:44:02,470\nto the point where it is so accurate.\n\n721\n00:44:02,603 --> 00:44:05,373\nAnd we've we've largely tested\nacross many of your examples,\n\n722\n00:44:05,473 --> 00:44:07,441\nwhen you interact with it that you go,\n\"You know what?\n\n723\n00:44:07,575 --> 00:44:08,876\nThis is really, really good.\"\n\n724\n00:44:08,976 --> 00:44:13,214\nI believe the answer that it's producing\nfor me, that condition is very important.\n\n725\n00:44:13,347 --> 00:44:15,216\nThe second thing is this.\n\n726\n00:44:18,319 --> 00:44:20,454\nToday's language models, today's AI\n\n727\n00:44:20,555 --> 00:44:22,823\nand everything that\nwe've shown are one shot.\n\n728\n00:44:23,691 --> 00:44:27,461\nAnd yet you and I both know that there are\nmany things that we think about that\n\n729\n00:44:27,562 --> 00:44:29,931\nare not one shot. You have to iterate.\n\n730\n00:44:30,031 --> 00:44:33,201\nAnd so how do you come up--\nHow do you reason about a plan?\n\n731\n00:44:33,334 --> 00:44:37,805\nHow do you come up with a strategy\nto solve a problem?\n\n732\n00:44:37,905 --> 00:44:41,442\nMaybe you need to use tools. Maybe you\nhave to look up some proprietary data.\n\n733\n00:44:41,576 --> 00:44:44,545\nMaybe you have to do\nsome research, in fact.\n\n734\n00:44:44,645 --> 00:44:48,115\nMaybe you have to ask another agent.\nMaybe you have another ask another AI.\n\n735\n00:44:48,216 --> 00:44:51,018\nMaybe you have to be human in a loop.\nAsk a human.\n\n736\n00:44:51,118 --> 00:44:53,721\nTriggering events, send an\nemail to somebody or text to somebody.\n\n737\n00:44:53,854 --> 00:44:58,092\nGet a response before you can move on to\nthe next step of that of that plan.\n\n738\n00:44:58,192 --> 00:45:02,163\nAnd so a large language model has\nto iterate and think of a plan.\n\n739\n00:45:02,930 --> 00:45:04,732\nThat's not a one shot thing.\n\n740\n00:45:04,865 --> 00:45:08,736\nAnd once it comes up with a plan\nas it traverses that graph,\n\n741\n00:45:09,170 --> 00:45:10,571\nthere's a whole bunch\nof language models\n\n742\n00:45:10,671 --> 00:45:13,374\nthat are going to get\ninstantiated and initiated.\n\n743\n00:45:13,474 --> 00:45:17,912\nAnd so I think your\nfuture models are going to iterate.\n\n744\n00:45:18,045 --> 00:45:23,284\nAnd so instead of a one shot model,\n\n745\n00:45:23,417 --> 00:45:24,986\nit's going to be a planning model\n\n746\n00:45:25,086 --> 00:45:26,754\nwith a whole bunch of\nother models around it\n\n747\n00:45:26,854 --> 00:45:28,990\nthat are particularly good at\nparticular skills.\n\n748\n00:45:29,090 --> 00:45:30,758\nAnd so I think\nwe have a long ways to go.\n\n749\n00:45:32,360 --> 00:45:35,863\n-Meta garnered a lot of attention last\nweek for the release of Llama 3,\n\n750\n00:45:35,963 --> 00:45:38,933\nwhich seems to be the most impressive\nopen-source model thus far.\n\n751\n00:45:39,033 --> 00:45:40,901\nAny thoughts on open-source models?\n\n752\n00:45:42,169 --> 00:45:46,073\n-If you ask me, what are the top\n\n753\n00:45:46,173 --> 00:45:48,643\nmost important events in\nthe last couple of years?\n\n754\n00:45:48,743 --> 00:45:51,112\nI would tell you, of course, ChatGPT,\n\n755\n00:45:51,212 --> 00:45:52,747\nreinforcement learning, human feedback,\n\n756\n00:45:52,880 --> 00:45:56,951\ngrounding it to human values and having\nthe technology necessary to do that.\n\n757\n00:45:57,051 --> 00:46:02,990\nObviously a breakthrough,\nand democratized computing.\n\n758\n00:46:03,524 --> 00:46:05,760\nIt made it possible for everybody\nto be a programmer.\n\n759\n00:46:05,893 --> 00:46:08,396\nEverybody is now doing amazing\nthings with it.\n\n760\n00:46:08,496 --> 00:46:12,033\nChatGPT, the work that OpenAI did,\n\n761\n00:46:12,166 --> 00:46:14,335\nGreg and Sam and the team,\nreally proud of them.\n\n762\n00:46:15,002 --> 00:46:21,008\nThe second thing that I would say\nthat is just as important,\n\n763\n00:46:21,142 --> 00:46:25,813\nI would say, is Llama,\nnot Llama 1, but Llama 2.\n\n764\n00:46:25,913 --> 00:46:30,685\nLlama 2 activated just\nabout every industry\n\n765\n00:46:30,785 --> 00:46:34,522\nto jump into working on generative AI.\n\n766\n00:46:35,122 --> 00:46:39,093\nAnd it opened the floodgates\nof every industry\n\n767\n00:46:39,193 --> 00:46:41,262\nbeing able to access this technology,\n\n768\n00:46:41,362 --> 00:46:44,231\nhealth care, financial services, \nyou name it,\n\n769\n00:46:44,332 --> 00:46:48,669\nmanufacturing, you name it,\ncustomer service, retail, all kinds.\n\n770\n00:46:48,803 --> 00:46:51,339\nI think Llama 2 and Llama 3,\n\n771\n00:46:52,006 --> 00:46:55,042\nbecause it's open-sourced,\nit engaged research,\n\n772\n00:46:55,142 --> 00:46:59,780\nit engaged startups, engaged industry.\nIt made generative AI accessible.\n\n773\n00:46:59,880 --> 00:47:02,149\nI think that's a very big deal.\n\n774\n00:47:02,249 --> 00:47:05,820\nAnd so I think ChatGPT\ndemocratized computing.\n\n775\n00:47:05,920 --> 00:47:09,490\nI think Llama democratized generative AI.\nDoes that make sense?\n\n776\n00:47:09,590 --> 00:47:11,525\nAnd I think without it,\n\n777\n00:47:11,659 --> 00:47:16,297\nit's very hard to have activated all of\nthe research on safety\n\n778\n00:47:16,397 --> 00:47:18,833\nand all of the different ways\nof chains of thoughts.\n\n779\n00:47:18,933 --> 00:47:22,136\nAnd all the reasoning technology\nthat's now being developed\n\n780\n00:47:22,269 --> 00:47:24,505\nand all the reinforcement learning stuff.\n\n781\n00:47:24,638 --> 00:47:27,742\nAnd that stuff would have been very hard\n\n782\n00:47:27,842 --> 00:47:29,710\nto have activated without Llama.\n\n783\n00:47:29,810 --> 00:47:34,582\n-Dario Amodei was on Ezra Klein's\npodcast two weeks ago.\n\n784\n00:47:34,715 --> 00:47:37,451\nAnd he, as many others have,\n\n785\n00:47:37,551 --> 00:47:42,456\nmany others in particular who are\ninvolved with Frontier Labs,\n\n786\n00:47:42,556 --> 00:47:46,727\nwas predicting AGI in\nthe relatively near term.\n\n787\n00:47:46,827 --> 00:47:48,729\nConceivably the next couple of years,\n\n788\n00:47:48,863 --> 00:47:52,433\nyears like 2027, and so on,\nare frequently thrown around.\n\n789\n00:47:52,533 --> 00:47:54,268\nThoughts?\n\n790\n00:47:54,735 --> 00:47:57,638\n-Depending on how you define AGI.\n\n791\n00:47:57,905 --> 00:48:00,574\nNow, first of all, as an engineer,\n\n792\n00:48:00,674 --> 00:48:04,945\nyou know that we can\nonly solve a problem,\n\n793\n00:48:05,045 --> 00:48:07,381\nultimately, if you can measure it.\n\n794\n00:48:08,082 --> 00:48:11,018\nAnd so you have to express\nthe problem statement,\n\n795\n00:48:11,118 --> 00:48:15,022\nthe mission, somehow,\nin some some measurable way.\n\n796\n00:48:15,156 --> 00:48:20,161\nIf you told me that AGI is the\nlist of benchmarks we currently use,\n\n797\n00:48:20,261 --> 00:48:23,831\nthere are math tests,\nand English comprehension tests,\n\n798\n00:48:23,931 --> 00:48:30,404\nand reasoning tests,\nand you got medical exams, and bars,\n\n799\n00:48:30,504 --> 00:48:33,240\nand you make your list of all\nof the tests that you want.\n\n800\n00:48:33,340 --> 00:48:36,177\nIt doesn't matter what it is,\njust make your list.\n\n801\n00:48:36,277 --> 00:48:39,480\nIf you make your list, I am certain\n\n802\n00:48:39,580 --> 00:48:43,884\nwe will achieve excellent results\nin a very nominal amount of time.\n\n803\n00:48:44,018 --> 00:48:48,055\nAnd if that's the definition of AGI,\nI'll make a guess,\n\n804\n00:48:48,189 --> 00:48:51,125\nit's probably definitely within\nthe next five years.\n\n805\n00:48:51,225 --> 00:48:56,130\nAnd so all of the tests that we currently\nmeasure these models with,\n\n806\n00:48:56,430 --> 00:48:58,933\nthey're improving their accuracy\n\n807\n00:48:59,033 --> 00:49:03,270\nor their error rate is reducing\nin half every six months.\n\n808\n00:49:03,704 --> 00:49:06,774\nAnd so there's no reason why\nwe shouldn't expect it\n\n809\n00:49:06,874 --> 00:49:08,542\nall to be superhuman pretty soon.\n\n810\n00:49:08,642 --> 00:49:10,444\n-So again, everyone in this audience--\n-That doesn't--\n\n811\n00:49:10,544 --> 00:49:13,514\nThat doesn't meet the standard.\nJust be clear.\n\n812\n00:49:13,647 --> 00:49:17,384\nThat doesn't meet the standard\nof a normal person thinking it's AGI.\n\n813\n00:49:17,518 --> 00:49:23,123\nDoes that make sense?\nAn on-the-street-person, \"Hey AGI.\"\n\n814\n00:49:23,257 --> 00:49:26,961\nThat's probably not what they're thinking,\nwhat I defined it as.\n\n815\n00:49:27,328 --> 00:49:30,431\nThe way I defined it is simply\nan engineering way of defining it\n\n816\n00:49:30,531 --> 00:49:32,066\nso that you can answer that question.\n\n817\n00:49:32,199 --> 00:49:35,636\nThe second way of answering the question\nis when can you achieve AGI\n\n818\n00:49:35,736 --> 00:49:38,172\nin an undefined way?\n\n819\n00:49:38,272 --> 00:49:41,141\nIf it's undefinable,\nthen how long would you know?\n\n820\n00:49:41,275 --> 00:49:43,110\nHow long would it take? Undefinable.\n\n821\n00:49:43,711 --> 00:49:46,247\n-And so everyone in this audience\nagain runs a business.\n\n822\n00:49:46,380 --> 00:49:51,886\nAnd a practical question they/we \nall face is\n\n823\n00:49:52,052 --> 00:49:57,958\nhow do you know if you're, in the face of\nthe kinds of changes you just depicted,\n\n824\n00:49:58,058 --> 00:50:00,561\nhow does one know, how can one know\n\n825\n00:50:00,661 --> 00:50:06,267\nwhether one is responding appropriately,\nsufficiently, in the right ways, etc.?\n\n826\n00:50:07,001 --> 00:50:08,068\nAny advice?\n\n827\n00:50:08,168 --> 00:50:15,442\n-If you're not engaging AI actively\nand aggressively, you're doing it wrong.\n\n828\n00:50:15,543 --> 00:50:17,678\nYou're not going to lose your job to AI.\n\n829\n00:50:17,778 --> 00:50:20,447\nYou're going to lose your job\nto somebody who uses AI.\n\n830\n00:50:20,881 --> 00:50:23,784\nYour company is not going to go\nout of business because of AI.\n\n831\n00:50:23,884 --> 00:50:27,922\nYour company is going to go out of\nbusiness because another company used AI.\n\n832\n00:50:28,322 --> 00:50:29,990\nThere's no question about that.\n\n833\n00:50:30,090 --> 00:50:32,593\nAnd so you have to engage\nAI as quickly as possible.\n\n834\n00:50:32,726 --> 00:50:34,361\nYou have to engage AI\nas quickly as possible,\n\n835\n00:50:34,495 --> 00:50:39,066\nso that you could do things that\nyou think cost too much to do.\n\n836\n00:50:39,199 --> 00:50:45,172\nFor example, if the marginal cost of\nintelligence was practically zero.\n\n837\n00:50:45,272 --> 00:50:48,108\nThere are a lot of things that\nyou would do now\n\n838\n00:50:48,208 --> 00:50:49,677\nthat you wouldn't have done otherwise.\n\n839\n00:50:49,777 --> 00:50:52,880\nAnd so notice how often we do search\n\n840\n00:50:53,013 --> 00:50:56,817\nand these days notice how\noften we ask questions.\n\n841\n00:50:56,917 --> 00:51:00,521\nAny random question,\nI'll be asking Perplexity.\n\n842\n00:51:00,955 --> 00:51:02,756\nRight. And so why not.\n\n843\n00:51:02,856 --> 00:51:05,125\n-Or, just gave a talk here at Sessions.\n\n844\n00:51:05,225 --> 00:51:07,861\n-Okay, I love using it.\n\n845\n00:51:07,962 --> 00:51:12,099\nAnd even if I know the answer,\nI'll just ask it anyways,\n\n846\n00:51:12,199 --> 00:51:13,867\njust to see what it comes up with.\n\n847\n00:51:14,001 --> 00:51:18,372\nAnd so, I think we want that to happen.\n\n848\n00:51:18,472 --> 00:51:19,807\nWe want the marginal cost\n\n849\n00:51:19,940 --> 00:51:22,076\nof these type of activities\nto be as low as possible\n\n850\n00:51:22,176 --> 00:51:24,078\nso that you use it in abundance.\n\n851\n00:51:24,211 --> 00:51:28,582\nSecond, if you could use AI\nto be productive--\n\n852\n00:51:29,817 --> 00:51:34,221\nYou know that productive companies\nleads to higher earnings.\n\n853\n00:51:34,355 --> 00:51:36,590\nHigher earnings leads to more employment.\n\n854\n00:51:37,725 --> 00:51:39,860\nMore employment leads to\nmore social growth.\n\n855\n00:51:39,994 --> 00:51:44,565\nAnd so, there's a lot of reasons to want \nto drive productivity into companies.\n\n856\n00:51:44,665 --> 00:51:49,603\n-And apart from just changing your\nmanufacturing plans and your CapEx plans,\n\n857\n00:51:49,703 --> 00:51:53,374\nhow has AI changed how NVIDIA\nworks internally?\n\n858\n00:51:53,907 --> 00:51:56,510\n-We were one of the first technology\ncompanies to invest\n\n859\n00:51:56,610 --> 00:51:59,213\nin our own AI supercomputers.\n\n860\n00:51:59,313 --> 00:52:03,484\nWe can't design a chip\nanymore without AI.\n\n861\n00:52:03,584 --> 00:52:08,222\nAt night, our AIs are\nexploring design spaces,\n\n862\n00:52:08,322 --> 00:52:10,691\nvast and wide that we would\nnever do ourselves\n\n863\n00:52:10,791 --> 00:52:12,459\nbecause it costs too much\nmoney to explore it.\n\n864\n00:52:12,559 --> 00:52:18,666\nAnd so our chips are so much better.\n\n865\n00:52:18,766 --> 00:52:22,036\nBecause of an AI, we could reduce the\namount of energy used for our chips\n\n866\n00:52:22,136 --> 00:52:24,371\nas higher performance.\n\n867\n00:52:24,772 --> 00:52:28,475\nOur software. We can't write software\nwithout AI anymore.\n\n868\n00:52:28,609 --> 00:52:30,077\nWe have to explore all the--\n\n869\n00:52:30,177 --> 00:52:34,415\nthe design space of optimizing\ncompilers is too large.\n\n870\n00:52:34,515 --> 00:52:38,552\nWe use AIs to file bugs.\n\n871\n00:52:38,652 --> 00:52:40,688\nSo our bugs database\n\n872\n00:52:40,788 --> 00:52:46,460\nactually tells you what's wrong with\nthe code, who's likely involved\n\n873\n00:52:46,560 --> 00:52:49,697\nand activates that person to go fix it.\n\n874\n00:52:49,830 --> 00:52:53,701\nSo, I think I want everybody,\n\n875\n00:52:53,801 --> 00:52:56,470\nevery organization in our company\nto use AI very aggressively.\n\n876\n00:52:56,603 --> 00:53:00,674\nI want to turn NVIDIA into a one giant AI.\nHow great would that be?\n\n877\n00:53:01,241 --> 00:53:02,943\nAnd then I'll have work life balance.\n\n878\n00:53:05,846 --> 00:53:07,948\nAre there any favorite examples\nyou've heard of\n\n879\n00:53:08,048 --> 00:53:11,485\nbusinesses in maybe some kind of \nunexpected sector,\n\n880\n00:53:11,585 --> 00:53:14,788\nsome unexpected use case\nwhere you feel that they\n\n881\n00:53:14,888 --> 00:53:18,192\ncan serve as a poster child for some\nof the dynamics you're describing,\n\n882\n00:53:18,325 --> 00:53:22,763\nwhere they've really realized\nsome of this opportunity?\n\n883\n00:53:23,797 --> 00:53:29,503\n-The biggest surprise of AI that shouldn't\nbe a surprise for a lot of people,\n\n884\n00:53:29,603 --> 00:53:35,109\nis that when we say it's\na large language model,\n\n885\n00:53:35,242 --> 00:53:38,679\nthe word language doesn't mean\nhuman language only,\n\n886\n00:53:38,812 --> 00:53:42,616\nand it doesn't mean English only,\nor French only,\n\n887\n00:53:42,750 --> 00:53:46,820\nor Irish only, or that's a whole\ndifferent language. But...\n\n888\n00:53:48,255 --> 00:53:50,257\nIs there a large language\nmodel for Irish?\n\n889\n00:53:50,591 --> 00:53:53,193\n-I've tried it.\n-It works?\n\n890\n00:53:53,293 --> 00:53:55,662\n-Yeah, it works well.\n\n891\n00:53:55,763 --> 00:54:01,769\nJohn and I spent most of our education\nin Ireland, being taught in Irish.\n\n892\n00:54:02,069 --> 00:54:06,340\nAnd so, these models are\nsome of the first people\n\n893\n00:54:06,440 --> 00:54:12,179\nI've had the chance to have\na dialog with as Gaeilge [in Irish].\n\n894\n00:54:12,312 --> 00:54:13,347\n-Very surprising.\n\n895\n00:54:13,447 --> 00:54:14,782\n-In many years. Yeah.\n\n896\n00:54:14,882 --> 00:54:17,584\nThey do well, and actually I've been--\nHave you played with Suno?\n\n897\n00:54:17,684 --> 00:54:24,358\n-Suno?\n-Suno is an app for creating music,\n\n898\n00:54:24,558 --> 00:54:26,193\nkind of synthetic music.\n-Okay.\n\n899\n00:54:26,293 --> 00:54:29,429\n-And I've been enjoying creating...\n-Irish music.\n\n900\n00:54:29,563 --> 00:54:31,165\n-I of course tested it on that.\n\n901\n00:54:31,265 --> 00:54:34,201\nAnd Celtic dubstep is\na thing that it can do.\n\n902\n00:54:35,335 --> 00:54:37,237\n-Fantastic. Okay. Makes sense.\n\n903\n00:54:37,371 --> 00:54:39,239\nLike if it could do that,\n\n904\n00:54:39,473 --> 00:54:42,442\nthen of course it could learn\nthe language of life.\n\n905\n00:54:43,477 --> 00:54:45,012\nOf course they can learn the--\n\n906\n00:54:45,112 --> 00:54:48,415\nAnd if a language model\ncan understand sound\n\n907\n00:54:48,515 --> 00:54:52,586\nwhich is a sequence time series,\nit's a sequence.\n\n908\n00:54:52,686 --> 00:54:57,057\nWhy can't it learn robotics articulation\nwhich is a sequence.\n\n909\n00:54:57,191 --> 00:55:00,260\nYou just have to figure out\nhow to tokenize it.\n\n910\n00:55:00,360 --> 00:55:04,097\nAnd so the idea that all of\na sudden, \"Oh hey, look, listen,\n\n911\n00:55:04,198 --> 00:55:08,001\nI could also learn SQL,\nI could learn ABAP, I can learn Lightning,\n\n912\n00:55:08,101 --> 00:55:10,671\nI can learn all these proprietary\nlanguages, I can learn Verilog,\n\n913\n00:55:10,804 --> 00:55:14,842\nI can learn, right. So all of a sudden\nyou realized, hang on a second,\n\n914\n00:55:14,942 --> 00:55:18,412\nI can put a copilot on top of\nevery tool on the planet.\n\n915\n00:55:18,512 --> 00:55:21,582\n-And to this point,\nNVIDIA being one big AI,\n\n916\n00:55:21,682 --> 00:55:26,019\nis the future one of 100,000 models\nor 100 million models,\n\n917\n00:55:26,119 --> 00:55:27,988\nor is the future one of one model?\n\n918\n00:55:28,121 --> 00:55:29,990\nAnd they're just a model\nthat does all the things.\n\n919\n00:55:30,224 --> 00:55:37,631\n-I think that it would be great\nto have super models\n\n920\n00:55:37,731 --> 00:55:40,033\nthat help you reason about things,\nin general.\n\n921\n00:55:40,133 --> 00:55:47,207\nBut, for us, for all companies that\nhave very specific\n\n922\n00:55:47,307 --> 00:55:49,042\ndomain-specific expertise,\n\n923\n00:55:49,176 --> 00:55:51,912\nwe're going to have to\ntrain our own models.\n\n924\n00:55:52,045 --> 00:55:54,581\nAnd the reason for that is because\nwe have a proprietary language.\n\n925\n00:55:54,681 --> 00:55:58,552\nThat difference between 99% and 99.3%\n\n926\n00:55:58,652 --> 00:56:01,355\nis the difference between\nlife and death for us.\n\n927\n00:56:01,455 --> 00:56:03,190\nAnd so it's too valuable to us.\n\n928\n00:56:03,290 --> 00:56:06,393\nNo different than fraud detection for you,\nit's too important to you.\n\n929\n00:56:06,493 --> 00:56:09,463\n-That's been exactly our experience.\n-Yeah. It's too important to you.\n\n930\n00:56:09,563 --> 00:56:12,466\nHowever good the general model is,\n\n931\n00:56:12,599 --> 00:56:15,235\nyou're going to want to take that\nand fine-tune it, right,\n\n932\n00:56:15,369 --> 00:56:19,373\nimprove it into perfection because it's\njust too important to you. Yeah.\n\n933\n00:56:19,506 --> 00:56:22,142\n-And so we're going to shortly\nrun out of time here.\n\n934\n00:56:22,242 --> 00:56:24,678\nAnd there's a whole bunch of questions\nI haven't gotten to yet.\n\n935\n00:56:24,778 --> 00:56:28,548\nI've exercised poor discipline on\nthe time management front.\n\n936\n00:56:28,682 --> 00:56:33,253\nSo there's a bunch that I think \nI was told I definitely had to ask you,\n\n937\n00:56:33,353 --> 00:56:36,723\nbut there's a couple that I really wanted\nto ask and it's only us up here.\n\n938\n00:56:36,857 --> 00:56:42,496\nSo, Lisa Sue is your\nfirst cousin once removed?\n\n939\n00:56:42,596 --> 00:56:44,598\n-Yes. Yeah. She's terrific. She's amazing.\n\n940\n00:56:44,731 --> 00:56:48,302\n-And AMD is now--\n-She's the CEO of AMD, by the way.\n\n941\n00:56:48,402 --> 00:56:52,072\n-Yeah, and AMD is now one of your\ncompetitors in the GPU space.\n\n942\n00:56:52,472 --> 00:56:53,507\n-No.\n\n943\n00:56:55,842 --> 00:56:57,844\nWe're family.\n\n944\n00:56:57,945 --> 00:57:01,448\n-Okay.\n-We're all in the industry.\n\n945\n00:57:01,548 --> 00:57:02,683\n-One of your partners in the industry.\n\n946\n00:57:02,783 --> 00:57:04,651\n-Yeah, yeah. We buy from AMD.\n\n947\n00:57:04,818 --> 00:57:06,353\n-What's going on in the water?\n\n948\n00:57:06,486 --> 00:57:09,456\nHow did we end up with two of the,\n\n949\n00:57:09,556 --> 00:57:14,227\narguably the two most important, GPU\ncompanies being run by close relatives?\n\n950\n00:57:14,328 --> 00:57:15,329\nWhat's going on?\n\n951\n00:57:17,731 --> 00:57:20,534\n-You got to keep it close to the family.\n\n952\n00:57:20,634 --> 00:57:23,370\nNo, I have no idea how it happened.\n\n953\n00:57:23,470 --> 00:57:25,038\nWe didn't grow up together.\n\n954\n00:57:25,138 --> 00:57:27,374\n-And that makes it even\nmore interesting, right?\n\n955\n00:57:27,507 --> 00:57:30,944\n-Yeah, yeah. We didn't even know each\nother until she was at IBM.\n\n956\n00:57:31,044 --> 00:57:35,716\nAnd her career is incredible. \nAnd she's really quite extraordinary.\n\n957\n00:57:38,218 --> 00:57:40,620\n-I think this question\nrequires further study.\n\n958\n00:57:40,721 --> 00:57:45,525\nSo you've been operating\nin Silicon Valley since the early 90s.\n\n959\n00:57:45,659 --> 00:57:47,694\n-Yes.\n\n960\n00:57:47,794 --> 00:57:49,896\n-How has Silicon Valley culture \nchanged in that time?\n\n961\n00:57:52,132 --> 00:57:53,233\n-Oh, wow.\n\n962\n00:57:55,369 --> 00:57:56,870\nI haven't thought about\nthis in a long time.\n\n963\n00:57:57,004 --> 00:58:03,677\nI guess in a lot of ways probably--\nOkay, here's one.\n\n964\n00:58:04,011 --> 00:58:11,184\nWhen I first started NVIDIA,\nI was 29 years old, and...\n\n965\n00:58:12,252 --> 00:58:19,826\nI was 29 years old with acne and,\n\n966\n00:58:19,926 --> 00:58:24,097\nyou go talk to your talk to your-- \ngo recruit law firms\n\n967\n00:58:24,197 --> 00:58:28,869\nand VCs and I got a big zit on my forehead\n\n968\n00:58:28,969 --> 00:58:33,140\nand, I don't have one today,\nso I feel comfortable talking about it,\n\n969\n00:58:33,240 --> 00:58:34,941\nbut it could it could happen.\n\n970\n00:58:35,075 --> 00:58:38,678\nAnd so anyways,\nyou feel rather insecure\n\n971\n00:58:38,812 --> 00:58:43,617\nbecause most of CEOs back then wore suits\nand they're quite accomplished.\n\n972\n00:58:43,750 --> 00:58:47,120\nAnd they sound like adults and\nthey use big words\n\n973\n00:58:47,220 --> 00:58:50,090\nand they talk about business\nand things like that.\n\n974\n00:58:50,190 --> 00:58:53,560\nAnd, so, when you're young,\nyou feel rather intimidated.\n\n975\n00:58:53,660 --> 00:58:55,762\nYou're surrounded by a bunch of adults.\n\n976\n00:58:55,896 --> 00:59:00,333\nWell, now if you don't have acne,\n\n977\n00:59:00,834 --> 00:59:02,903\nI don't think you deserve\nto start a company.\n\n978\n00:59:08,008 --> 00:59:11,411\nThat's one big difference. Acne.\n\n979\n00:59:11,511 --> 00:59:14,047\nThe takeaway from Jensen's speech.\n\n980\n00:59:14,181 --> 00:59:20,287\nWhat it means is really we've enabled\nyounger people to be extraordinary.\n\n981\n00:59:20,387 --> 00:59:23,090\nI think the young generation of CEOs,\n\n982\n00:59:23,423 --> 00:59:26,493\nthe type of things that you guys know\nat such a such a young age,\n\n983\n00:59:26,593 --> 00:59:27,627\nis really quite extraordinary.\n\n984\n00:59:27,727 --> 00:59:30,697\nI mean, it took me decades to learn it.\n\n985\n00:59:30,831 --> 00:59:32,365\n-Last question.\n\n986\n00:59:33,233 --> 00:59:36,036\n-That was a compliment. \nSee how quickly he changed the...\n\n987\n00:59:37,838 --> 00:59:40,340\nI wasn't saying you have acne. \nI was just saying you were smart.\n\n988\n00:59:43,310 --> 00:59:46,513\n-NVIDIA has a market cap\nof roughly $2 trillion.\n\n989\n00:59:46,613 --> 00:59:53,653\nAnd you're now within spitting\ndistance of Apple and Microsoft.\n\n990\n00:59:53,753 --> 00:59:56,590\nAnd I just checked and they have\n\n991\n00:59:56,690 --> 01:00:02,863\n220,000 and 160,000 employees,\nrespectively.\n\n992\n01:00:02,963 --> 01:00:06,766\nNVIDIA has 28,000 employees, so,\n\n993\n01:00:06,900 --> 01:00:10,770\nless than a fifth of the\nsmaller of the two there.\n\n994\n01:00:11,404 --> 01:00:15,709\nAnd then you just said when we\nwere chatting backstage,\n\n995\n01:00:15,809 --> 01:00:22,516\nand I jotted this down, \"You can achieve\noperational excellence through process,\n\n996\n01:00:22,649 --> 01:00:25,785\nbut craft can only be\nachieved with tenure.\"\n\n997\n01:00:26,553 --> 01:00:32,058\nAnd so NVIDIA is considerably smaller\nthan any of the other giants.\n\n998\n01:00:32,192 --> 01:00:34,761\nAnd you seem to think that\ntenure really matters.\n\n999\n01:00:34,861 --> 01:00:37,164\nAnd I guess that craft really matters.\n\n1000\n01:00:37,264 --> 01:00:41,401\nDo you want to say\na little bit more there?\n\n1001\n01:00:43,470 --> 01:00:48,341\n-I think extraordinary things-- I think\na lot of good things could be made--\n\n1002\n01:00:48,441 --> 01:00:52,612\ngood things are made\nwith operational excellence.\n\n1003\n01:00:52,712 --> 01:00:58,318\nBut you can't make extraordinary things\nthrough just operational excellence.\n\n1004\n01:00:58,451 --> 01:01:01,855\nAnd the reason for that is because\na lot of the great things\n\n1005\n01:01:01,955 --> 01:01:05,659\nin your body of work and\nthe products that you make,\n\n1006\n01:01:05,759 --> 01:01:10,564\nthe company you created,\nthe organizations you've nurtured,\n\n1007\n01:01:10,830 --> 01:01:13,867\nit takes loving care.\n\n1008\n01:01:13,967 --> 01:01:15,936\nYou can't even put it in words.\n\n1009\n01:01:16,069 --> 01:01:18,972\nHow do you put love and care in an email?\n\n1010\n01:01:19,105 --> 01:01:22,342\nAnd for people to go,\n\"Oh, I know exactly what to do.\"\n\n1011\n01:01:22,442 --> 01:01:27,480\nYou can't put that in a business process,\nlove and care.\n\n1012\n01:01:28,481 --> 01:01:31,184\n-Is Love and Care an NVIDIA catchphrase?\n\n1013\n01:01:31,685 --> 01:01:37,991\n-Well I use Love fairly abundantly,\nand Care I use abundantly.\n\n1014\n01:01:38,124 --> 01:01:41,061\n-At Stripe we talk a lot\nabout Craft and Beauty.\n\n1015\n01:01:41,161 --> 01:01:43,430\n-Yeah, right.\nYou have to use these words because\n\n1016\n01:01:43,563 --> 01:01:47,834\nin a lot of ways there are\nno other words to describe it.\n\n1017\n01:01:48,301 --> 01:01:52,239\nYou can't put in numbers, you can't write\nit in the product specification.\n\n1018\n01:01:52,372 --> 01:01:53,707\nThe product specification says,\n\n1019\n01:01:53,840 --> 01:01:56,610\n\"I want you to build something great\nthat's incredibly beautiful.\n\n1020\n01:01:56,710 --> 01:02:01,047\nThat's in great, great craft.\"\nYou can't specify these things.\n\n1021\n01:02:01,147 --> 01:02:02,816\n-But I'm sure there's\npeople at Stripe who think\n\n1022\n01:02:02,916 --> 01:02:05,485\nPatrick's always yammering on\nabout Craft and Beauty.\n\n1023\n01:02:05,585 --> 01:02:07,921\n-And it's this kind of-- \n-I never yammer.\n\n1024\n01:02:08,021 --> 01:02:11,258\nI just want to let you know that.\nI don't even know what that sounds like.\n\n1025\n01:02:11,491 --> 01:02:13,627\n-Okay. Well, yeah.\n-Yammering on. Go ahead, go ahead. Yeah.\n\n1026\n01:02:13,760 --> 01:02:15,762\n-You're more lucid than I am.\n\n1027\n01:02:15,895 --> 01:02:17,430\nI just babble, but, hey,\n\n1028\n01:02:17,530 --> 01:02:20,166\n\"So Patrick is always going\non about this Craft and Beauty stuff,\n\n1029\n01:02:20,267 --> 01:02:24,604\nand wants things to have this particular\nineffable character,\n\n1030\n01:02:24,738 --> 01:02:27,907\nbut it doesn't directly serve some\ncustomer need and so forth.\"\n\n1031\n01:02:28,041 --> 01:02:30,010\nLike customers aren't coming\nto us and saying,\n\n1032\n01:02:30,143 --> 01:02:31,678\n\"I want the product to be more beautiful.\"\n\n1033\n01:02:31,778 --> 01:02:33,880\nThey're saying,\n\"I want it to feature X or feature Y.\"\n\n1034\n01:02:34,114 --> 01:02:37,117\nAnd yet we believe that\nthe Craft and Beauty really matters.\n\n1035\n01:02:37,250 --> 01:02:40,620\nSounds like you're getting at something\nsimilar. Why do you think it matters?\n\n1036\n01:02:41,521 --> 01:02:45,125\n-Actually  your customers,\neven though they didn't say it,\n\n1037\n01:02:45,225 --> 01:02:50,430\nthey might not have the words to say it,\nbut when they experience it, they know it.\n\n1038\n01:02:50,530 --> 01:02:54,167\nThere's no question.\nLook...\n\n1039\n01:02:55,001 --> 01:03:01,174\nStripe's work has beauty,\nhas elegance, has simplicity.\n\n1040\n01:03:01,341 --> 01:03:03,777\nSimplicity is not simple,\nas you guys know.\n\n1041\n01:03:03,877 --> 01:03:06,646\nSimplicity and simple are not...\nnot the same thing.\n\n1042\n01:03:06,746 --> 01:03:12,752\nAnd it has elegance and\nit solves the problem, but just enough,\n\n1043\n01:03:12,852 --> 01:03:15,322\nit burdens you, but not too much,\n\n1044\n01:03:15,422 --> 01:03:18,758\nand so that balance is hard to find.\n\n1045\n01:03:18,858 --> 01:03:21,361\nAnd you can't specify that,\nyou just feel your way there.\n\n1046\n01:03:21,461 --> 01:03:24,664\nAnd when you have a team that is with you,\n\n1047\n01:03:24,764 --> 01:03:27,567\nthat feels the way they're together,\n\n1048\n01:03:27,667 --> 01:03:32,172\nin a lot of ways we've codified,\nwe've encoded,\n\n1049\n01:03:32,672 --> 01:03:37,444\nthe magic of the company\nin a way that no words can describe.\n\n1050\n01:03:37,544 --> 01:03:39,913\nAnd you don't want to lose that.\nYou don't want to lose that.\n\n1051\n01:03:40,013 --> 01:03:43,283\nYou want to take that and take it\nto the next level next time.\n\n1052\n01:03:43,383 --> 01:03:45,552\nAnd so I don't want to reset.\n\n1053\n01:03:45,652 --> 01:03:50,123\nI don't like working with\nnew people for that reason.\n\n1054\n01:03:50,223 --> 01:03:54,160\nBecause I've encoded, I've embodied,\nI've deposited so much pain,\n\n1055\n01:03:54,361 --> 01:03:57,464\nsuffering, joy, knowledge. Right.\n\n1056\n01:03:57,597 --> 01:03:59,065\nAll that experience, life experience.\n\n1057\n01:03:59,165 --> 01:04:01,368\nYou've encoded it in all the people\nthat you've worked with.\n\n1058\n01:04:01,501 --> 01:04:04,170\nYou want to carry it on.\nYou want to take it to the next level.\n\n1059\n01:04:04,304 --> 01:04:08,708\nAnd that's really the reason why\nI really, deeply believe in tenure.\n\n1060\n01:04:09,109 --> 01:04:13,346\nAnd because of that,\nsmall teams could do great things.\n\n1061\n01:04:13,446 --> 01:04:16,516\nAnd NVIDIA is kind of a small team\nwith 28,000 people,\n\n1062\n01:04:16,649 --> 01:04:20,854\npeople think we punch well above\nour weight because of that reason.\n\n1063\n01:04:20,987 --> 01:04:24,290\nAnd so it's amazing \nwhat you guys have done\n\n1064\n01:04:24,391 --> 01:04:26,760\nand how incredibly small you are,\n\n1065\n01:04:26,860 --> 01:04:31,531\n7,000 people supporting\n$1 trillion worth of\n\n1066\n01:04:31,631 --> 01:04:36,269\necosystem and industry and economy\n\n1067\n01:04:36,369 --> 01:04:38,304\nand who knows how\nfar you guys can go.\n\n1068\n01:04:38,405 --> 01:04:39,606\nSo I'm very proud of you.\n\n1069\n01:04:39,706 --> 01:04:41,207\n-Jensen. Thank you.\n\n","srtEs":"1\r\n00:00:11,909 --> 00:00:15,512\r\nBienvenido de nuevo al escenario, Patrick Collison.\r\n\r\n2\r\n00:00:28,792 --> 00:00:30,494\r\n-[Patrick] Muy bien. Buenas tardes, amigos.\r\n\r\n3\r\n00:00:30,594 --> 00:00:33,697\r\nEspero que hayas disfrutado de las sesiones hasta ahora.\r\n\r\n4\r\n00:00:33,830 --> 00:00:36,233\r\ny cuando te vimos por Ãºltima vez esta maÃ±ana.\r\n\r\n5\r\n00:00:36,800 --> 00:00:40,737\r\nPara la conferencia principal de esta tarde, o charla informal, supongo,\r\n\r\n6\r\n00:00:40,871 --> 00:00:44,074\r\nEstoy a punto de presentarles a alguien que no necesita presentaciÃ³n.\r\n\r\n7\r\n00:00:44,207 --> 00:00:48,211\r\nAunque un dato curioso que quizÃ¡s no sepas sobre Jensen Huang es que\r\n\r\n8\r\n00:00:48,312 --> 00:00:51,582\r\nEste mes cumple 31 aÃ±os como CEO de NVIDIA,\r\n\r\n9\r\n00:00:51,682 --> 00:00:55,586\r\nlo que lo convierte en el CEO con mÃ¡s aÃ±os de servicio en la industria tecnolÃ³gica\r\n\r\n10\r\n00:00:55,686 --> 00:00:58,755\r\ny por lo tanto lÃ³gicamente--\r\n\r\n11\r\n00:01:03,327 --> 00:01:06,563\r\nJohn y yo sÃ³lo llevamos haciÃ©ndolo unos 14 aÃ±os.\r\n\r\n12\r\n00:01:06,697 --> 00:01:11,168\r\nAsÃ­ que, incluso si duplicamos eso, todavÃ­a estaremos en segundo lugar detrÃ¡s de Ã©l.\r\n\r\n13\r\n00:01:12,836 --> 00:01:15,572\r\nJensen, bueno, hablaremos de esto en el escenario.\r\n\r\n14\r\n00:01:15,906 --> 00:01:20,310\r\nAsistiÃ³ al Instituto Bautista Oneida en Kentucky.\r\n\r\n15\r\n00:01:20,911 --> 00:01:22,379\r\nDefinitivamente le preguntaremos sobre esto.\r\n\r\n16\r\n00:01:23,880 --> 00:01:30,020\r\nEstado de Oregon, trabajÃ³ como camarero en Denny's.\r\n\r\n17\r\n00:01:30,320 --> 00:01:32,623\r\nDe hecho, hay un Denny's cerca de aquÃ­.\r\n\r\n18\r\n00:01:32,723 --> 00:01:35,892\r\nLSI Logic y luego AMD, que por supuesto es\r\n\r\n19\r\n00:01:35,993 --> 00:01:39,963\r\nAhora dirigido por su primo hermano, que una vez fue eliminado.\r\n\r\n20\r\n00:01:40,130 --> 00:01:41,932\r\nDefinitivamente preguntaremos sobre eso.\r\n\r\n21\r\n00:01:42,265 --> 00:01:45,435\r\nAntes de fundar NVIDIA en 1993...\r\n\r\n22\r\n00:01:45,602 --> 00:01:53,343\r\ny la capitalizaciÃ³n de mercado de NVIDIA era de 8 mil millones de dÃ³lares cuando Stripe se lanzÃ³ en 2011.\r\n\r\n23\r\n00:01:53,710 --> 00:01:59,016\r\nY ahora, por supuesto, es mÃ¡s de 200 veces mayor. AsÃ­ que ha estado ocupado desde entonces.\r\n\r\n24\r\n00:01:59,282 --> 00:02:01,451\r\nPor favor, den la bienvenida al escenario a Jensen Huang.\r\n\r\n25\r\n00:02:16,867 --> 00:02:18,235\r\n-[Jensen] Â¡Hola a todos!\r\n\r\n26\r\n00:02:20,037 --> 00:02:22,773\r\n-[Patrick] Â¿Entonces viste la conferencia principal antes?\r\n\r\n27\r\n00:02:22,873 --> 00:02:26,910\r\n-SÃ­. Nunca habÃ­a visto un dueto. -[Patrick riendo] Bueno...\r\n\r\n28\r\n00:02:27,010 --> 00:02:30,881\r\n-Nunca habÃ­a visto un dueto. Estaban tan sincronizados.\r\n\r\n29\r\n00:02:31,081 --> 00:02:35,252\r\nParecÃ­a que se conocÃ­an. Es increÃ­ble.\r\n\r\n30\r\n00:02:35,686 --> 00:02:38,088\r\n-AlgÃºn conocido.\r\n\r\n31\r\n00:02:38,188 --> 00:02:40,891\r\nHas estado haciendo conferencias magistrales durante mucho tiempo.\r\n\r\n32\r\n00:02:40,991 --> 00:02:44,628\r\nTÃº eres la cabra clave. AsÃ­ que danos...\r\n\r\n33\r\n00:02:44,995 --> 00:02:46,163\r\n-Basta.\r\n\r\n34\r\n00:02:46,263 --> 00:02:49,633\r\n-Danos tu, como... aÃºn no tenemos ni siquiera un traje caracterÃ­stico.\r\n\r\n35\r\n00:02:49,766 --> 00:02:53,270\r\nAquÃ­ solo somos aficionados. AsÃ­ que danos...\r\n\r\n36\r\n00:02:53,837 --> 00:02:55,505\r\n-Es porque todavÃ­a eres joven.\r\n\r\n37\r\n00:02:56,873 --> 00:03:00,110\r\nBueno, danos tu anÃ¡lisis de la presentaciÃ³n principal. Â¿QuÃ© te pareciÃ³?\r\n\r\n38\r\n00:03:00,210 --> 00:03:05,282\r\n-PensÃ© que era A+. PensÃ© que era A+. Â¡En serio!\r\n\r\n39\r\n00:03:06,850 --> 00:03:10,921\r\nLo explicaste perfectamente\r\n\r\n40\r\n00:03:11,021 --> 00:03:15,759\r\nEl propÃ³sito de la empresa. Â¿QuÃ© los inspira?\r\n\r\n41\r\n00:03:15,892 --> 00:03:18,695\r\nÂ¿QuÃ© los mantiene despiertos? Â¿QuÃ© los motiva a trabajar tan duro?\r\n\r\n42\r\n00:03:18,962 --> 00:03:22,833\r\nEl ecosistema al que sirves, la increÃ­ble plataforma que construiste,\r\n\r\n43\r\n00:03:22,999 --> 00:03:26,436\r\nLa increÃ­ble contribuciÃ³n que usted hace a la economÃ­a mundial.\r\n\r\n44\r\n00:03:26,536 --> 00:03:28,238\r\nEs increÃ­ble. Â¡Me pareciÃ³ genial!\r\n\r\n45\r\n00:03:28,338 --> 00:03:31,908\r\nY habÃ­a un montÃ³n de cosas tecnolÃ³gicas, cosas de caracterÃ­sticas, cosas de dinero.\r\n\r\n46\r\n00:03:32,142 --> 00:03:35,512\r\nNo entendÃ­ nada de eso, pero...\r\n\r\n47\r\n00:03:35,612 --> 00:03:39,616\r\nAlgo sobre un CYK o algo asÃ­, Â¿quÃ© era eso?\r\n\r\n48\r\n00:03:39,716 --> 00:03:43,053\r\n-Un KYC. -KYC, sÃ­.\r\n\r\n49\r\n00:03:43,186 --> 00:03:44,187\r\nPensÃ© que era...\r\n\r\n50\r\n00:03:44,321 --> 00:03:45,489\r\n-Es un gran problema en nuestro mundo.\r\n\r\n51\r\n00:03:45,589 --> 00:03:47,457\r\n-Â¿Es cierto? Â¿Kentucky Fried Chicken?\r\n\r\n52\r\n00:03:48,191 --> 00:03:52,395\r\n-Nos encargamos del KYC para que puedas asociarnos con Kentucky Fried Chicken.\r\n\r\n53\r\n00:03:52,529 --> 00:03:53,730\r\n-EstÃ¡ bien. Entendido.\r\n\r\n54\r\n00:03:54,131 --> 00:03:57,667\r\n-Servicios financieros definidos por software,\r\n\r\n55\r\n00:03:57,768 --> 00:04:00,537\r\nEsta idea, Â¿te hizo sentido?\r\n\r\n56\r\n00:04:00,637 --> 00:04:02,873\r\n-Bueno, lo primero que creo es una idea gigante.\r\n\r\n57\r\n00:04:03,640 --> 00:04:05,308\r\n-Â¿Sabes de donde vino?\r\n\r\n58\r\n00:04:07,410 --> 00:04:09,112\r\n-Â¿Me lo vas a decir?\r\n\r\n59\r\n00:04:09,980 --> 00:04:12,115\r\n-Entonces Jensen y yo nos estÃ¡bamos poniendo al dÃ­a...\r\n\r\n60\r\n00:04:12,215 --> 00:04:16,219\r\n-La parte que me encantÃ³ fue cÃ³mo te diste cuenta.\r\n\r\n61\r\n00:04:16,319 --> 00:04:22,959\r\nAl principio, los pagos financieros tenÃ­an que ver con cÃ³digo y no con finanzas.\r\n\r\n62\r\n00:04:23,059 --> 00:04:24,661\r\nMe pareciÃ³ increÃ­ble.\r\n\r\n63\r\n00:04:24,761 --> 00:04:27,097\r\nY me lo explicaste la primera vez que nos conocimos.\r\n\r\n64\r\n00:04:27,197 --> 00:04:29,766\r\nEntonces, Jensen y yo nos estÃ¡bamos poniendo al dÃ­a hace unos 18 meses,\r\n\r\n65\r\n00:04:29,900 --> 00:04:32,702\r\nY supongo que han pasado un par de aÃ±os desde la Ãºltima vez que hablamos.\r\n\r\n66\r\n00:04:32,836 --> 00:04:36,506\r\nEntonces Ã©l me pidiÃ³ la actualizaciÃ³n sobre Stripe, y yo le expliquÃ©.\r\n\r\n67\r\n00:04:36,606 --> 00:04:43,013\r\nY usted dijo: \"Oh, entonces es como una red definida por software, pero por dinero\".\r\n\r\n68\r\n00:04:43,113 --> 00:04:46,683\r\nY eso todavÃ­a resonaba en mi mente.\r\n\r\n69\r\n00:04:46,817 --> 00:04:50,821\r\nDe ahÃ­ surgiÃ³ la idea de los servicios financieros definidos por software.\r\n\r\n70\r\n00:04:50,954 --> 00:04:52,756\r\nAsÃ­ que espero que no tengamos que pagar una tarifa de licencia por eso.\r\n\r\n71\r\n00:04:52,889 --> 00:04:54,691\r\n-No recibÃ­ ningÃºn valor por esa buena idea.\r\n\r\n72\r\n00:04:54,825 --> 00:04:56,726\r\n-EstÃ¡ bien.\r\n\r\n73\r\n00:04:56,827 --> 00:04:58,562\r\nEstÃ¡n bien, chicos. Estaba pensando en esto.\r\n\r\n74\r\n00:04:58,695 --> 00:05:01,131\r\nLas ganancias de Tesla se informaron ayer,\r\n\r\n75\r\n00:05:01,231 --> 00:05:06,269\r\ny Elon anunciÃ³ que Tesla tendrÃ¡ 85.000\r\n\r\n76\r\n00:05:06,570 --> 00:05:09,272\r\nA100 a finales de este aÃ±o.\r\n\r\n77\r\n00:05:09,406 --> 00:05:12,142\r\nY estaba reflexionando sobre...\r\n\r\n78\r\n00:05:12,242 --> 00:05:14,110\r\nEs todo un Ã©xito construir un negocio.\r\n\r\n79\r\n00:05:14,211 --> 00:05:17,747\r\ndonde los directores ejecutivos compiten entre sÃ­ para anunciar\r\n\r\n80\r\n00:05:17,848 --> 00:05:20,050\r\nÂ¿QuiÃ©n ha gastado mÃ¡s comprando tu producto?\r\n\r\n81\r\n00:05:20,183 --> 00:05:22,652\r\nAsÃ­ que creo que has hecho algo bastante impresionante.\r\n\r\n82\r\n00:05:22,786 --> 00:05:24,754\r\nPero de todos modos, en realidad quiero comenzar hablando un poco sobre...\r\n\r\n83\r\n00:05:24,888 --> 00:05:28,859\r\n-Todos mis amigos directores ejecutivos son los que mÃ¡s tienen.\r\n\r\n84\r\n00:05:32,229 --> 00:05:35,866\r\n-Entonces, quiero comenzar hablando un poco sobre,\r\n\r\n85\r\n00:05:36,733 --> 00:05:41,771\r\nun comentario que usted hizo en un evento de Stanford recientemente, en el GSB, creo.\r\n\r\n86\r\n00:05:41,938 --> 00:05:49,379\r\nY dijiste: \"Os deseo abundantes dosis de dolor y sufrimiento\".\r\n\r\n87\r\n00:05:50,614 --> 00:05:51,615\r\nElaborar.\r\n\r\n88\r\n00:05:53,917 --> 00:06:00,357\r\nâ€”Bueno. A ver. Hay un malentendido.\r\n\r\n89\r\n00:06:01,224 --> 00:06:02,559\r\nHay una frase que dice:\r\n\r\n90\r\n00:06:02,659 --> 00:06:08,632\r\n\"Debes elegir tu carrera en funciÃ³n de tu pasiÃ³n\".\r\n\r\n91\r\n00:06:08,732 --> 00:06:13,536\r\nGeneralmente la gente relaciona la pasiÃ³n con la felicidad.\r\n\r\n92\r\n00:06:14,604 --> 00:06:17,641\r\nY creo que ahÃ­ falta algo.\r\n\r\n93\r\n00:06:17,741 --> 00:06:20,911\r\nNo pasa nada, pero falta algo.\r\n\r\n94\r\n00:06:21,044 --> 00:06:23,847\r\nY la razÃ³n de esto es porque si quieres hacer grandes cosas,\r\n\r\n95\r\n00:06:23,947 --> 00:06:27,684\r\nY sÃ© que esto es cierto acerca de tu creaciÃ³n de Stripe.\r\n\r\n96\r\n00:06:27,784 --> 00:06:32,522\r\nY, por cierto, este es uno de los mejores directores ejecutivos del mundo.\r\n\r\n97\r\n00:06:32,622 --> 00:06:34,457\r\nPor joven que sea, sÃ­.\r\n\r\n98\r\n00:06:37,894 --> 00:06:41,665\r\nYa saben, he conocido a muchos directores ejecutivos y he oÃ­do hablar de muchas empresas.\r\n\r\n99\r\n00:06:41,798 --> 00:06:45,402\r\nY Ã©sta es realmente una de las grandes empresas visionarias del mundo.\r\n\r\n100\r\n00:06:45,502 --> 00:06:48,038\r\nY de todos modos, sÃ³lo quiero decir eso.\r\n\r\n101\r\n00:06:48,204 --> 00:06:50,707\r\nEs la razÃ³n por la que me encanta lo que...\r\n\r\n102\r\n00:06:50,807 --> 00:06:53,209\r\n-Ya no se permiten mÃ¡s cumplidos, nos hacen sentir terriblemente incÃ³modos.\r\n\r\n103\r\n00:06:53,343 --> 00:06:56,546\r\n-Lo sÃ©, lo notÃ©. Lo vi, estaba empezando a sudar.\r\n\r\n104\r\n00:06:57,981 --> 00:07:03,253\r\nY asÃ­ es como funciona, cuando quieres construir algo grandioso,\r\n\r\n105\r\n00:07:03,353 --> 00:07:05,722\r\nNo es fÃ¡cil hacerlo.\r\n\r\n106\r\n00:07:05,822 --> 00:07:07,590\r\nY cuando estÃ¡s haciendo algo que no es fÃ¡cil de hacer,\r\n\r\n107\r\n00:07:07,724 --> 00:07:10,460\r\nNo siempre lo disfrutas.\r\n\r\n108\r\n00:07:10,560 --> 00:07:13,229\r\nNo amo todos los dÃ­as de mi trabajo.\r\n\r\n109\r\n00:07:13,363 --> 00:07:15,432\r\nNo creo que todos los dÃ­as me traigan alegrÃ­a,\r\n\r\n110\r\n00:07:15,532 --> 00:07:19,269\r\nLa alegrÃ­a tampoco tiene por quÃ© ser la definiciÃ³n de un buen dÃ­a.\r\n\r\n111\r\n00:07:19,369 --> 00:07:21,237\r\nY cada dÃ­a no soy feliz.\r\n\r\n112\r\n00:07:21,338 --> 00:07:23,573\r\nCada aÃ±o no estoy contento con la empresa.\r\n\r\n113\r\n00:07:23,773 --> 00:07:26,876\r\nPero amo la compaÃ±Ã­a cada segundo.\r\n\r\n114\r\n00:07:27,677 --> 00:07:31,247\r\nY por eso creo que lo que la gente malinterpreta es...\r\n\r\n115\r\n00:07:31,381 --> 00:07:35,986\r\nDe alguna manera los mejores trabajos son los que te traen felicidad todo el tiempo.\r\n\r\n116\r\n00:07:36,119 --> 00:07:41,324\r\nNo creo que eso sea correcto. Tienes que sufrir.\r\n\r\n117\r\n00:07:41,458 --> 00:07:44,961\r\nTienes que luchar. Tienes que esforzarte.\r\n\r\n118\r\n00:07:45,095 --> 00:07:46,796\r\nTienes que hacer esas cosas difÃ­ciles\r\n\r\n119\r\n00:07:46,930 --> 00:07:51,568\r\ny trabajar en ello para poder apreciar realmente lo que has hecho.\r\n\r\n120\r\n00:07:52,102 --> 00:07:56,006\r\nY no hay cosas grandiosas que sean fÃ¡ciles de hacer.\r\n\r\n121\r\n00:07:56,473 --> 00:08:01,211\r\nY entonces, por definiciÃ³n, yo dirÃ­a por tanto,\r\n\r\n122\r\n00:08:01,311 --> 00:08:05,749\r\nOs deseo grandeza, que, a mi modo de decirlo,\r\n\r\n123\r\n00:08:05,882 --> 00:08:09,419\r\nTe deseo mucho dolor y sufrimiento.\r\n\r\n124\r\n00:08:16,192 --> 00:08:19,629\r\n-Â¿Hay algo en tu crianza que te haya enseÃ±ado esa idea?\r\n\r\n125\r\n00:08:19,763 --> 00:08:22,465\r\nÂ¿O es simplemente algo innato en tu maquillaje?\r\n\r\n126\r\n00:08:29,305 --> 00:08:31,441\r\n-No me di cuenta que tenÃ­a que acostarme para esto.\r\n\r\n127\r\n00:08:36,146 --> 00:08:39,315\r\nVoy a contarte cosas que nunca le he contado a nadie. Ni siquiera a mi familia.\r\n\r\n128\r\n00:08:43,053 --> 00:08:48,324\r\nYo era inmigrante. Y cuando lleguÃ© en 1973, tenÃ­a nueve aÃ±os.\r\n\r\n129\r\n00:08:48,425 --> 00:08:50,760\r\nMi hermano mayor tenÃ­a casi 11 aÃ±os.\r\n\r\n130\r\n00:08:50,894 --> 00:08:57,600\r\nY Ã©ste era un paÃ­s extranjero y no habÃ­a nada fÃ¡cil en ello.\r\n\r\n131\r\n00:08:57,734 --> 00:09:04,674\r\nY tambiÃ©n crecimos con unos padres realmente fabulosos.\r\n\r\n132\r\n00:09:04,774 --> 00:09:09,179\r\nPero no Ã©ramos ricos. AsÃ­ que trabajaron duro, y siguen trabajando duro hoy.\r\n\r\n133\r\n00:09:09,512 --> 00:09:13,917\r\nY asÃ­ transmitieron muchas lecciones de vida trabajando duro.\r\n\r\n134\r\n00:09:14,184 --> 00:09:16,820\r\nTuve todo tipo de trabajos.\r\n\r\n135\r\n00:09:16,920 --> 00:09:23,359\r\ny fuimos a una escuela que incluÃ­a muchas tareas.\r\n\r\n136\r\n00:09:23,460 --> 00:09:24,461\r\n-Â¿EstÃ¡ en Kentucky?\r\n\r\n137\r\n00:09:24,561 --> 00:09:28,098\r\n-SÃ­, Kentucky. Instituto Bautista Oneida.\r\n\r\n138\r\n00:09:28,198 --> 00:09:33,970\r\nNo creo que sea lo mismo que 'MIT', que 'yo' no sea lo mismo.\r\n\r\n139\r\n00:09:34,070 --> 00:09:39,075\r\nEs la misma palabra, pero es diferente. Es un tipo diferente de instituto.\r\n\r\n140\r\n00:09:39,476 --> 00:09:44,047\r\nPero mi instituto requerÃ­a que fueras a la escuela, y era un dormitorio,\r\n\r\n141\r\n00:09:44,180 --> 00:09:45,548\r\nY entonces hubo muchas tareas.\r\n\r\n142\r\n00:09:45,782 --> 00:09:47,383\r\nYo era el niÃ±o mÃ¡s pequeÃ±o de la escuela,\r\n\r\n143\r\n00:09:47,517 --> 00:09:49,886\r\nY asÃ­ todos los demÃ¡s niÃ±os tuvieron que trabajar duro.\r\n\r\n144\r\n00:09:49,986 --> 00:09:53,690\r\nEllos tenÃ­an que trabajar en la plantaciÃ³n de tabaco y a mÃ­ me tocÃ³ el trabajo fÃ¡cil.\r\n\r\n145\r\n00:09:53,823 --> 00:09:57,660\r\nYo tenÃ­a nueve aÃ±os y despuÃ©s de que se fueron, tuve que limpiar todos los baÃ±os.\r\n\r\n146\r\n00:09:58,628 --> 00:10:02,332\r\nNunca sentÃ­ que conseguÃ­ el trabajo fÃ¡cil,\r\n\r\n147\r\n00:10:02,465 --> 00:10:07,070\r\nPorque lo que dejaron atrÃ¡s fue... No puedes dejar de ver ese tipo de cosas.\r\n\r\n148\r\n00:10:09,072 --> 00:10:12,008\r\nPero ese era mi trabajo y lo hice con mucho gusto.\r\n\r\n149\r\n00:10:12,108 --> 00:10:17,046\r\nY luego tuve muchos otros trabajos, y Denny's fue uno de ellos.\r\n\r\n150\r\n00:10:17,147 --> 00:10:21,050\r\nY empecÃ© como lavaplatos, luego me convertÃ­ en ayudante de camarero y luego en camarero.\r\n\r\n151\r\n00:10:21,151 --> 00:10:23,887\r\nY los amÃ© a todos. Los amÃ© a todos.\r\n\r\n152\r\n00:10:23,987 --> 00:10:28,825\r\nDe alguna manera, siempre he encontrado...\r\n\r\n153\r\n00:10:30,160 --> 00:10:31,961\r\nQuisiera decir alegrÃ­a, pero no es del todo correcto.\r\n\r\n154\r\n00:10:32,095 --> 00:10:37,600\r\nSimplemente todo lo que hacÃ­a, querÃ­a hacerlo lo mejor que pudiera.\r\n\r\n155\r\n00:10:37,700 --> 00:10:40,136\r\nY tal vez eso estuvo arraigado desde el principio,\r\n\r\n156\r\n00:10:40,236 --> 00:10:45,475\r\nPero definitivamente fui el mejor limpiador de baÃ±os que el mundo haya visto jamÃ¡s.\r\n\r\n157\r\n00:10:45,575 --> 00:10:47,243\r\nEstoy seguro de ello. SÃ­.\r\n\r\n158\r\n00:10:47,343 --> 00:10:53,516\r\n-Entonces, si avanzamos rÃ¡pidamente un poco hasta la NVIDIA de hoy,\r\n\r\n159\r\n00:10:53,616 --> 00:10:55,285\r\nÂ¿QuÃ© tan grande es su equipo de liderazgo?\r\n\r\n160\r\n00:10:55,618 --> 00:10:58,354\r\n-Â¿QuÃ© tan grande es...? -Tu equipo de liderazgo.\r\n\r\n161\r\n00:10:59,722 --> 00:11:04,260\r\n-El equipo de liderazgo de NVIDIA estÃ¡ formado por mÃ¡s de 60 personas.\r\n\r\n162\r\n00:11:04,394 --> 00:11:05,962\r\n-Â¿Y todos ellos te rinden cuentas a ti?\r\n\r\n163\r\n00:11:06,062 --> 00:11:08,364\r\n-SÃ­, todos me reportan a mÃ­. -Â¿60 informes directos?\r\n\r\n164\r\n00:11:08,464 --> 00:11:09,832\r\n-60 informes directos. SÃ­.\r\n\r\n165\r\n00:11:09,933 --> 00:11:13,436\r\n-Lo cual no se considera convencionalmente una mejor prÃ¡ctica.\r\n\r\n166\r\n00:11:15,505 --> 00:11:17,740\r\nEstoy de acuerdo en que la mejor prÃ¡ctica...\r\n\r\n167\r\n00:11:18,041 --> 00:11:19,609\r\n-Estoy seguro de que es la mejor prÃ¡ctica.\r\n\r\n168\r\n00:11:19,742 --> 00:11:23,546\r\nNo es convencional, pero estoy seguro de que es la mejor prÃ¡ctica.\r\n\r\n169\r\n00:11:27,450 --> 00:11:31,254\r\nY al final de esto, voy a convencerlos a todos.\r\n\r\n170\r\n00:11:31,354 --> 00:11:33,456\r\ntener 60 personas bajo su supervisiÃ³n directa.\r\n\r\n171\r\n00:11:34,290 --> 00:11:35,491\r\n-La palabra es tuya.\r\n\r\n172\r\n00:11:36,626 --> 00:11:38,661\r\n-La razÃ³n...en primer lugar,\r\n\r\n173\r\n00:11:38,761 --> 00:11:43,833\r\nLa razÃ³n es que el nivel de jerarquÃ­a en tu empresa realmente importa.\r\n\r\n174\r\n00:11:43,933 --> 00:11:46,202\r\nLa informaciÃ³n realmente importa.\r\n\r\n175\r\n00:11:46,302 --> 00:11:51,107\r\nCreo que tu aportaciÃ³n a...\r\n\r\n176\r\n00:11:51,241 --> 00:11:56,179\r\nEl trabajo no debe basarse en el acceso privilegiado a la informaciÃ³n.\r\n\r\n177\r\n00:11:57,280 --> 00:12:02,352\r\nNo hago reuniones individuales y no... Mi personal es bastante grande,\r\n\r\n178\r\n00:12:02,952 --> 00:12:06,022\r\ny casi todo lo que digo, lo digo a todo el mundo al mismo tiempo.\r\n\r\n179\r\n00:12:06,589 --> 00:12:08,324\r\nY la razÃ³n de esto es porque\r\n\r\n180\r\n00:12:08,591 --> 00:12:11,661\r\nRealmente no creo que exista ninguna informaciÃ³n con la que opere,\r\n\r\n181\r\n00:12:11,794 --> 00:12:15,131\r\nDe algo que, de alguna manera, sÃ³lo una o dos personas deberÃ­an enterarse.\r\n\r\n182\r\n00:12:15,231 --> 00:12:17,900\r\nY estos son los retos de la empresa\r\n\r\n183\r\n00:12:18,001 --> 00:12:19,068\r\no este es el problema que estoy tratando de resolver,\r\n\r\n184\r\n00:12:19,168 --> 00:12:21,237\r\no esta es la direcciÃ³n en la que estamos tratando de ir.\r\n\r\n185\r\n00:12:21,337 --> 00:12:23,039\r\nÃ‰stos son los nuevos esfuerzos.\r\n\r\n186\r\n00:12:23,139 --> 00:12:24,907\r\nEsto no funciona. Eso funciona bien.\r\n\r\n187\r\n00:12:25,008 --> 00:12:29,946\r\nY todo este tipo de informaciÃ³n deberÃ­a poder ser escuchada por todo el mundo.\r\n\r\n188\r\n00:12:30,046 --> 00:12:33,650\r\nMe encanta que todos trabajen con la misma partitura.\r\n\r\n189\r\n00:12:33,750 --> 00:12:37,253\r\nMe encanta que no haya acceso privilegiado a la informaciÃ³n.\r\n\r\n190\r\n00:12:37,353 --> 00:12:40,890\r\nMe encanta que todos podamos contribuir a resolver un problema.\r\n\r\n191\r\n00:12:41,024 --> 00:12:44,560\r\nY cuando tienes 60 personas en una habitaciÃ³n y muchas veces...\r\n\r\n192\r\n00:12:44,661 --> 00:12:47,830\r\nBueno, mis reuniones de personal son una vez cada dos semanas,\r\n\r\n193\r\n00:12:47,930 --> 00:12:51,701\r\nY todo se basa en problemas, cualesquiera que sean los problemas que tengamos.\r\n\r\n194\r\n00:12:51,801 --> 00:12:54,404\r\nTodos estÃ¡n allÃ­ trabajando en ello al mismo tiempo.\r\n\r\n195\r\n00:12:54,504 --> 00:12:58,041\r\nTodo el mundo escuchÃ³ el razonamiento del problema.\r\n\r\n196\r\n00:12:58,174 --> 00:13:01,911\r\nTodos escucharon el razonamiento de la soluciÃ³n. Todos lo oyeron todo.\r\n\r\n197\r\n00:13:02,011 --> 00:13:04,847\r\nY eso empodera a la gente.\r\n\r\n198\r\n00:13:04,947 --> 00:13:07,317\r\nCreo que cuando se da a todos el mismo acceso a la informaciÃ³n,\r\n\r\n199\r\n00:13:07,417 --> 00:13:11,287\r\nEmpodera a las personas. Y eso es lo primero: empoderar.\r\n\r\n200\r\n00:13:11,387 --> 00:13:19,028\r\nNÃºmero dos, si el personal directo del CEO es de 60 personas,\r\n\r\n201\r\n00:13:19,162 --> 00:13:21,230\r\nla cantidad de capas que has eliminado en una empresa\r\n\r\n202\r\n00:13:21,331 --> 00:13:24,167\r\nProbablemente sea algo asÃ­ como siete, dependiendo de cÃ³mo sea.\r\n\r\n203\r\n00:13:24,267 --> 00:13:26,869\r\n-60 en cada capa, o solo 60?\r\n\r\n204\r\n00:13:26,969 --> 00:13:33,509\r\nEs decir, si soy uno de los 60 afortunados, Â¿tambiÃ©n tengo 60 informes directos?\r\n\r\n205\r\n00:13:33,609 --> 00:13:37,013\r\n-No. Tampoco creo que eso sea escalable hacia abajo.\r\n\r\n206\r\n00:13:37,113 --> 00:13:42,185\r\nY la razÃ³n de esto es porque se necesita cada vez mÃ¡s supervisiÃ³n,\r\n\r\n207\r\n00:13:42,318 --> 00:13:45,421\r\ndependiendo de ciertos niveles.\r\n\r\n208\r\n00:13:45,521 --> 00:13:48,658\r\nY a nivel de personal electrÃ³nico,\r\n\r\n209\r\n00:13:48,758 --> 00:13:52,095\r\nSi tienes la mala suerte de formar parte del equipo electrÃ³nico de NVIDIA,\r\n\r\n210\r\n00:13:52,195 --> 00:13:57,600\r\nEs muy poco probable que necesites mucha gestiÃ³n.\r\n\r\n211\r\n00:13:57,700 --> 00:14:02,839\r\n-Y por eso rara vez me encuentro en la situaciÃ³n de tener que defender la sabidurÃ­a convencional.\r\n\r\n212\r\n00:14:02,939 --> 00:14:05,842\r\nPero si tuviera que forjar el otro lado, dirÃ­a, bueno,\r\n\r\n213\r\n00:14:05,942 --> 00:14:08,411\r\nLas sesiones individuales son donde se brinda entrenamiento,\r\n\r\n214\r\n00:14:08,511 --> 00:14:11,614\r\ndonde tal vez puedan hablar sobre objetivos juntos, objetivos personales,\r\n\r\n215\r\n00:14:11,748 --> 00:14:13,883\r\navance profesional, lo que sea.\r\n\r\n216\r\n00:14:14,183 --> 00:14:17,220\r\nDonde quizÃ¡s das retroalimentaciÃ³n sobre algo que ves que alguien hace\r\n\r\n217\r\n00:14:17,320 --> 00:14:18,888\r\nsistemÃ¡ticamente no lo hacen tan bien y asÃ­ sucesivamente.\r\n\r\n218\r\n00:14:18,988 --> 00:14:20,990\r\nAsÃ­ que hay todas estas cosas que uno, de nuevo,\r\n\r\n219\r\n00:14:21,124 --> 00:14:23,826\r\nconvencionalmente se supone que debe hacerse en el uno contra uno.\r\n\r\n220\r\n00:14:23,960 --> 00:14:26,629\r\nÂ¿No hacÃ©is esas cosas o las hacÃ©is de otra manera?\r\n\r\n221\r\n00:14:26,796 --> 00:14:29,932\r\n-Muy buena pregunta. La hago ahÃ­ mismo.\r\n\r\n222\r\n00:14:31,167 --> 00:14:32,335\r\nLo hago allÃ­ mismo.\r\n\r\n223\r\n00:14:32,802 --> 00:14:35,505\r\nTe doy mi opiniÃ³n allÃ­ mismo, delante de todos.\r\n\r\n224\r\n00:14:36,439 --> 00:14:39,375\r\nY, de hecho, esto es realmente algo muy importante.\r\n\r\n225\r\n00:14:39,709 --> 00:14:43,713\r\nEn primer lugar, la retroalimentaciÃ³n es aprendizaje. La retroalimentaciÃ³n es aprendizaje.\r\n\r\n226\r\n00:14:43,813 --> 00:14:47,283\r\nÂ¿Por quÃ© razÃ³n eres tÃº la Ãºnica persona que deberÃ­a aprender esto?\r\n\r\n227\r\n00:14:47,950 --> 00:14:50,620\r\nAhora has creado las condiciones\r\n\r\n228\r\n00:14:51,387 --> 00:14:54,991\r\npor algÃºn error que cometiste\r\n\r\n229\r\n00:14:55,158 --> 00:15:01,230\r\no tonterÃ­as que tÃº mismo has provocado.\r\n\r\n230\r\n00:15:04,734 --> 00:15:07,570\r\nTodos deberÃ­amos aprender de esa oportunidad.\r\n\r\n231\r\n00:15:07,703 --> 00:15:10,473\r\nAsÃ­ que ustedes crearon las condiciones, pero todos deberÃ­amos aprender de ello.\r\n\r\n232\r\n00:15:10,606 --> 00:15:12,375\r\nÂ¿Tiene eso sentido?\r\n\r\n233\r\n00:15:12,475 --> 00:15:15,211\r\nY entonces, para explicarte por quÃ© eso no tiene sentido.\r\n\r\n234\r\n00:15:15,311 --> 00:15:18,581\r\no en quÃ© me diferencio de Ã©l: la mitad del tiempo no tengo razÃ³n.\r\n\r\n235\r\n00:15:18,714 --> 00:15:21,818\r\nPero para mÃ­, razonarlo delante de todos.\r\n\r\n236\r\n00:15:21,918 --> 00:15:24,353\r\nAyuda a todos a aprender a razonar.\r\n\r\n237\r\n00:15:24,454 --> 00:15:29,592\r\nY entonces el problema... el problema que tengo con las reuniones individuales y dejar de lado la retroalimentaciÃ³n...\r\n\r\n238\r\n00:15:29,692 --> 00:15:32,929\r\nes privar a un montÃ³n de personas de ese mismo aprendizaje.\r\n\r\n239\r\n00:15:33,029 --> 00:15:36,499\r\nAprendiendo de los errores, de los errores de los demÃ¡s,\r\n\r\n240\r\n00:15:36,599 --> 00:15:37,767\r\nEs la mejor manera de aprender.\r\n\r\n241\r\n00:15:37,867 --> 00:15:40,937\r\nÂ¿Por quÃ© aprender de tus propios errores?\r\n\r\n242\r\n00:15:41,037 --> 00:15:43,139\r\nÂ¿Por quÃ© aprender de tu propia vergÃ¼enza?\r\n\r\n243\r\n00:15:43,239 --> 00:15:44,807\r\nTienes que aprender de la vergÃ¼enza ajena.\r\n\r\n244\r\n00:15:44,907 --> 00:15:48,244\r\nPor eso tenemos estudios de caso, Â¿no?\r\n\r\n245\r\n00:15:48,344 --> 00:15:52,215\r\nEstamos tratando de leer los desastres de otras personas, las tragedias de otras personas.\r\n\r\n246\r\n00:15:52,315 --> 00:15:54,383\r\nNada nos hace mÃ¡s felices que eso.\r\n\r\n247\r\n00:15:56,352 --> 00:16:00,723\r\n-Â¿Ha logrado que otros lÃ­deres de NVIDIA adopten esta prÃ¡ctica?\r\n\r\n248\r\n00:16:00,857 --> 00:16:02,258\r\nÂ¿O eso es difÃ­cil?\r\n\r\n249\r\n00:16:02,391 --> 00:16:05,061\r\n-Doy a las personas la oportunidad de decidir por sÃ­ mismas,\r\n\r\n250\r\n00:16:05,161 --> 00:16:07,396\r\nPero realmente desaconsejo los encuentros uno a uno.\r\n\r\n251\r\n00:16:07,497 --> 00:16:09,031\r\nRealmente desaconsejo los encuentros uno a uno.\r\n\r\n252\r\n00:16:09,131 --> 00:16:13,302\r\nNo hay nada peor que la idea de que alguien diga:\r\n\r\n253\r\n00:16:13,402 --> 00:16:16,672\r\n\"Oh, Jensen quiere que hagamos esto\".\r\n\r\n254\r\n00:16:16,772 --> 00:16:20,109\r\nÂ¿Por quÃ© hay que decirle eso a alguien? Todo el mundo deberÃ­a saberlo.\r\n\r\n255\r\n00:16:20,209 --> 00:16:25,281\r\nO alguien dijo: \"Ese personal electrÃ³nico dijo eso\".\r\n\r\n256\r\n00:16:25,381 --> 00:16:27,483\r\nNada me vuelve mÃ¡s loco que eso.\r\n\r\n257\r\n00:16:28,718 --> 00:16:34,023\r\n-Una vez me dijiste que realmente no te gustaba despedir a la gente y que muy rara vez lo hacÃ­as.\r\n\r\n258\r\n00:16:34,123 --> 00:16:35,791\r\nÂ¿Puedes explicarme eso con mÃ¡s detalle?\r\n\r\n259\r\n00:16:36,959 --> 00:16:40,930\r\n-Bueno, prefiero mejorarte que renunciar a ti.\r\n\r\n260\r\n00:16:41,030 --> 00:16:44,700\r\nCuando despides a alguien, en cierto modo estÃ¡s diciendo...\r\n\r\n261\r\n00:16:45,334 --> 00:16:47,436\r\nBueno, mucha gente dice:\r\n\r\n262\r\n00:16:47,570 --> 00:16:51,974\r\n\"No fue tu culpa\" o \"TomÃ© la decisiÃ³n equivocada\" o--\r\n\r\n263\r\n00:16:52,108 --> 00:16:55,111\r\nHay muy pocos trabajos... Mira, yo solÃ­a limpiar baÃ±os,\r\n\r\n264\r\n00:16:55,244 --> 00:16:59,148\r\nY ahora que soy CEO de una empresa, creo que podrÃ­as aprenderlo.\r\n\r\n265\r\n00:16:59,248 --> 00:17:01,417\r\nEstoy bastante seguro de que puedes aprender esto.\r\n\r\n266\r\n00:17:01,517 --> 00:17:04,287\r\nY hay muchas cosas en la vida que creo que puedes aprender.\r\n\r\n267\r\n00:17:04,387 --> 00:17:06,455\r\nY sÃ³lo hay que darle la oportunidad de aprenderlo.\r\n\r\n268\r\n00:17:06,556 --> 00:17:09,425\r\nTuve el beneficio de ver a mucha gente inteligente hacer muchas cosas.\r\n\r\n269\r\n00:17:09,559 --> 00:17:12,328\r\nEstoy rodeado de 60 personas. Siempre hacen cosas inteligentes.\r\n\r\n270\r\n00:17:12,428 --> 00:17:14,397\r\nY probablemente no se dan cuenta,\r\n\r\n271\r\n00:17:14,497 --> 00:17:17,066\r\npero estoy aprendiendo constantemente de cada uno de ellos.\r\n\r\n272\r\n00:17:17,166 --> 00:17:21,904\r\nY por eso no me gusta renunciar a las personas porque creo que podrÃ­an mejorar.\r\n\r\n273\r\n00:17:22,004 --> 00:17:24,774\r\nY entonces hay un... es como una broma...\r\n\r\n274\r\n00:17:24,874 --> 00:17:29,545\r\nPero la gente sabe que prefiero torturarlos para que alcancen la grandeza.\r\n\r\n275\r\n00:17:29,712 --> 00:17:32,281\r\n-Esa era la frase que esperaba descubrir.\r\n\r\n276\r\n00:17:32,381 --> 00:17:33,783\r\nSÃ­, recuerdo que mencionaste eso.\r\n\r\n277\r\n00:17:33,916 --> 00:17:37,286\r\n-SÃ­. AsÃ­ que prefiero torturarte para que alcances la grandeza porque creo en ti.\r\n\r\n278\r\n00:17:37,954 --> 00:17:43,326\r\nY creo que los entrenadores que realmente creen en su equipo...\r\n\r\n279\r\n00:17:43,426 --> 00:17:45,127\r\ntorturarlos hasta la grandeza.\r\n\r\n280\r\n00:17:45,261 --> 00:17:49,198\r\nY muchas veces estÃ¡n tan cerca. No te rindas, estÃ¡n tan cerca.\r\n\r\n281\r\n00:17:49,298 --> 00:17:52,868\r\nLa grandeza llega de repente, un dÃ­a dices: \"Lo conseguÃ­\".\r\n\r\n282\r\n00:17:53,002 --> 00:17:54,203\r\nÂ¿Sabes lo que estoy diciendo?\r\n\r\n283\r\n00:17:54,303 --> 00:17:55,805\r\nEsa sensaciÃ³n de que no lo entendiste ayer\r\n\r\n284\r\n00:17:55,905 --> 00:17:59,208\r\nY de repente, un dÃ­a, algo hizo clic. \"Ah, lo entiendo\".\r\n\r\n285\r\n00:17:59,308 --> 00:18:03,212\r\nÂ¿Te imaginas renunciar justo en ese momento antes de conseguirlo?\r\n\r\n286\r\n00:18:03,512 --> 00:18:04,981\r\nNo quiero que renuncies a eso,\r\n\r\n287\r\n00:18:05,081 --> 00:18:07,116\r\nAsÃ­ que seguirÃ© torturÃ¡ndote.\r\n\r\n288\r\n00:18:09,118 --> 00:18:13,990\r\n-Â¿CÃ³mo es tu equilibrio entre vida laboral y personal?\r\n\r\n289\r\n00:18:21,764 --> 00:18:23,733\r\n-Bueno, depende de a quiÃ©n le preguntes.\r\n\r\n290\r\n00:18:24,934 --> 00:18:27,436\r\nCreo que mi equilibrio entre trabajo y vida personal es realmente genial.\r\n\r\n291\r\n00:18:27,870 --> 00:18:34,644\r\nEs realmente genial. Trabajo tanto como puedo.\r\n\r\n292\r\n00:18:38,147 --> 00:18:39,815\r\nSiento que me estÃ¡ juzgando.\r\n\r\n293\r\n00:18:42,551 --> 00:18:46,022\r\nSoy mayor que tÃº. Tengo mÃ¡s sabidurÃ­a que tÃº.\r\n\r\n294\r\n00:18:46,122 --> 00:18:47,390\r\nEntonces lo que yo...\r\n\r\n295\r\n00:18:48,658 --> 00:18:50,693\r\n-Estos son todos los aspectos mÃ¡s destacados de nuestras conversaciones.\r\n\r\n296\r\n00:18:50,793 --> 00:18:53,262\r\nque creo que mÃ¡s gente deberÃ­a poder escuchar, asÃ­ que...\r\n\r\n297\r\n00:18:54,664 --> 00:18:59,402\r\n-Bueno, trabajo desde el momento en que me despierto hasta el momento en que me acuesto,\r\n\r\n298\r\n00:18:59,502 --> 00:19:01,871\r\ny trabajo siete dÃ­as a la semana.\r\n\r\n299\r\n00:19:02,004 --> 00:19:04,306\r\nCuando no estoy trabajando, pienso en trabajar.\r\n\r\n300\r\n00:19:05,107 --> 00:19:09,011\r\nY cuando estoy trabajando, estoy trabajando, y entonces...\r\n\r\n301\r\n00:19:10,179 --> 00:19:13,783\r\nY me siento a ver pelÃ­culas, pero no las recuerdo.\r\n\r\n302\r\n00:19:13,883 --> 00:19:15,418\r\nPorque estoy pensando en el trabajo.\r\n\r\n303\r\n00:19:17,386 --> 00:19:21,724\r\nY eso es...\r\n\r\n304\r\n00:19:22,091 --> 00:19:28,297\r\nPero mi trabajo no es, como sabÃ©is, no es trabajar en el sentido de,\r\n\r\n305\r\n00:19:28,431 --> 00:19:31,367\r\nExiste este problema y estÃ¡s intentando resolverlo.\r\n\r\n306\r\n00:19:31,467 --> 00:19:34,203\r\nEstÃ¡s pensando en lo que puede ser la empresa,\r\n\r\n307\r\n00:19:34,336 --> 00:19:37,173\r\nÂ¿Y hay cosas que podrÃ­amos hacer aÃºn mejor?\r\n\r\n308\r\n00:19:37,306 --> 00:19:40,342\r\nO a veces simplemente se trata de intentar resolver un problema, Â¿sabes?\r\n\r\n309\r\n00:19:40,476 --> 00:19:42,611\r\nPero a veces estÃ¡s imaginando el futuro.\r\n\r\n310\r\n00:19:42,712 --> 00:19:44,880\r\nY, muchacho, si hiciÃ©ramos esto y aquello.\r\n\r\n311\r\n00:19:44,980 --> 00:19:49,552\r\nEstÃ¡ funcionando, estÃ¡s fantaseando, estÃ¡s soÃ±ando, Â¿verdad?\r\n\r\n312\r\n00:19:49,652 --> 00:19:51,053\r\nQuiero decir, eso es increÃ­ble.\r\n\r\n313\r\n00:19:51,187 --> 00:19:53,289\r\n-Bueno, pues sÃ­, para concretar esto un poquito\r\n\r\n314\r\n00:19:53,389 --> 00:19:57,993\r\nY vamos a empezar a hablar de IA, que segÃºn tengo entendido es un tema de actualidad estos dÃ­as.\r\n\r\n315\r\n00:19:58,094 --> 00:20:01,363\r\n-Es una cosa. -SÃ­, oficialmente una cosa.\r\n\r\n316\r\n00:20:01,464 --> 00:20:04,233\r\nPero para concretar esto un poco,\r\n\r\n317\r\n00:20:04,366 --> 00:20:07,503\r\nÂ¿CÃ³mo es un dÃ­a en la vida de Jensen?\r\n\r\n318\r\n00:20:08,237 --> 00:20:10,306\r\n-Bueno, yo solÃ­a despertarme a las cinco.\r\n\r\n319\r\n00:20:10,406 --> 00:20:13,375\r\nEstos dÃ­as, me despierto a las seis por mis perros.\r\n\r\n320\r\n00:20:13,476 --> 00:20:16,879\r\nY la razÃ³n por la que seis, es que de alguna manera lo decidimos.\r\n\r\n321\r\n00:20:16,979 --> 00:20:19,248\r\nQue a las seis es cuando deben despertarse.\r\n\r\n322\r\n00:20:19,348 --> 00:20:24,754\r\nY no sÃ© quÃ© es. No me importa despertar a nadie.\r\n\r\n323\r\n00:20:24,854 --> 00:20:27,022\r\npero me siento culpable cuando despierto a los cachorros.\r\n\r\n324\r\n00:20:29,091 --> 00:20:33,696\r\nY realmente me agobia. AsÃ­ que no quiero mudarme.\r\n\r\n325\r\n00:20:33,829 --> 00:20:38,367\r\nDetectan cualquier vibraciÃ³n en la casa.\r\n\r\n326\r\n00:20:38,701 --> 00:20:42,872\r\ny los despierta y entonces nos quedamos en la cama.\r\n\r\n327\r\n00:20:43,005 --> 00:20:46,976\r\nY yo solo leo en la cama hasta las seis y es hora...\r\n\r\n328\r\n00:20:47,109 --> 00:20:48,978\r\n-Â¿Pero estÃ¡s pensando en las GPU?\r\n\r\n329\r\n00:20:49,145 --> 00:20:54,250\r\n-Ah, sÃ­. SÃ­, sÃ­, claro. Estoy obsesionado con las GPU. O sea, Â¿quÃ© se le va a hacer?\r\n\r\n330\r\n00:20:54,383 --> 00:20:57,319\r\nEstoy constantemente... No. Solo estoy...\r\n\r\n331\r\n00:20:57,586 --> 00:21:00,322\r\n-Y luego el dÃ­a es todo, supongo, reuniones de grupo.\r\n\r\n332\r\n00:21:00,422 --> 00:21:02,224\r\nPorque no pueden ser reuniones uno a uno.\r\n\r\n333\r\n00:21:02,324 --> 00:21:06,695\r\n-SÃ­. Hago mi trabajo antes de ir a trabajar y luego, cuando llego al trabajo...\r\n\r\n334\r\n00:21:08,731 --> 00:21:11,066\r\n-Â¿Y cuÃ¡ntas reuniones hay en un dÃ­a tÃ­pico?\r\n\r\n335\r\n00:21:12,434 --> 00:21:14,403\r\n-PrÃ¡cticamente todo el dÃ­a.\r\n\r\n336\r\n00:21:14,537 --> 00:21:17,306\r\nY asÃ­ selecciono las reuniones que son realmente importantes para mÃ­.\r\n\r\n337\r\n00:21:17,439 --> 00:21:21,710\r\nIntento no tener reuniones periÃ³dicas,\r\n\r\n338\r\n00:21:21,811 --> 00:21:24,079\r\nreuniones operativas regulares.\r\n\r\n339\r\n00:21:24,180 --> 00:21:26,482\r\nPorque tengo gente increÃ­ble en la empresa que es...\r\n\r\n340\r\n00:21:26,582 --> 00:21:28,184\r\nrealizando reuniones operativas periÃ³dicas.\r\n\r\n341\r\n00:21:28,284 --> 00:21:32,221\r\nY entonces somos bateadores suplentes, los directores ejecutivos son bateadores suplentes.\r\n\r\n342\r\n00:21:32,354 --> 00:21:35,591\r\nDeberÃ­amos estar trabajando en las cosas que nadie mÃ¡s puede o nadie mÃ¡s estÃ¡ haciendo.\r\n\r\n343\r\n00:21:35,691 --> 00:21:38,794\r\nEntonces estÃ¡s entrando en proyectos que estÃ¡n estancados o fuera de curso.\r\n\r\n344\r\n00:21:38,894 --> 00:21:40,362\r\n-AsÃ­ es. -O nuevas ideas.\r\n\r\n345\r\n00:21:40,462 --> 00:21:44,266\r\nDonde podamos lograr un cambio. Sin reuniones informativas.\r\n\r\n346\r\n00:21:44,366 --> 00:21:46,735\r\nOdio informar sobre las reuniones. No tienen por quÃ© rendirme cuentas.\r\n\r\n347\r\n00:21:46,869 --> 00:21:48,103\r\nSolo tengo reuniones problemÃ¡ticas.\r\n\r\n348\r\n00:21:48,204 --> 00:21:51,140\r\nY asÃ­, las reuniones de problemas, o las reuniones de ideas, o las reuniones de lluvia de ideas,\r\n\r\n349\r\n00:21:51,240 --> 00:21:53,843\r\no reuniones de creaciÃ³n, o lo que sea,\r\n\r\n350\r\n00:21:53,943 --> 00:21:55,311\r\nEsas son las reuniones a las que asisto.\r\n\r\n351\r\n00:21:55,444 --> 00:21:58,080\r\nY asÃ­ suelo llamarlos,\r\n\r\n352\r\n00:21:58,214 --> 00:22:01,517\r\nIntento con todas mis fuerzas evitar que Outlook gestione mi vida.\r\n\r\n353\r\n00:22:01,617 --> 00:22:03,752\r\nY asÃ­ decidimos deliberadamente\r\n\r\n354\r\n00:22:03,853 --> 00:22:06,255\r\nQuÃ© tipo de cosas queremos hacer, en quÃ© queremos trabajar.\r\n\r\n355\r\n00:22:06,355 --> 00:22:08,190\r\nY asÃ­ trato de vivir una vida con propÃ³sito,\r\n\r\n356\r\n00:22:08,290 --> 00:22:12,661\r\nY administro mi tiempo en consecuencia. SÃ­.\r\n\r\n357\r\n00:22:13,262 --> 00:22:17,867\r\n-Usted usÃ³ una frase, una vez, mercados de 0 mil millones de dÃ³lares,\r\n\r\n358\r\n00:22:17,967 --> 00:22:20,536\r\nQue los mercados de $0 mil millones sean sus mercados favoritos.\r\n\r\n359\r\n00:22:20,636 --> 00:22:22,071\r\n-SÃ­. -Â¿QuÃ© quieres decir?\r\n\r\n360\r\n00:22:22,938 --> 00:22:27,209\r\n-Si das un paso atrÃ¡s, nuestro propÃ³sito,\r\n\r\n361\r\n00:22:27,343 --> 00:22:30,479\r\nCasi todos nuestros propÃ³sitos deberÃ­an ser\r\n\r\n362\r\n00:22:30,579 --> 00:22:36,252\r\nir y hacer algo que nunca se ha hecho antes.\r\n\r\n363\r\n00:22:37,019 --> 00:22:39,388\r\nEso es increÃ­blemente difÃ­cil de hacer.\r\n\r\n364\r\n00:22:39,488 --> 00:22:43,626\r\nQue si lo logras podrÃ­as hacer un verdadero aporte.\r\n\r\n365\r\n00:22:43,726 --> 00:22:46,762\r\nSÃ© que su empresa lo hace. Intento hacerlo.\r\n\r\n366\r\n00:22:47,429 --> 00:22:51,967\r\nY si ese es el caso, no se ha hecho antes, es increÃ­blemente difÃ­cil de hacer,\r\n\r\n367\r\n00:22:52,301 --> 00:22:54,737\r\nProbablemente sea asÃ­, y nunca se ha hecho antes.\r\n\r\n368\r\n00:22:55,137 --> 00:22:58,474\r\nEse mercado probablemente tiene un tamaÃ±o de 0 mil millones de dÃ³lares.\r\n\r\n369\r\n00:22:59,074 --> 00:23:01,010\r\nPorque nunca se habÃ­a hecho antes.\r\n\r\n370\r\n00:23:01,143 --> 00:23:06,181\r\nPrefiero ser un creador de mercado, un creador de mercado, que un tomador de mercado.\r\n\r\n371\r\n00:23:07,983 --> 00:23:13,055\r\nCrear algo nuevo que nunca existiÃ³ antes versus pensar en compartir\r\n\r\n372\r\n00:23:13,155 --> 00:23:16,558\r\nNo me gusta pensar en compartir. No me gusta el concepto de compartir.\r\n\r\n373\r\n00:23:17,293 --> 00:23:23,599\r\nY la razÃ³n de esto es que si lo piensas en el panorama general,\r\n\r\n374\r\n00:23:23,699 --> 00:23:27,303\r\nStripe surgiÃ³ de la nada y tÃº te vaporizaste.\r\n\r\n375\r\n00:23:27,436 --> 00:23:29,371\r\nCreaste algo a partir del vapor.\r\n\r\n376\r\n00:23:29,471 --> 00:23:34,710\r\nNo era como si hubiera algo mÃ¡s.\r\n\r\n377\r\n00:23:34,810 --> 00:23:41,784\r\nY me gustarÃ­a pensar que podemos llegar a algo que no supere los 0 mil millones de dÃ³lares.\r\n\r\n378\r\n00:23:41,884 --> 00:23:47,389\r\nUn mercado de 0 mil millones de dÃ³lares es una buena manera de hacer que la empresa piense en\r\n\r\n379\r\n00:23:47,523 --> 00:23:49,892\r\nCÃ³mo crear algo por primera vez.\r\n\r\n380\r\n00:23:50,159 --> 00:23:52,728\r\n-Entonces nuestra misiÃ³n es hacer crecer el PIB de Internet,\r\n\r\n381\r\n00:23:52,861 --> 00:23:57,066\r\ny el PIB de Internet--\r\n\r\n382\r\n00:23:57,199 --> 00:23:59,668\r\nLa clÃ¡usula que suele recibir la mayor atenciÃ³n.\r\n\r\n383\r\n00:23:59,768 --> 00:24:03,706\r\nPero creo que la parte mÃ¡s importante es precisamente el verbo \"crecer\".\r\n\r\n384\r\n00:24:04,173 --> 00:24:08,711\r\nPorque, como dices, no deberÃ­amos estar pensando en, bueno,\r\n\r\n385\r\n00:24:08,911 --> 00:24:11,146\r\nÂ¿CuÃ¡les son las transacciones que ya estÃ¡n sucediendo?\r\n\r\n386\r\n00:24:11,246 --> 00:24:13,248\r\no cuales son los negocios que ya existen,\r\n\r\n387\r\n00:24:13,349 --> 00:24:15,651\r\nDeberÃ­amos estar pensando en cuÃ¡les son las transacciones que no existen.\r\n\r\n388\r\n00:24:15,751 --> 00:24:17,786\r\ny cuales son los negocios que no existen.\r\n\r\n389\r\n00:24:17,886 --> 00:24:21,490\r\nEl PIB del mundo ronda los 100 billones de dÃ³lares.\r\n\r\n390\r\n00:24:21,623 --> 00:24:22,891\r\nPero no tiene por quÃ© ser 100 billones de dÃ³lares.\r\n\r\n391\r\n00:24:23,025 --> 00:24:26,161\r\nPodrÃ­an ser 200 billones o 1.000 billones de dÃ³lares.\r\n\r\n392\r\n00:24:26,295 --> 00:24:28,263\r\n-Eso es exactamente correcto.\r\n\r\n393\r\n00:24:28,364 --> 00:24:33,235\r\nY la mayor parte del valor que vamos a crear en las prÃ³ximas dÃ©cadas\r\n\r\n394\r\n00:24:33,335 --> 00:24:39,108\r\nProbablemente no estÃ©n limitados por cosas fÃ­sicas.\r\n\r\n395\r\n00:24:39,241 --> 00:24:41,610\r\nAsÃ­ que este es un momento bastante extraordinario.\r\n\r\n396\r\n00:24:41,710 --> 00:24:45,047\r\n-Y asÃ­, con este concepto de mercados de 0 mil millones de dÃ³lares,\r\n\r\n397\r\n00:24:45,180 --> 00:24:49,385\r\nSi estoy en NVIDIA, Â¿vengo a ti con alguna propuesta para algÃºn proyecto?\r\n\r\n398\r\n00:24:49,518 --> 00:24:53,422\r\ny tal vez haya varios miles de millones de dÃ³lares de gastos de capital involucrados o\r\n\r\n399\r\n00:24:53,522 --> 00:24:56,458\r\nEs una bÃºsqueda que dura muchos aÃ±os o algo asÃ­.\r\n\r\n400\r\n00:24:56,558 --> 00:25:00,195\r\nY hoy no hay clientes para ello,\r\n\r\n401\r\n00:25:00,329 --> 00:25:02,431\r\nNo hay ninguna demanda que yo pueda demostrar por ello.\r\n\r\n402\r\n00:25:02,531 --> 00:25:06,935\r\nY ustedes simplemente estÃ¡n tomando una decisiÃ³n instintiva al decir eso,\r\n\r\n403\r\n00:25:07,036 --> 00:25:10,973\r\n\"SÃ­, nadie estÃ¡ haciendo esto hoy. Creemos que podrÃ­an, creemos que deberÃ­an.\r\n\r\n404\r\n00:25:11,073 --> 00:25:12,975\r\nY por eso vamos a perseguirlo\".\r\n\r\n405\r\n00:25:13,075 --> 00:25:14,743\r\n-Muy cerca. SÃ­. Es algo asÃ­.\r\n\r\n406\r\n00:25:14,843 --> 00:25:18,781\r\nY es una decisiÃ³n instintiva en el sentido de que\r\n\r\n407\r\n00:25:18,881 --> 00:25:22,384\r\nTu intuiciÃ³n te dice algo como tesis de partida,\r\n\r\n408\r\n00:25:22,484 --> 00:25:24,153\r\nPero luego tienes que razonarlo.\r\n\r\n409\r\n00:25:24,753 --> 00:25:28,757\r\nY el razonamiento detrÃ¡s de esto es mucho, mucho mÃ¡s importante para mÃ­ que una hoja de cÃ¡lculo.\r\n\r\n410\r\n00:25:29,391 --> 00:25:32,795\r\nOdio las hojas de cÃ¡lculo porque puedes hacer que hagan lo que quieras.\r\n\r\n411\r\n00:25:32,928 --> 00:25:35,330\r\nPuedes crear cualquier grÃ¡fico que quieras a partir de una hoja de cÃ¡lculo.\r\n\r\n412\r\n00:25:35,431 --> 00:25:37,032\r\nSÃ³lo tienes que escribir algunos nÃºmeros.\r\n\r\n413\r\n00:25:37,199 --> 00:25:39,535\r\nY por ese motivo no me gustan las hojas de cÃ¡lculo.\r\n\r\n414\r\n00:25:39,635 --> 00:25:42,671\r\nAmo las palabras por eso, las palabras son razonamiento.\r\n\r\n415\r\n00:25:42,805 --> 00:25:45,574\r\nDime, Â¿cÃ³mo lo resolviste? Â¿CuÃ¡l es tu intuiciÃ³n?\r\n\r\n416\r\n00:25:45,707 --> 00:25:48,844\r\nÂ¿Por quÃ© creemos que eso importa? Â¿Por quÃ© pensamos que es difÃ­cil?\r\n\r\n417\r\n00:25:49,711 --> 00:25:52,781\r\nMe gustan las cosas difÃ­ciles porque requieren mucho tiempo para hacerse.\r\n\r\n418\r\n00:25:53,082 --> 00:25:55,017\r\ny si tarda mucho tiempo en hacerlo...\r\n\r\n419\r\n00:25:55,884 --> 00:26:00,189\r\nMucha gente menos comprometida probablemente no lo harÃ¡.\r\n\r\n420\r\n00:26:00,756 --> 00:26:04,226\r\nSi es realmente muy difÃ­cil de hacer, lleva mucho tiempo hacerlo,\r\n\r\n421\r\n00:26:04,326 --> 00:26:07,563\r\nSe necesita una persona muy resiliente y muy dedicada.\r\n\r\n422\r\n00:26:07,663 --> 00:26:09,965\r\nPersona realmente comprometida a ir tras ello.\r\n\r\n423\r\n00:26:10,099 --> 00:26:13,102\r\nY si ademÃ¡s tarda mucho tiempo en hacerlo,\r\n\r\n424\r\n00:26:13,202 --> 00:26:17,339\r\nPuedes andar dando tumbos durante un par de aÃ±os y nadie se da cuenta.\r\n\r\n425\r\n00:26:17,439 --> 00:26:21,577\r\nY asÃ­ podrÃ­a ser incompetente durante varios aÃ±os.\r\n\r\n426\r\n00:26:21,710 --> 00:26:24,746\r\ny todo el mundo dice: \"Bueno, Â¿quiÃ©n lo vio?\"\r\n\r\n427\r\n00:26:25,047 --> 00:26:27,816\r\n-Â¿Y de dÃ³nde saliÃ³ Cuda?\r\n\r\n428\r\n00:26:29,051 --> 00:26:35,524\r\nCuda surgiÃ³ de dos ideas. Una se llama...\r\n\r\n429\r\n00:26:36,758 --> 00:26:38,227\r\nOdio ser tÃ©cnico,\r\n\r\n430\r\n00:26:38,360 --> 00:26:42,431\r\nPero nosotros creamos, fuimos pioneros en esta idea llamada computaciÃ³n acelerada.\r\n\r\n431\r\n00:26:42,531 --> 00:26:48,203\r\nLa computaciÃ³n acelerada es como un dispositivo IO,\r\n\r\n432\r\n00:26:48,303 --> 00:26:50,272\r\nalgo en lo que te sientas PCI Express,\r\n\r\n433\r\n00:26:50,372 --> 00:26:51,940\r\nSi alguien estÃ¡ en el negocio de las computadoras,\r\n\r\n434\r\n00:26:52,040 --> 00:26:58,847\r\nun dispositivo IO que permite que la aplicaciÃ³n interactÃºe con ese dispositivo IO\r\n\r\n435\r\n00:26:58,947 --> 00:27:02,518\r\nde tal manera que se aceleren partes de la aplicaciÃ³n.\r\n\r\n436\r\n00:27:03,085 --> 00:27:08,457\r\nY la UDA fue una invenciÃ³n en 1993, y es realmente una invenciÃ³n profunda,\r\n\r\n437\r\n00:27:08,590 --> 00:27:16,031\r\npermite al programador de software programar directamente un dispositivo IO,\r\n\r\n438\r\n00:27:16,165 --> 00:27:18,700\r\nescribir una aplicaciÃ³n directamente en el dispositivo IO,\r\n\r\n439\r\n00:27:18,834 --> 00:27:22,571\r\nporque el dispositivo IO estÃ¡ virtualizado y...\r\n\r\n440\r\n00:27:22,704 --> 00:27:27,009\r\nEs arquitectÃ³nicamente compatible a lo largo de mÃºltiples generaciones.\r\n\r\n441\r\n00:27:27,109 --> 00:27:31,280\r\nDe todos modos, inventamos esta idea llamada computaciÃ³n acelerada,\r\n\r\n442\r\n00:27:31,413 --> 00:27:35,884\r\nY eso fue... lo llamamos arquitectura de controlador unificada por alguna razÃ³n.\r\n\r\n443\r\n00:27:36,485 --> 00:27:39,221\r\nY luego, varios aÃ±os despuÃ©s,\r\n\r\n444\r\n00:27:39,354 --> 00:27:42,658\r\nPensamos que podrÃ­amos hacer que nuestras GPU sean mÃ¡s programables\r\n\r\n445\r\n00:27:42,758 --> 00:27:44,159\r\na lenguajes de programaciÃ³n de alto nivel.\r\n\r\n446\r\n00:27:44,259 --> 00:27:47,429\r\nY nosotros inventamos esta idea llamada CG,\r\n\r\n447\r\n00:27:47,529 --> 00:27:51,567\r\nC para grÃ¡ficos, vale. C para procesadores grÃ¡ficos.\r\n\r\n448\r\n00:27:51,733 --> 00:27:57,372\r\nY eso abriÃ³ algunas oportunidades realmente emocionantes.\r\n\r\n449\r\n00:27:57,506 --> 00:27:59,741\r\nY pensamos, ya sabÃ©is que esto va a funcionar.\r\n\r\n450\r\n00:27:59,875 --> 00:28:03,178\r\nPero CG, el modelo de programaciÃ³n, no era del todo correcto.\r\n\r\n451\r\n00:28:03,278 --> 00:28:08,817\r\nY asÃ­ inventamos Cuda, que es computaciÃ³n, ya sabes...\r\n\r\n452\r\n00:28:08,917 --> 00:28:12,087\r\nBueno, bueno, asÃ­ es. Es una historia horrible, francamente.\r\n\r\n453\r\n00:28:12,554 --> 00:28:15,224\r\nDe todos modos, inventamos esta idea llamada computaciÃ³n acelerada.\r\n\r\n454\r\n00:28:15,357 --> 00:28:16,692\r\nFuimos pioneros en este enfoque.\r\n\r\n455\r\n00:28:16,825 --> 00:28:20,429\r\nSupongo que la verdadera pregunta es: Â¿fue un Ã©xito rotundo de la noche a la maÃ±ana?\r\n\r\n456\r\n00:28:21,196 --> 00:28:27,069\r\n-No, fue un desastre increÃ­ble de la noche a la maÃ±ana.\r\n\r\n457\r\n00:28:27,169 --> 00:28:29,571\r\nY fue mÃ¡s o menos asÃ­.\r\n\r\n458\r\n00:28:29,671 --> 00:28:32,674\r\n-Este es uno de los mercados de $0 mil millones que perseguiste.\r\n\r\n459\r\n00:28:32,808 --> 00:28:35,010\r\n-SÃ­. -Y fue un desastre.\r\n\r\n460\r\n00:28:35,110 --> 00:28:38,080\r\n-SÃ­. Porque perseguimos un objetivo de 0 mil millones de dÃ³lares.\r\n\r\n461\r\n00:28:38,213 --> 00:28:41,750\r\nPero costÃ³ mucho ir tras ese mercado de 0 mil millones de dÃ³lares,\r\n\r\n462\r\n00:28:41,850 --> 00:28:45,921\r\nEn realidad, aplastÃ³ el mercado de mil millones de dÃ³lares que estÃ¡bamos disfrutando.\r\n\r\n463\r\n00:28:46,021 --> 00:28:50,592\r\nY la razÃ³n de esto es porque\r\n\r\n464\r\n00:28:50,726 --> 00:28:57,299\r\nCuda agregÃ³ un montÃ³n de costos a nuestros chips, pero no hubo aplicaciones.\r\n\r\n465\r\n00:28:57,399 --> 00:28:58,800\r\nY si no hay aplicaciones,\r\n\r\n466\r\n00:28:58,934 --> 00:29:02,971\r\nLos clientes no valoran el producto y no pagarÃ¡n una prima por Ã©l.\r\n\r\n467\r\n00:29:03,238 --> 00:29:06,141\r\nY si la gente no estÃ¡ dispuesta a pagarte por ello, pero tu costo aumentÃ³,\r\n\r\n468\r\n00:29:06,241 --> 00:29:09,211\r\nEntonces sus mÃ¡rgenes brutos se reducen drÃ¡sticamente.\r\n\r\n469\r\n00:29:09,311 --> 00:29:15,050\r\nNuestra capitalizaciÃ³n de mercado era baja y cayÃ³ a niveles realmente bajos.\r\n\r\n470\r\n00:29:15,150 --> 00:29:19,187\r\nCreo que nuestra capitalizaciÃ³n de mercado bajÃ³ a mil millones de dÃ³lares o algo asÃ­.\r\n\r\n471\r\n00:29:19,288 --> 00:29:21,089\r\nOjalÃ¡ lo hubiera comprado, pero bueno.\r\n\r\n472\r\n00:29:21,189 --> 00:29:25,227\r\n-De acuerdo. Y por eso cancelaste inmediatamente Cuda.\r\n\r\n473\r\n00:29:25,327 --> 00:29:26,895\r\ny volviÃ³ a la antigua estrategia.\r\n\r\n474\r\n00:29:26,995 --> 00:29:31,266\r\nâ€”No, no, yo creÃ­a en Cuda, porque tÃº lo razonaste. TÃº lo razonaste.\r\n\r\n475\r\n00:29:31,400 --> 00:29:35,170\r\nMira, realmente creÃ­amos que la computaciÃ³n acelerada\r\n\r\n476\r\n00:29:35,270 --> 00:29:39,775\r\niba a ser capaz de resolver problemas que las computadoras normales no podÃ­an.\r\n\r\n477\r\n00:29:40,309 --> 00:29:43,645\r\nY si quisiÃ©ramos ampliar la arquitectura\r\n\r\n478\r\n00:29:43,779 --> 00:29:49,451\r\nPara ser mucho mÃ¡s generales, tuvimos que hacer ese sacrificio.\r\n\r\n479\r\n00:29:49,584 --> 00:29:55,190\r\nY asÃ­ creÃ­ profundamente en la misiÃ³n de nuestra empresa,\r\n\r\n480\r\n00:29:55,290 --> 00:29:58,727\r\nCreÃ­ profundamente en sus oportunidades. -Y los analistas tambiÃ©n-\r\n\r\n481\r\n00:29:58,827 --> 00:30:01,797\r\n-Creo profundamente que la gente estaba equivocada.\r\n\r\n482\r\n00:30:01,897 --> 00:30:06,168\r\nSimplemente no apreciaron lo que construimos. Yo lo creÃ­a profundamente.\r\n\r\n483\r\n00:30:06,268 --> 00:30:08,670\r\n-Y lo mismo ocurriÃ³ con los analistas, la junta directiva y los empleados--\r\n\r\n484\r\n00:30:08,770 --> 00:30:11,640\r\nHas torpedeado este flujo de ingresos existente.\r\n\r\n485\r\n00:30:11,740 --> 00:30:17,312\r\nTienes esta cosa tan publicitada que estÃ¡s vendiendo un sueÃ±o elevado.\r\n\r\n486\r\n00:30:17,412 --> 00:30:18,847\r\nque nadie parece querer realmente.\r\n\r\n487\r\n00:30:18,980 --> 00:30:21,850\r\nEl negocio estÃ¡ realmente sufriendo,\r\n\r\n488\r\n00:30:22,984 --> 00:30:25,287\r\nCuÃ©ntanoslo. TÃº lo creÃ­ste.\r\n\r\n489\r\n00:30:26,188 --> 00:30:30,592\r\n-Bueno, es algo asÃ­ como: \"Oh, Dios, son tan tontos\".\r\n\r\n490\r\n00:30:32,260 --> 00:30:35,797\r\nAlgo asÃ­: negaciÃ³n.\r\n\r\n491\r\n00:30:35,897 --> 00:30:38,734\r\nNo, solo bromeo. Vuelve a lo que crees.\r\n\r\n492\r\n00:30:38,834 --> 00:30:40,702\r\nY si crees en algo...\r\n\r\n493\r\n00:30:40,802 --> 00:30:43,372\r\n-Â¿La directiva te presionÃ³ durante este proceso?\r\n\r\n494\r\n00:30:44,539 --> 00:30:49,845\r\n-Ellos... Empiezo cada conversaciÃ³n con lo que creo profundamente.\r\n\r\n495\r\n00:30:49,945 --> 00:30:54,416\r\ny lo creyeron porque me vieron creerlo profundamente.\r\n\r\n496\r\n00:30:54,516 --> 00:30:55,751\r\nY razonÃ© sobre ello.\r\n\r\n497\r\n00:30:55,851 --> 00:30:59,521\r\nNo era como si fuera una hoja de cÃ¡lculo,\r\n\r\n498\r\n00:30:59,621 --> 00:31:01,423\r\ny por lo tanto tienes que creer en la hoja de cÃ¡lculo.\r\n\r\n499\r\n00:31:01,556 --> 00:31:03,525\r\nTenÃ­an que creer en el razonamiento, en las palabras.\r\n\r\n500\r\n00:31:03,658 --> 00:31:06,962\r\n-Â¿CuÃ¡nto tiempo tardÃ³ en empezar a funcionar?\r\n\r\n501\r\n00:31:09,865 --> 00:31:12,534\r\n-Probablemente diez aÃ±os, sÃ­.\r\n\r\n502\r\n00:31:13,368 --> 00:31:15,103\r\nNo pasÃ³ tanto tiempo\r\n\r\n503\r\n00:31:16,304 --> 00:31:18,640\r\nDiez aÃ±os. Va y viene.\r\n\r\n504\r\n00:31:18,874 --> 00:31:21,109\r\n-Diez aÃ±os. -Menos de un tercio de su mandato.\r\n\r\n505\r\n00:31:21,209 --> 00:31:24,012\r\n-SÃ­, va y viene. Apenas lo recordaba.\r\n\r\n506\r\n00:31:24,112 --> 00:31:26,281\r\nEl sufrimiento, apenas lo recordaba.\r\n\r\n507\r\n00:31:26,381 --> 00:31:31,486\r\n-Â¿PodrÃ­a NVIDIA tener tanto Ã©xito en IA sin Cuda?\r\n\r\n508\r\n00:31:31,586 --> 00:31:33,054\r\n-No, imposible.\r\n\r\n509\r\n00:31:33,188 --> 00:31:38,693\r\nEs potencialmente uno de los inventos mÃ¡s importantes de la informÃ¡tica moderna.\r\n\r\n510\r\n00:31:38,794 --> 00:31:41,430\r\nInventamos esta idea llamada computaciÃ³n acelerada,\r\n\r\n511\r\n00:31:41,530 --> 00:31:46,968\r\ny la idea es tan simple pero profundamente profunda.\r\n\r\n512\r\n00:31:47,235 --> 00:31:50,071\r\nDice la gran mayorÃ­a...\r\n\r\n513\r\n00:31:50,172 --> 00:31:54,309\r\nun pequeÃ±o porcentaje del cÃ³digo de los programas\r\n\r\n514\r\n00:31:54,409 --> 00:31:59,648\r\nocupa, consume, el 99,999% del tiempo de ejecuciÃ³n.\r\n\r\n515\r\n00:31:59,748 --> 00:32:03,118\r\nY esto es cierto para muchas aplicaciones muy importantes.\r\n\r\n516\r\n00:32:03,218 --> 00:32:07,222\r\nY ese pequeÃ±o grano,\r\n\r\n517\r\n00:32:07,322 --> 00:32:12,794\r\no varios nÃºcleos, se pueden acelerar.\r\n\r\n518\r\n00:32:12,894 --> 00:32:16,031\r\nNo es todo sÃ³lo procesamiento paralelo.\r\n\r\n519\r\n00:32:16,131 --> 00:32:17,199\r\nNo es tan sencillo como eso.\r\n\r\n520\r\n00:32:17,332 --> 00:32:20,202\r\nPero la idea es que podemos tomar ese nÃºcleo,\r\n\r\n521\r\n00:32:20,302 --> 00:32:22,103\r\nesa pieza de software, esa parte del software,\r\n\r\n522\r\n00:32:22,204 --> 00:32:24,339\r\ny acelerar el proceso de vida.\r\n\r\n523\r\n00:32:24,439 --> 00:32:27,142\r\nY hoy, cuando la Ley de Moore ha cumplido su curso,\r\n\r\n524\r\n00:32:27,275 --> 00:32:33,482\r\ny el escalado de la CPU se detiene bÃ¡sicamente, y si no aceleramos todo el software,\r\n\r\n525\r\n00:32:33,582 --> 00:32:37,018\r\nVeremos una inflaciÃ³n computacional extraordinaria.\r\n\r\n526\r\n00:32:37,118 --> 00:32:39,254\r\nPorque la cantidad de computaciÃ³n que el mundo hace\r\n\r\n527\r\n00:32:39,354 --> 00:32:40,956\r\nSe sigue duplicando cada aÃ±o,\r\n\r\n528\r\n00:32:41,089 --> 00:32:43,658\r\ny, sin embargo, si las CPU y las computadoras de propÃ³sito general\r\n\r\n529\r\n00:32:43,758 --> 00:32:46,461\r\nNo estÃ¡n aumentando en rendimiento porque se detuvieron,\r\n\r\n530\r\n00:32:46,595 --> 00:32:47,963\r\nEntonces Â¿cuÃ¡l es tu alternativa?\r\n\r\n531\r\n00:32:48,063 --> 00:32:50,832\r\nEl coste de su informÃ¡tica seguirÃ¡ aumentando exponencialmente.\r\n\r\n532\r\n00:32:50,932 --> 00:32:53,468\r\nAsÃ­ que ha llegado el momento de que lo hagamos.\r\n\r\n533\r\n00:32:53,568 --> 00:32:56,438\r\n-Entonces aquÃ­ todo el mundo tiene un negocio y...\r\n\r\n534\r\n00:32:56,705 --> 00:32:58,073\r\n-Â¡Acelera todo!\r\n\r\n535\r\n00:32:58,173 --> 00:32:59,674\r\n-Y lo escuchaste aquÃ­ primero.\r\n\r\n536\r\n00:32:59,774 --> 00:33:04,513\r\nY probablemente todo el mundo tiene alguna versiÃ³n de Cuda.\r\n\r\n537\r\n00:33:04,613 --> 00:33:09,284\r\no algo que crean que realmente tiene sentido para el sector\r\n\r\n538\r\n00:33:09,384 --> 00:33:12,220\r\no tiene sentido para su tecnologÃ­a o lo que sea,\r\n\r\n539\r\n00:33:12,320 --> 00:33:15,023\r\npero donde el mercado aÃºn no lo ve.\r\n\r\n540\r\n00:33:15,323 --> 00:33:19,628\r\nÂ¿Crees que es posible extraer algÃºn tipo de principios generalizables?\r\n\r\n541\r\n00:33:19,761 --> 00:33:23,198\r\nCuando realmente deberÃ­as confiar tenazmente en esa visiÃ³n,\r\n\r\n542\r\n00:33:23,298 --> 00:33:25,834\r\ny cuando quizÃ¡s valga la pena reconsiderarlo\r\n\r\n543\r\n00:33:25,934 --> 00:33:28,103\r\nde una manera que pudiÃ©ramos extrapolar,\r\n\r\n544\r\n00:33:28,236 --> 00:33:30,939\r\nen el caso de Cuda y otros \"Cudas\" que han existido a lo largo de\r\n\r\n545\r\n00:33:31,072 --> 00:33:32,140\r\nÂ¿CuÃ¡l ha sido el curso de la historia de NVIDIA?\r\n\r\n546\r\n00:33:32,240 --> 00:33:38,079\r\n-SÃ­. La cuestiÃ³n es determinaciÃ³n y compromiso versus terquedad.\r\n\r\n547\r\n00:33:38,179 --> 00:33:42,150\r\ny esa lÃ­nea es borrosa.\r\n\r\n548\r\n00:33:43,018 --> 00:33:47,689\r\nMe puse a prueba mis creencias fundamentales todos los dÃ­as,\r\n\r\n549\r\n00:33:47,822 --> 00:33:50,692\r\nTodavÃ­a lo hago. Y compruÃ©balo tÃº mismo.\r\n\r\n550\r\n00:33:50,792 --> 00:33:55,931\r\nLos primeros principios con los que razonas sobre tus estrategias,\r\n\r\n551\r\n00:33:56,031 --> 00:33:58,667\r\nlos primeros principios por los cuales razonas acerca de tus estrategias,\r\n\r\n552\r\n00:33:58,767 --> 00:34:04,973\r\nEsos primeros principios son fÃ¡ciles de recordar y no es una lista larga.\r\n\r\n553\r\n00:34:05,073 --> 00:34:10,145\r\nY ahora la pregunta es, Â¿son esos principios,\r\n\r\n554\r\n00:34:10,245 --> 00:34:12,581\r\nÂ¿Cambiaron de alguna manera fundamental?\r\n\r\n555\r\n00:34:12,681 --> 00:34:17,886\r\nÂ¿Las condiciones externas son tales que ya no importan tanto como antes?\r\n\r\n556\r\n00:34:18,019 --> 00:34:20,121\r\nÂ¿Alguien mÃ¡s resolviÃ³ el problema?\r\n\r\n557\r\n00:34:20,221 --> 00:34:23,825\r\nÂ¿Y por eso ese problema ahora ha desaparecido?\r\n\r\n558\r\n00:34:23,925 --> 00:34:28,196\r\nÂ¿SerÃ¡ que nunca habrÃ¡ necesidad?\r\n\r\n559\r\n00:34:28,296 --> 00:34:30,432\r\nLo revisas instintivamente, Â¿verdad?, constantemente.\r\n\r\n560\r\n00:34:30,532 --> 00:34:34,569\r\nY en la medida en que... ese es el nÃºmero uno, una revisiÃ³n instintiva.\r\n\r\n561\r\n00:34:34,703 --> 00:34:38,440\r\nEn primer lugar, hay que tener mucho cuidado de destilar el primer principio,\r\n\r\n562\r\n00:34:38,540 --> 00:34:42,844\r\nEn lugar de, \"quiero algo\", eso es terquedad.\r\n\r\n563\r\n00:34:42,944 --> 00:34:44,913\r\nNo puedes razonar sobre ello. Solo lo quiero.\r\n\r\n564\r\n00:34:45,013 --> 00:34:46,648\r\nNo tenemos cinco aÃ±os Â¿verdad?\r\n\r\n565\r\n00:34:46,748 --> 00:34:50,485\r\nAsÃ­ que, en primer lugar, tienes que razonar sobre ello.\r\n\r\n566\r\n00:34:50,585 --> 00:34:52,754\r\nNÃºmero dos, tienes que ser inteligente.\r\n\r\n567\r\n00:34:52,854 --> 00:34:56,024\r\nEl hecho es que se estÃ¡n creando muchas empresas nuevas aquÃ­.\r\n\r\n568\r\n00:34:56,124 --> 00:35:00,095\r\nEs sorprendente cuÃ¡ntas grandes empresas hay entre el pÃºblico.\r\n\r\n569\r\n00:35:00,195 --> 00:35:02,330\r\ny empresas jÃ³venes entre el pÃºblico.\r\n\r\n570\r\n00:35:02,430 --> 00:35:03,498\r\nTienes que ser inteligente.\r\n\r\n571\r\n00:35:03,598 --> 00:35:08,670\r\nY asÃ­ encontramos formas de monetizar,\r\n\r\n572\r\n00:35:08,770 --> 00:35:12,340\r\nAunque sea en pequeÃ±a medida, Cuda.\r\n\r\n573\r\n00:35:12,440 --> 00:35:16,745\r\nY asÃ­ encontramos... buscamos aplicaciones por todas partes.\r\n\r\n574\r\n00:35:16,845 --> 00:35:19,180\r\nEncontramos una aplicaciÃ³n con la reconstrucciÃ³n por TC.\r\n\r\n575\r\n00:35:19,280 --> 00:35:21,650\r\nEncontramos una aplicaciÃ³n con procesamiento sÃ­smico.\r\n\r\n576\r\n00:35:21,750 --> 00:35:25,387\r\nEncontramos otra aplicaciÃ³n con la dinÃ¡mica molecular,\r\n\r\n577\r\n00:35:25,487 --> 00:35:27,656\r\nY por eso estamos constantemente buscando aplicaciones.\r\n\r\n578\r\n00:35:27,756 --> 00:35:34,162\r\nNo fue un jonrÃ³n, pero nos sostuvo lo suficiente.\r\n\r\n579\r\n00:35:34,262 --> 00:35:38,400\r\nY nos dio tiempo para que realmente sucediera.\r\n\r\n580\r\n00:35:38,500 --> 00:35:42,137\r\n-EstÃ¡ bien. Hablemos de la IA.\r\n\r\n581\r\n00:35:42,237 --> 00:35:44,639\r\nVoy a hacer algunos cÃ¡lculos para fundamentar las cosas aquÃ­.\r\n\r\n582\r\n00:35:44,773 --> 00:35:46,808\r\nDigamos simplemente que el total,\r\n\r\n583\r\n00:35:46,941 --> 00:35:53,281\r\nEl promedio de capacidad de cÃ³mputo de todas las GPU del mundo hoy en dÃ­a es X.\r\n\r\n584\r\n00:35:53,381 --> 00:35:56,151\r\nQuÃ© opinas...\r\n\r\n585\r\n00:35:56,251 --> 00:36:02,624\r\nÂ¿En quÃ© mÃºltiplo de X estaremos dentro de cinco aÃ±os?\r\n\r\n586\r\n00:36:04,426 --> 00:36:08,697\r\n-Lo primero que sabes es que me voy a arrepentir de haber dicho esto.\r\n\r\n587\r\n00:36:09,731 --> 00:36:10,999\r\nY esto es ser...\r\n\r\n588\r\n00:36:11,099 --> 00:36:14,402\r\nSoy una empresa pÃºblica, estÃ¡s loco.\r\n\r\n589\r\n00:36:16,037 --> 00:36:22,010\r\nÂ¡QuÃ© agradable es tener privacidad!\r\n\r\n590\r\n00:36:29,250 --> 00:36:31,186\r\n-Se podrÃ­a decir con seguridad mucho mÃ¡s.\r\n\r\n591\r\n00:36:32,053 --> 00:36:35,890\r\nBueno, vamos a razonar sobre ello, Â¿de acuerdo? Bien. Bueno, vamos a razonar sobre ello.\r\n\r\n592\r\n00:36:35,990 --> 00:36:37,759\r\nVamos a razonar nuestro camino, Â¿de acuerdo?\r\n\r\n593\r\n00:36:37,859 --> 00:36:39,828\r\nLo primero que voy a decir es esto:\r\n\r\n594\r\n00:36:39,928 --> 00:36:43,431\r\nEl mundo ha instalado centros de datos por un valor de aproximadamente un billÃ³n de dÃ³lares.\r\n\r\n595\r\n00:36:43,665 --> 00:36:47,902\r\nEsos centros de datos, que valen billones de dÃ³lares, utilizan informÃ¡tica de propÃ³sito general.\r\n\r\n596\r\n00:36:48,002 --> 00:36:50,705\r\nLa informÃ¡tica de propÃ³sito general ha llegado a su fin.\r\n\r\n597\r\n00:36:51,172 --> 00:36:53,541\r\nNo podemos seguir procesando de esa manera.\r\n\r\n598\r\n00:36:53,641 --> 00:36:57,846\r\nY asÃ­, el mundo va a acelerar todo: el procesamiento de datos, etc.\r\n\r\n599\r\n00:36:57,946 --> 00:36:59,981\r\nY entonces vamos a acelerar todo.\r\n\r\n600\r\n00:37:00,081 --> 00:37:02,484\r\nCuando aceleramos todo, cada centro de datos,\r\n\r\n601\r\n00:37:02,617 --> 00:37:05,453\r\nCada computadora serÃ¡ un servidor acelerado.\r\n\r\n602\r\n00:37:05,687 --> 00:37:10,391\r\nBueno, hay alrededor de un billÃ³n de dÃ³lares en computadoras si no crecemos en absoluto.\r\n\r\n603\r\n00:37:10,492 --> 00:37:13,895\r\nEn los prÃ³ximos cuatro aÃ±os tendremos que reemplazarlos.\r\n\r\n604\r\n00:37:13,995 --> 00:37:16,498\r\nCuatro aÃ±os, seis aÃ±os, elige tu nÃºmero de aÃ±os.\r\n\r\n605\r\n00:37:16,598 --> 00:37:20,435\r\nPero si la industria informÃ¡tica continÃºa creciendo a un ritmo del 20% aproximadamente,\r\n\r\n606\r\n00:37:20,735 --> 00:37:24,272\r\nProbablemente tendremos que reemplazarlo, en el transcurso del prÃ³ximo, elija su nÃºmero de aÃ±os,\r\n\r\n607\r\n00:37:24,405 --> 00:37:28,643\r\nalrededor de 2 billones de dÃ³lares en computadoras con computaciÃ³n acelerada.\r\n\r\n608\r\n00:37:29,144 --> 00:37:32,046\r\nEntonces simplemente hagan esas GPU, Â¿de acuerdo?\r\n\r\n609\r\n00:37:32,147 --> 00:37:34,215\r\nEse es el nÃºmero uno. Y esta es la segunda parte.\r\n\r\n610\r\n00:37:34,315 --> 00:37:38,787\r\nEsta es la razÃ³n por la que todos ustedes, Stripe,\r\n\r\n611\r\n00:37:38,887 --> 00:37:42,123\r\nEstÃ¡s ante algo absolutamente monumental.\r\n\r\n612\r\n00:37:42,223 --> 00:37:48,696\r\nEsta idea se llama, y â€‹â€‹me has oÃ­do decir, revoluciÃ³n industrial,\r\n\r\n613\r\n00:37:48,797 --> 00:37:50,231\r\nDÃ©jame decirte por quÃ©.\r\n\r\n614\r\n00:37:50,331 --> 00:37:52,634\r\nEstamos produciendo algo por primera vez.\r\n\r\n615\r\n00:37:52,734 --> 00:37:55,003\r\nque nunca se ha producido antes.\r\n\r\n616\r\n00:37:55,103 --> 00:37:57,572\r\nY lo estamos produciendo en un volumen extremadamente alto.\r\n\r\n617\r\n00:37:57,705 --> 00:38:00,575\r\nY la producciÃ³n de esta \"cosa\"\r\n\r\n618\r\n00:38:00,675 --> 00:38:03,344\r\nrequiere un nuevo instrumento que nunca existiÃ³ antes.\r\n\r\n619\r\n00:38:03,444 --> 00:38:05,313\r\nEs una GPU.\r\n\r\n620\r\n00:38:05,547 --> 00:38:08,416\r\nY lo que estamos produciendo por primera vez,\r\n\r\n621\r\n00:38:08,516 --> 00:38:11,219\r\nPara los matemÃ¡ticos y todos los informÃ¡ticos en la sala,\r\n\r\n622\r\n00:38:11,352 --> 00:38:14,589\r\nPara todos ustedes, saben que estamos produciendo tokens.\r\n\r\n623\r\n00:38:14,756 --> 00:38:20,228\r\nEstamos produciendo nÃºmeros de punto flotante a gran volumen por primera vez en la historia.\r\n\r\n624\r\n00:38:20,361 --> 00:38:22,297\r\ny los nÃºmeros de punto flotante tienen valor.\r\n\r\n625\r\n00:38:22,430 --> 00:38:24,966\r\nLa razÃ³n por la que tienen valor es porque es inteligencia.\r\n\r\n626\r\n00:38:25,099 --> 00:38:26,668\r\nEs inteligencia artificial.\r\n\r\n627\r\n00:38:26,768 --> 00:38:28,403\r\nPuedes tomar estos nÃºmeros de punto flotante.\r\n\r\n628\r\n00:38:28,536 --> 00:38:33,741\r\nLo reformulas de tal manera que se convierte en inglÃ©s, francÃ©s, proteÃ­nas,\r\n\r\n629\r\n00:38:33,842 --> 00:38:39,981\r\nproductos quÃ­micos, grÃ¡ficos, imÃ¡genes, vÃ­deos, articulaciÃ³n robÃ³tica,\r\n\r\n630\r\n00:38:40,081 --> 00:38:41,583\r\nArticulaciÃ³n del volante.\r\n\r\n631\r\n00:38:41,683 --> 00:38:45,153\r\nEstamos produciendo tokens a una escala extraordinaria.\r\n\r\n632\r\n00:38:45,253 --> 00:38:48,289\r\nAhora hemos descubierto una manera de realizar todo el trabajo que hacemos.\r\n\r\n633\r\n00:38:48,389 --> 00:38:52,060\r\ncon inteligencia artificial, para producir tokens de casi cualquier tipo.\r\n\r\n634\r\n00:38:52,694 --> 00:38:58,933\r\nAhora el mundo va a producir una enorme cantidad de tokens.\r\n\r\n635\r\n00:38:59,033 --> 00:39:02,470\r\nAhora estos tokens se producirÃ¡n en nuevos tipos de centros de datos.\r\n\r\n636\r\n00:39:02,570 --> 00:39:05,006\r\nLas llamamos fÃ¡bricas de IA.\r\n\r\n637\r\n00:39:05,106 --> 00:39:10,178\r\nEn la Ãºltima revoluciÃ³n industrial, el agua entraba en una mÃ¡quina,\r\n\r\n638\r\n00:39:10,278 --> 00:39:12,814\r\nEnciendes el agua al fuego, Â¿verdad?\r\n\r\n639\r\n00:39:12,947 --> 00:39:17,252\r\nConviÃ©rtalo en vapor y luego se convertirÃ¡ en electrones.\r\n\r\n640\r\n00:39:17,552 --> 00:39:19,554\r\nLos Ã¡tomos entran, los electrones salen.\r\n\r\n641\r\n00:39:19,654 --> 00:39:21,389\r\nEn esta nueva revoluciÃ³n industrial,\r\n\r\n642\r\n00:39:21,489 --> 00:39:24,959\r\nLos electrones entran y salen nÃºmeros de punto flotante.\r\n\r\n643\r\n00:39:25,393 --> 00:39:27,896\r\nY al igual que la Ãºltima revoluciÃ³n industrial,\r\n\r\n644\r\n00:39:28,029 --> 00:39:32,267\r\nNadie entendiÃ³ por quÃ© la electricidad es tan valiosa y ahora se vende,\r\n\r\n645\r\n00:39:32,400 --> 00:39:35,536\r\nkilovatios hora comercializados por dÃ³lar.\r\n\r\n646\r\n00:39:35,870 --> 00:39:39,674\r\nY ahora tenemos un millÃ³n de tokens por dÃ³lar.\r\n\r\n647\r\n00:39:39,941 --> 00:39:42,844\r\nY entonces esa misma lÃ³gica\r\n\r\n648\r\n00:39:42,944 --> 00:39:47,215\r\nEs tan incomprensible para mucha gente como la Ãºltima revoluciÃ³n industrial,\r\n\r\n649\r\n00:39:47,315 --> 00:39:51,853\r\nPero serÃ¡ completamente normal en los prÃ³ximos diez aÃ±os.\r\n\r\n650\r\n00:39:52,353 --> 00:39:58,826\r\nEstos tokens van a crear nuevos productos, nuevos servicios,\r\n\r\n651\r\n00:39:58,927 --> 00:40:02,330\r\nmejorar la productividad en una gran variedad de industrias.\r\n\r\n652\r\n00:40:02,430 --> 00:40:05,533\r\nIndustrias por valor de 100 billones de dÃ³lares encima de nosotros.\r\n\r\n653\r\n00:40:05,633 --> 00:40:07,669\r\nY esta industria va a ser gigantesca.\r\n\r\n654\r\n00:40:07,769 --> 00:40:14,742\r\nY para monetizar eso, para realizar transacciones, necesitarÃ¡s Stripe.\r\n\r\n655\r\n00:40:20,048 --> 00:40:23,051\r\nTengo que decirles que esta es una de mis empresas favoritas.\r\n\r\n656\r\n00:40:23,151 --> 00:40:26,220\r\nLa primera vez que conocÃ­ a Patrick, tuvo que explicarme quÃ© era Stripe.\r\n\r\n657\r\n00:40:26,321 --> 00:40:29,190\r\nLo primero que dije fue que era muy complicado.\r\n\r\n658\r\n00:40:31,059 --> 00:40:33,895\r\n-Intentamos refinar las descripciones con el tiempo.\r\n\r\n659\r\n00:40:33,995 --> 00:40:37,165\r\n-En primer lugar, estÃ¡s en un negocio complicado, pase lo que pase.\r\n\r\n660\r\n00:40:37,265 --> 00:40:41,235\r\nPero aÃºn asÃ­, me inspirÃ³ mucho.\r\n\r\n661\r\n00:40:41,336 --> 00:40:42,637\r\nIncreÃ­ble lo que han construido.\r\n\r\n662\r\n00:40:42,737 --> 00:40:44,372\r\n-Lo vamos a migrar a Stripe Billing\r\n\r\n663\r\n00:40:44,472 --> 00:40:46,174\r\nÂ¿Ahora que tenemos una facturaciÃ³n basada en uso?\r\n\r\n664\r\n00:40:46,307 --> 00:40:49,711\r\n-DesearÃ­a tener un negocio que requiriera facturaciÃ³n.\r\n\r\n665\r\n00:40:50,712 --> 00:40:54,415\r\n-Creo que su presentaciÃ³n pÃºblica sugiere que estÃ¡ realizando mucha facturaciÃ³n.\r\n\r\n666\r\n00:40:56,918 --> 00:40:59,687\r\nLe daremos seguimiento. EstÃ¡ bien.\r\n\r\n667\r\n00:41:00,521 --> 00:41:03,458\r\n-Solo son diez transacciones. Para que lo sepas.\r\n\r\n668\r\n00:41:04,592 --> 00:41:09,731\r\nSu economÃ­a al servicio de nosotros no es nada, es como diez transacciones.\r\n\r\n669\r\n00:41:10,198 --> 00:41:11,632\r\n-AceptarÃ­amos con mucho gusto el 2,9%.\r\n\r\n670\r\n00:41:11,733 --> 00:41:15,670\r\nPero de todos modos podemos discutirlo por separado.\r\n\r\n671\r\n00:41:18,906 --> 00:41:20,375\r\n-Â¡Hecho!\r\n\r\n672\r\n00:41:20,475 --> 00:41:24,412\r\n-No puedes decir eso. Eres una empresa pÃºblica.\r\n\r\n673\r\n00:41:24,512 --> 00:41:27,315\r\nPensando en estas fÃ¡bricas de fichas.\r\n\r\n674\r\n00:41:27,448 --> 00:41:31,753\r\nSiento que una gran pregunta en este momento es si los modelos se saturan.\r\n\r\n675\r\n00:41:31,886 --> 00:41:34,989\r\nEn el sentido de que hicimos una demostraciÃ³n del asistente Sigma en el escenario anteriormente.\r\n\r\n676\r\n00:41:35,123 --> 00:41:37,859\r\ny puedes escribir algÃºn lenguaje natural y lo convertimos a SQL.\r\n\r\n677\r\n00:41:37,959 --> 00:41:41,763\r\nY partiendo de un modelo de quizÃ¡s 7 mil millones de parÃ¡metros\r\n\r\n678\r\n00:41:41,896 --> 00:41:44,432\r\na un modelo de 70 mil millones de parÃ¡metros o algo asÃ­,\r\n\r\n679\r\n00:41:44,565 --> 00:41:46,401\r\nPodrÃ­a haber un tipo significativo de,\r\n\r\n680\r\n00:41:46,501 --> 00:41:49,971\r\nMejora consecuente en la precisiÃ³n de las consultas para el usuario\r\n\r\n681\r\n00:41:50,071 --> 00:41:52,373\r\npara los tipos tÃ­picos de consultas que la gente tiende a construir.\r\n\r\n682\r\n00:41:52,473 --> 00:41:56,144\r\nPero quizÃ¡ optar por un modelo diez veces mÃ¡s grande sea algo innecesario.\r\n\r\n683\r\n00:41:56,277 --> 00:41:57,345\r\nComo en algÃºn momento llegas a ser lo suficientemente bueno,\r\n\r\n684\r\n00:41:57,478 --> 00:41:59,714\r\nPuede convertir de forma confiable el lenguaje natural a SQL.\r\n\r\n685\r\n00:41:59,881 --> 00:42:01,682\r\nCreo que hay una cuestiÃ³n de,\r\n\r\n686\r\n00:42:01,783 --> 00:42:05,386\r\npara los casos de uso para los cuales se estÃ¡n implementando los LLM,\r\n\r\n687\r\n00:42:05,520 --> 00:42:07,889\r\nÂ¿CÃ³mo se ve esa curva de saturaciÃ³n?\r\n\r\n688\r\n00:42:07,989 --> 00:42:11,559\r\nÂ¿Y para cuÃ¡ntos casos de uso se necesita un modelo de un billÃ³n de parÃ¡metros?\r\n\r\n689\r\n00:42:11,692 --> 00:42:12,760\r\nÂ¿Un modelo de 10 billones de parÃ¡metros?\r\n\r\n690\r\n00:42:12,894 --> 00:42:16,030\r\nÂ¿O simplemente llegamos a un punto en el que, digamos, algÃºn nÃºmero,\r\n\r\n691\r\n00:42:16,164 --> 00:42:19,467\r\nÂ¿Menos de 100 mil millones es suficiente?\r\n\r\n692\r\n00:42:19,567 --> 00:42:23,304\r\nÂ¿Tienes algÃºn punto de vista sobre eso, o es incluso...\r\n\r\n693\r\n00:42:23,404 --> 00:42:26,541\r\nÂ¿Una manera razonable de abordar la cuestiÃ³n en primer lugar?\r\n\r\n694\r\n00:42:26,641 --> 00:42:28,576\r\nBueno, vamos a desglosarlo. Vamos a razonarlo.\r\n\r\n695\r\n00:42:30,044 --> 00:42:31,045\r\n-En pÃºblico.\r\n\r\n696\r\n00:42:31,312 --> 00:42:33,081\r\n-Casi todo, cada pregunta que me hacen,\r\n\r\n697\r\n00:42:33,181 --> 00:42:35,116\r\nBien, vamos a desglosarlo, vamos a razonarlo.\r\n\r\n698\r\n00:42:35,216 --> 00:42:37,485\r\nEntonces, comencemos con un ejemplo.\r\n\r\n699\r\n00:42:37,752 --> 00:42:41,856\r\nEn 2012, AlexNet era una visiÃ³n por computadora.\r\n\r\n700\r\n00:42:41,956 --> 00:42:47,762\r\nImageNet, reconocimiento de imÃ¡genes 82%, o algo asÃ­, de precisiÃ³n.\r\n\r\n701\r\n00:42:47,895 --> 00:42:52,100\r\nDurante los prÃ³ximos... casi diez aÃ±os,\r\n\r\n702\r\n00:42:52,233 --> 00:42:53,801\r\nCreo que fueron como siete aÃ±os,\r\n\r\n703\r\n00:42:54,102 --> 00:42:59,841\r\nCada aÃ±o, el error de precisiÃ³n se reduce a la mitad.\r\n\r\n704\r\n00:42:59,974 --> 00:43:04,445\r\nCada aÃ±o el error se reduce a la mitad o tambiÃ©n conocido como ley de Moore.\r\n\r\n705\r\n00:43:04,545 --> 00:43:10,084\r\nAsÃ­ que duplicas el rendimiento y la precisiÃ³n.\r\n\r\n706\r\n00:43:10,518 --> 00:43:14,322\r\ny duplicas su credibilidad cada aÃ±o.\r\n\r\n707\r\n00:43:14,422 --> 00:43:16,891\r\nEn el transcurso de siete aÃ±os, ahora es sobrehumano.\r\n\r\n708\r\n00:43:17,525 --> 00:43:19,260\r\nLo mismo ocurre con el reconocimiento de voz.\r\n\r\n709\r\n00:43:19,360 --> 00:43:22,797\r\nLo mismo ocurre con la comprensiÃ³n del lenguaje natural.\r\n\r\n710\r\n00:43:22,897 --> 00:43:27,301\r\nQueremos saber, queremos creer, no saber,\r\n\r\n711\r\n00:43:27,401 --> 00:43:31,806\r\nQueremos creer que la respuesta que se nos predice es precisa.\r\n\r\n712\r\n00:43:31,906 --> 00:43:33,174\r\nQueremos creer eso.\r\n\r\n713\r\n00:43:33,274 --> 00:43:36,644\r\nY entonces la industria va a perseguir\r\n\r\n714\r\n00:43:36,744 --> 00:43:39,514\r\nesa credibilidad o esa exactitud,\r\n\r\n715\r\n00:43:39,614 --> 00:43:46,454\r\ny duplicar su precisiÃ³n dos veces al aÃ±o.\r\n\r\n716\r\n00:43:46,554 --> 00:43:50,258\r\nCreo que sucederÃ¡ lo mismo con la comprensiÃ³n del lenguaje natural.\r\n\r\n717\r\n00:43:50,391 --> 00:43:54,829\r\nY, por supuesto, el espacio de problemas es mucho mÃ¡s complicado.\r\n\r\n718\r\n00:43:54,962 --> 00:43:56,864\r\npero tengo toda la certeza\r\n\r\n719\r\n00:43:56,964 --> 00:44:00,067\r\nque vamos a duplicar su precisiÃ³n cada aÃ±o\r\n\r\n720\r\n00:44:00,168 --> 00:44:02,470\r\nhasta el punto de que es tan preciso.\r\n\r\n721\r\n00:44:02,603 --> 00:44:05,373\r\nY hemos probado en gran medida muchos de sus ejemplos,\r\n\r\n722\r\n00:44:05,473 --> 00:44:07,441\r\nCuando interactÃºas con Ã©l piensas: \"Â¿Sabes quÃ©?\r\n\r\n723\r\n00:44:07,575 --> 00:44:08,876\r\n\"Esto es realmente, realmente bueno.\"\r\n\r\n724\r\n00:44:08,976 --> 00:44:13,214\r\nCreo que la respuesta que estÃ¡ produciendo para mÃ­, esa condiciÃ³n es muy importante.\r\n\r\n725\r\n00:44:13,347 --> 00:44:15,216\r\nLa segunda cosa es esta.\r\n\r\n726\r\n00:44:18,319 --> 00:44:20,454\r\nLos modelos de lenguaje de hoy, la IA de hoy\r\n\r\n727\r\n00:44:20,555 --> 00:44:22,823\r\nY todo lo que hemos mostrado es una sola toma.\r\n\r\n728\r\n00:44:23,691 --> 00:44:27,461\r\nY, sin embargo, tÃº y yo sabemos que hay muchas cosas en las que pensamos que...\r\n\r\n729\r\n00:44:27,562 --> 00:44:29,931\r\nNo se trata de una sola toma. Hay que iterar.\r\n\r\n730\r\n00:44:30,031 --> 00:44:33,201\r\nÂ¿Y entonces cÃ³mo se llega... cÃ³mo se razona acerca de un plan?\r\n\r\n731\r\n00:44:33,334 --> 00:44:37,805\r\nÂ¿CÃ³mo se elabora una estrategia para resolver un problema?\r\n\r\n732\r\n00:44:37,905 --> 00:44:41,442\r\nQuizÃ¡s necesites usar herramientas. QuizÃ¡s tengas que buscar datos confidenciales.\r\n\r\n733\r\n00:44:41,576 --> 00:44:44,545\r\nQuizÃ¡s tengas que investigar un poco, de hecho.\r\n\r\n734\r\n00:44:44,645 --> 00:44:48,115\r\nQuizÃ¡s tengas que preguntarle a otro agente. QuizÃ¡s tengas que preguntarle a otra IA.\r\n\r\n735\r\n00:44:48,216 --> 00:44:51,018\r\nQuizÃ¡s tengas que ser humano en un bucle. PregÃºntale a un humano.\r\n\r\n736\r\n00:44:51,118 --> 00:44:53,721\r\nActivar eventos, enviar un correo electrÃ³nico o un mensaje de texto a alguien.\r\n\r\n737\r\n00:44:53,854 --> 00:44:58,092\r\nObtenga una respuesta antes de poder pasar al siguiente paso de ese plan.\r\n\r\n738\r\n00:44:58,192 --> 00:45:02,163\r\nY por eso un modelo de lenguaje grande tiene que iterar y pensar en un plan.\r\n\r\n739\r\n00:45:02,930 --> 00:45:04,732\r\nEsto no es algo que se hace una sola vez.\r\n\r\n740\r\n00:45:04,865 --> 00:45:08,736\r\nY una vez que se le ocurre un plan a medida que recorre ese grÃ¡fico,\r\n\r\n741\r\n00:45:09,170 --> 00:45:10,571\r\nHay un montÃ³n de modelos de lenguaje\r\n\r\n742\r\n00:45:10,671 --> 00:45:13,374\r\nque se van a instanciar e iniciar.\r\n\r\n743\r\n00:45:13,474 --> 00:45:17,912\r\nAsÃ­ que creo que sus modelos futuros se van a iterar.\r\n\r\n744\r\n00:45:18,045 --> 00:45:23,284\r\nY asÃ­, en lugar de un modelo de un solo disparo,\r\n\r\n745\r\n00:45:23,417 --> 00:45:24,986\r\nSerÃ¡ un modelo de planificaciÃ³n\r\n\r\n746\r\n00:45:25,086 --> 00:45:26,754\r\ncon un montÃ³n de otros modelos a su alrededor\r\n\r\n747\r\n00:45:26,854 --> 00:45:28,990\r\nque son particularmente buenos en habilidades particulares.\r\n\r\n748\r\n00:45:29,090 --> 00:45:30,758\r\nAsÃ­ que creo que todavÃ­a tenemos un largo camino por recorrer.\r\n\r\n749\r\n00:45:32,360 --> 00:45:35,863\r\n-Meta recibiÃ³ mucha atenciÃ³n la semana pasada por el lanzamiento de Llama 3,\r\n\r\n750\r\n00:45:35,963 --> 00:45:38,933\r\nque parece ser el modelo de cÃ³digo abierto mÃ¡s impresionante hasta el momento.\r\n\r\n751\r\n00:45:39,033 --> 00:45:40,901\r\nÂ¿Alguna idea sobre los modelos de cÃ³digo abierto?\r\n\r\n752\r\n00:45:42,169 --> 00:45:46,073\r\n-Si me preguntas Â¿cuÃ¡les son los mejores?\r\n\r\n753\r\n00:45:46,173 --> 00:45:48,643\r\nÂ¿CuÃ¡les fueron los acontecimientos mÃ¡s importantes de los Ãºltimos aÃ±os?\r\n\r\n754\r\n00:45:48,743 --> 00:45:51,112\r\nTe dirÃ­a, por supuesto, ChatGPT,\r\n\r\n755\r\n00:45:51,212 --> 00:45:52,747\r\naprendizaje de refuerzo, retroalimentaciÃ³n humana,\r\n\r\n756\r\n00:45:52,880 --> 00:45:56,951\r\narraigÃ¡ndolo en valores humanos y contando con la tecnologÃ­a necesaria para hacerlo.\r\n\r\n757\r\n00:45:57,051 --> 00:46:02,990\r\nObviamente un gran avance y una informÃ¡tica democratizada.\r\n\r\n758\r\n00:46:03,524 --> 00:46:05,760\r\nHizo posible que todos pudieran ser programadores.\r\n\r\n759\r\n00:46:05,893 --> 00:46:08,396\r\nAhora todo el mundo estÃ¡ haciendo cosas increÃ­bles con Ã©l.\r\n\r\n760\r\n00:46:08,496 --> 00:46:12,033\r\nChatGPT, el trabajo que realizÃ³ OpenAI,\r\n\r\n761\r\n00:46:12,166 --> 00:46:14,335\r\nGreg y Sam y el equipo, realmente orgullosos de ellos.\r\n\r\n762\r\n00:46:15,002 --> 00:46:21,008\r\nLa segunda cosa que dirÃ­a que es igualmente importante,\r\n\r\n763\r\n00:46:21,142 --> 00:46:25,813\r\nYo dirÃ­a, es Llama, no Llama 1, sino Llama 2.\r\n\r\n764\r\n00:46:25,913 --> 00:46:30,685\r\nLlama 2 activÃ³ casi todas las industrias\r\n\r\n765\r\n00:46:30,785 --> 00:46:34,522\r\npara empezar a trabajar en la IA generativa.\r\n\r\n766\r\n00:46:35,122 --> 00:46:39,093\r\nY abriÃ³ las compuertas de todas las industrias.\r\n\r\n767\r\n00:46:39,193 --> 00:46:41,262\r\npoder acceder a esta tecnologÃ­a,\r\n\r\n768\r\n00:46:41,362 --> 00:46:44,231\r\natenciÃ³n mÃ©dica, servicios financieros, lo que sea,\r\n\r\n769\r\n00:46:44,332 --> 00:46:48,669\r\nFabricaciÃ³n, lo que sea, servicio al cliente, venta minorista, de todo tipo.\r\n\r\n770\r\n00:46:48,803 --> 00:46:51,339\r\nCreo que Llama 2 y Llama 3,\r\n\r\n771\r\n00:46:52,006 --> 00:46:55,042\r\nPorque es de cÃ³digo abierto, involucrÃ³ investigaciÃ³n,\r\n\r\n772\r\n00:46:55,142 --> 00:46:59,780\r\nInvolucrÃ³ a startups y a la industria. Hizo accesible la IA generativa.\r\n\r\n773\r\n00:46:59,880 --> 00:47:02,149\r\nPienso que esto es algo muy importante.\r\n\r\n774\r\n00:47:02,249 --> 00:47:05,820\r\nY creo que ChatGPT democratizÃ³ la informÃ¡tica.\r\n\r\n775\r\n00:47:05,920 --> 00:47:09,490\r\nCreo que Llama democratizÃ³ la IA generativa. Â¿Tiene sentido?\r\n\r\n776\r\n00:47:09,590 --> 00:47:11,525\r\nY creo que sin ella,\r\n\r\n777\r\n00:47:11,659 --> 00:47:16,297\r\nEs muy difÃ­cil haber activado toda la investigaciÃ³n sobre seguridad.\r\n\r\n778\r\n00:47:16,397 --> 00:47:18,833\r\ny todas las diferentes formas de cadenas de pensamientos.\r\n\r\n779\r\n00:47:18,933 --> 00:47:22,136\r\nY toda la tecnologÃ­a de razonamiento que se estÃ¡ desarrollando ahora\r\n\r\n780\r\n00:47:22,269 --> 00:47:24,505\r\ny todo el asunto del aprendizaje de refuerzo.\r\n\r\n781\r\n00:47:24,638 --> 00:47:27,742\r\nY eso habrÃ­a sido muy difÃ­cil.\r\n\r\n782\r\n00:47:27,842 --> 00:47:29,710\r\nhaber activado sin Llama.\r\n\r\n783\r\n00:47:29,810 --> 00:47:34,582\r\n-Dario Amodei estuvo en el podcast de Ezra Klein hace dos semanas.\r\n\r\n784\r\n00:47:34,715 --> 00:47:37,451\r\nY Ã©l, como muchos otros,\r\n\r\n785\r\n00:47:37,551 --> 00:47:42,456\r\nmuchos otros en particular que estÃ¡n involucrados con Frontier Labs,\r\n\r\n786\r\n00:47:42,556 --> 00:47:46,727\r\nestaba prediciendo IAG en un plazo relativamente corto.\r\n\r\n787\r\n00:47:46,827 --> 00:47:48,729\r\nEs posible que en los prÃ³ximos aÃ±os,\r\n\r\n788\r\n00:47:48,863 --> 00:47:52,433\r\nCon frecuencia se mencionan aÃ±os como 2027 y asÃ­ sucesivamente.\r\n\r\n789\r\n00:47:52,533 --> 00:47:54,268\r\nÂ¿Pensamientos?\r\n\r\n790\r\n00:47:54,735 --> 00:47:57,638\r\n-Dependiendo de cÃ³mo definas IAG.\r\n\r\n791\r\n00:47:57,905 --> 00:48:00,574\r\nAhora bien, en primer lugar, como ingeniero,\r\n\r\n792\r\n00:48:00,674 --> 00:48:04,945\r\nSabes que sÃ³lo podemos resolver un problema,\r\n\r\n793\r\n00:48:05,045 --> 00:48:07,381\r\nEn Ãºltima instancia, si puedes medirlo.\r\n\r\n794\r\n00:48:08,082 --> 00:48:11,018\r\nY entonces tienes que expresar el enunciado del problema,\r\n\r\n795\r\n00:48:11,118 --> 00:48:15,022\r\nla misiÃ³n, de alguna manera, de alguna manera medible.\r\n\r\n796\r\n00:48:15,156 --> 00:48:20,161\r\nSi me dijeras que AGI es la lista de puntos de referencia que utilizamos actualmente,\r\n\r\n797\r\n00:48:20,261 --> 00:48:23,831\r\nHay pruebas de matemÃ¡ticas y pruebas de comprensiÃ³n de inglÃ©s,\r\n\r\n798\r\n00:48:23,931 --> 00:48:30,404\r\ny pruebas de razonamiento, y exÃ¡menes mÃ©dicos, y barras,\r\n\r\n799\r\n00:48:30,504 --> 00:48:33,240\r\ny haces tu lista de todas las pruebas que quieres.\r\n\r\n800\r\n00:48:33,340 --> 00:48:36,177\r\nNo importa lo que sea, simplemente haz tu lista.\r\n\r\n801\r\n00:48:36,277 --> 00:48:39,480\r\nSi haces tu lista, estoy seguro\r\n\r\n802\r\n00:48:39,580 --> 00:48:43,884\r\nLograremos excelentes resultados en un tiempo muy nominal.\r\n\r\n803\r\n00:48:44,018 --> 00:48:48,055\r\nY si esa es la definiciÃ³n de IAG, harÃ© una conjetura,\r\n\r\n804\r\n00:48:48,189 --> 00:48:51,125\r\nProbablemente serÃ¡ definitivamente dentro de los prÃ³ximos cinco aÃ±os.\r\n\r\n805\r\n00:48:51,225 --> 00:48:56,130\r\nY entonces todas las pruebas con las que actualmente medimos estos modelos,\r\n\r\n806\r\n00:48:56,430 --> 00:48:58,933\r\nEstÃ¡n mejorando su precisiÃ³n\r\n\r\n807\r\n00:48:59,033 --> 00:49:03,270\r\no su tasa de error se reduce a la mitad cada seis meses.\r\n\r\n808\r\n00:49:03,704 --> 00:49:06,774\r\nAsÃ­ que no hay ninguna razÃ³n por la que no deberÃ­amos esperarlo.\r\n\r\n809\r\n00:49:06,874 --> 00:49:08,542\r\nTodos serÃ¡n sobrehumanos muy pronto.\r\n\r\n810\r\n00:49:08,642 --> 00:49:10,444\r\n-AsÃ­ que de nuevo, todos en esta audiencia-- -Eso no--\r\n\r\n811\r\n00:49:10,544 --> 00:49:13,514\r\nEso no cumple con el estÃ¡ndar. Solo sÃ© claro.\r\n\r\n812\r\n00:49:13,647 --> 00:49:17,384\r\nEso no cumple con el estÃ¡ndar de una persona normal que piensa que es IAG.\r\n\r\n813\r\n00:49:17,518 --> 00:49:23,123\r\nÂ¿Tiene sentido? Una persona comÃºn y corriente: \"Oye, AGI\".\r\n\r\n814\r\n00:49:23,257 --> 00:49:26,961\r\nProbablemente no es eso lo que estÃ¡n pensando, lo que yo definÃ­ como tal.\r\n\r\n815\r\n00:49:27,328 --> 00:49:30,431\r\nLa forma en que lo definÃ­ es simplemente una forma ingenieril de definirlo.\r\n\r\n816\r\n00:49:30,531 --> 00:49:32,066\r\npara que puedas responder esa pregunta.\r\n\r\n817\r\n00:49:32,199 --> 00:49:35,636\r\nLa segunda forma de responder a la pregunta es cuÃ¡ndo se puede lograr la IAG.\r\n\r\n818\r\n00:49:35,736 --> 00:49:38,172\r\nÂ¿de manera indefinida?\r\n\r\n819\r\n00:49:38,272 --> 00:49:41,141\r\nSi es indefinible, Â¿cuÃ¡nto tiempo lo sabrÃ­as?\r\n\r\n820\r\n00:49:41,275 --> 00:49:43,110\r\nÂ¿CuÃ¡nto tardarÃ­a? Indefinible.\r\n\r\n821\r\n00:49:43,711 --> 00:49:46,247\r\n-Y entonces todos en esta audiencia nuevamente dirigen un negocio.\r\n\r\n822\r\n00:49:46,380 --> 00:49:51,886\r\nY una pregunta prÃ¡ctica a la que todos nos enfrentamos es\r\n\r\n823\r\n00:49:52,052 --> 00:49:57,958\r\nÂ¿CÃ³mo sabes si, frente a los tipos de cambios que acabas de describir, estÃ¡s preparado?\r\n\r\n824\r\n00:49:58,058 --> 00:50:00,561\r\nÂ¿CÃ³mo se sabe? Â¿CÃ³mo se puede saber?\r\n\r\n825\r\n00:50:00,661 --> 00:50:06,267\r\nÂ¿Si uno estÃ¡ respondiendo apropiadamente, suficientemente, de la manera correcta, etc.?\r\n\r\n826\r\n00:50:07,001 --> 00:50:08,068\r\nÂ¿AlgÃºn consejo?\r\n\r\n827\r\n00:50:08,168 --> 00:50:15,442\r\n-Si no estÃ¡s utilizando la IA de forma activa y agresiva, lo estÃ¡s haciendo mal.\r\n\r\n828\r\n00:50:15,543 --> 00:50:17,678\r\nNo vas a perder tu trabajo por culpa de la IA.\r\n\r\n829\r\n00:50:17,778 --> 00:50:20,447\r\nVas a perder tu trabajo por alguien que usa IA.\r\n\r\n830\r\n00:50:20,881 --> 00:50:23,784\r\nSu empresa no va a desaparecer por culpa de la IA.\r\n\r\n831\r\n00:50:23,884 --> 00:50:27,922\r\nSu empresa va a quebrar porque otra empresa utilizÃ³ IA.\r\n\r\n832\r\n00:50:28,322 --> 00:50:29,990\r\nNo hay duda de ello.\r\n\r\n833\r\n00:50:30,090 --> 00:50:32,593\r\nPor lo tanto, es necesario utilizar la IA lo mÃ¡s rÃ¡pidamente posible.\r\n\r\n834\r\n00:50:32,726 --> 00:50:34,361\r\nTienes que activar la IA lo mÃ¡s rÃ¡pido posible,\r\n\r\n835\r\n00:50:34,495 --> 00:50:39,066\r\npara que puedas hacer cosas que crees que cuestan demasiado hacer.\r\n\r\n836\r\n00:50:39,199 --> 00:50:45,172\r\nPor ejemplo, si el coste marginal de la inteligencia fuera prÃ¡cticamente cero.\r\n\r\n837\r\n00:50:45,272 --> 00:50:48,108\r\nHay muchas cosas que harÃ­as ahora\r\n\r\n838\r\n00:50:48,208 --> 00:50:49,677\r\nque de otra manera no lo hubieras hecho.\r\n\r\n839\r\n00:50:49,777 --> 00:50:52,880\r\nY entonces observe con quÃ© frecuencia realizamos bÃºsquedas.\r\n\r\n840\r\n00:50:53,013 --> 00:50:56,817\r\nY hoy en dÃ­a fÃ­jense con quÃ© frecuencia hacemos preguntas.\r\n\r\n841\r\n00:50:56,917 --> 00:51:00,521\r\nCualquier pregunta al azar, le preguntarÃ© a Perplexity.\r\n\r\n842\r\n00:51:00,955 --> 00:51:02,756\r\nCierto. Y entonces, Â¿por quÃ© no?\r\n\r\n843\r\n00:51:02,856 --> 00:51:05,125\r\n-O simplemente dio una charla aquÃ­ en Sessions.\r\n\r\n844\r\n00:51:05,225 --> 00:51:07,861\r\n-EstÃ¡ bien, me encanta usarlo.\r\n\r\n845\r\n00:51:07,962 --> 00:51:12,099\r\nY aunque sepa la respuesta, la preguntarÃ© de todos modos.\r\n\r\n846\r\n00:51:12,199 --> 00:51:13,867\r\nSÃ³lo para ver quÃ© surge.\r\n\r\n847\r\n00:51:14,001 --> 00:51:18,372\r\nY creo que queremos que eso suceda.\r\n\r\n848\r\n00:51:18,472 --> 00:51:19,807\r\nQueremos el costo marginal\r\n\r\n849\r\n00:51:19,940 --> 00:51:22,076\r\nde este tipo de actividades sea lo mÃ¡s baja posible\r\n\r\n850\r\n00:51:22,176 --> 00:51:24,078\r\npara que lo uses en abundancia.\r\n\r\n851\r\n00:51:24,211 --> 00:51:28,582\r\nEn segundo lugar, si pudieras usar la IA para ser productivo...\r\n\r\n852\r\n00:51:29,817 --> 00:51:34,221\r\nYa sabÃ©is que las empresas productivas conducen a mayores ganancias.\r\n\r\n853\r\n00:51:34,355 --> 00:51:36,590\r\nMayores ingresos conducen a mÃ¡s empleo.\r\n\r\n854\r\n00:51:37,725 --> 00:51:39,860\r\nMÃ¡s empleo conduce a un mayor crecimiento social.\r\n\r\n855\r\n00:51:39,994 --> 00:51:44,565\r\nAsÃ­ que hay muchas razones para querer impulsar la productividad en las empresas.\r\n\r\n856\r\n00:51:44,665 --> 00:51:49,603\r\n-Y ademÃ¡s de simplemente cambiar sus planes de fabricaciÃ³n y sus planes de CapEx,\r\n\r\n857\r\n00:51:49,703 --> 00:51:53,374\r\nÂ¿CÃ³mo ha cambiado la IA la forma en que NVIDIA trabaja internamente?\r\n\r\n858\r\n00:51:53,907 --> 00:51:56,510\r\n-Fuimos una de las primeras empresas tecnolÃ³gicas en invertir\r\n\r\n859\r\n00:51:56,610 --> 00:51:59,213\r\nen nuestras propias supercomputadoras de IA.\r\n\r\n860\r\n00:51:59,313 --> 00:52:03,484\r\nYa no podemos diseÃ±ar un chip sin IA.\r\n\r\n861\r\n00:52:03,584 --> 00:52:08,222\r\nPor la noche, nuestras IA exploran espacios de diseÃ±o,\r\n\r\n862\r\n00:52:08,322 --> 00:52:10,691\r\nvasto y amplio que nunca harÃ­amos nosotros mismos\r\n\r\n863\r\n00:52:10,791 --> 00:52:12,459\r\nPorque cuesta demasiado dinero explorarlo.\r\n\r\n864\r\n00:52:12,559 --> 00:52:18,666\r\nY por eso nuestras patatas fritas son mucho mejores.\r\n\r\n865\r\n00:52:18,766 --> 00:52:22,036\r\nGracias a la IA, podrÃ­amos reducir la cantidad de energÃ­a utilizada en nuestros chips\r\n\r\n866\r\n00:52:22,136 --> 00:52:24,371\r\ncomo un mayor rendimiento.\r\n\r\n867\r\n00:52:24,772 --> 00:52:28,475\r\nNuestro software. Ya no podemos escribir software sin IA.\r\n\r\n868\r\n00:52:28,609 --> 00:52:30,077\r\nTenemos que explorar todo el--\r\n\r\n869\r\n00:52:30,177 --> 00:52:34,415\r\nEl espacio de diseÃ±o de los compiladores optimizadores es demasiado grande.\r\n\r\n870\r\n00:52:34,515 --> 00:52:38,552\r\nUsamos IA para informar errores.\r\n\r\n871\r\n00:52:38,652 --> 00:52:40,688\r\nAsÃ­ que nuestra base de datos de errores\r\n\r\n872\r\n00:52:40,788 --> 00:52:46,460\r\nEn realidad te dice quÃ© estÃ¡ mal con el cÃ³digo y quiÃ©n es probable que estÃ© involucrado.\r\n\r\n873\r\n00:52:46,560 --> 00:52:49,697\r\ny activa a esa persona para ir a arreglarlo.\r\n\r\n874\r\n00:52:49,830 --> 00:52:53,701\r\nEntonces, creo que quiero que todos,\r\n\r\n875\r\n00:52:53,801 --> 00:52:56,470\r\nCada organizaciÃ³n de nuestra empresa utilizarÃ¡ la IA de forma muy agresiva.\r\n\r\n876\r\n00:52:56,603 --> 00:53:00,674\r\nQuiero convertir a NVIDIA en una IA gigante. Â¿No serÃ­a genial?\r\n\r\n877\r\n00:53:01,241 --> 00:53:02,943\r\nY luego tendrÃ© equilibrio entre vida laboral y personal.\r\n\r\n878\r\n00:53:05,846 --> 00:53:07,948\r\nÂ¿Hay algÃºn ejemplo favorito del que hayas oÃ­do hablar?\r\n\r\n879\r\n00:53:08,048 --> 00:53:11,485\r\nempresas en quizÃ¡s algÃºn tipo de sector inesperado,\r\n\r\n880\r\n00:53:11,585 --> 00:53:14,788\r\nalgÃºn caso de uso inesperado en el que sientes que ellos\r\n\r\n881\r\n00:53:14,888 --> 00:53:18,192\r\nPuede servir como ejemplo de algunas de las dinÃ¡micas que estÃ¡s describiendo.\r\n\r\n882\r\n00:53:18,325 --> 00:53:22,763\r\nÂ¿DÃ³nde realmente se han dado cuenta de algunas de estas oportunidades?\r\n\r\n883\r\n00:53:23,797 --> 00:53:29,503\r\n-La mayor sorpresa de la IA que no deberÃ­a ser una sorpresa para mucha gente,\r\n\r\n884\r\n00:53:29,603 --> 00:53:35,109\r\nÂ¿Es que cuando decimos que es un modelo de lenguaje grande,\r\n\r\n885\r\n00:53:35,242 --> 00:53:38,679\r\nLa palabra lenguaje no significa sÃ³lo lenguaje humano,\r\n\r\n886\r\n00:53:38,812 --> 00:53:42,616\r\ny no significa sÃ³lo inglÃ©s, o sÃ³lo francÃ©s,\r\n\r\n887\r\n00:53:42,750 --> 00:53:46,820\r\no solo irlandÃ©s, o ese es un idioma completamente diferente. Pero...\r\n\r\n888\r\n00:53:48,255 --> 00:53:50,257\r\nÂ¿Existe un gran modelo lingÃ¼Ã­stico para el irlandÃ©s?\r\n\r\n889\r\n00:53:50,591 --> 00:53:53,193\r\n-Lo he probado. -Â¿Funciona?\r\n\r\n890\r\n00:53:53,293 --> 00:53:55,662\r\n-SÃ­, funciona bien.\r\n\r\n891\r\n00:53:55,763 --> 00:54:01,769\r\nJohn y yo pasamos la mayor parte de nuestra educaciÃ³n en Irlanda, recibiendo clases en irlandÃ©s.\r\n\r\n892\r\n00:54:02,069 --> 00:54:06,340\r\nY entonces, estos modelos son algunas de las primeras personas\r\n\r\n893\r\n00:54:06,440 --> 00:54:12,179\r\nTuve la oportunidad de dialogar con un gaÃ©lico [en irlandÃ©s].\r\n\r\n894\r\n00:54:12,312 --> 00:54:13,347\r\n-Muy sorprendente.\r\n\r\n895\r\n00:54:13,447 --> 00:54:14,782\r\n-En muchos aÃ±os. SÃ­.\r\n\r\n896\r\n00:54:14,882 --> 00:54:17,584\r\nLo hacen bien, y de hecho he estado... Â¿Has jugado con Suno?\r\n\r\n897\r\n00:54:17,684 --> 00:54:24,358\r\n-Suno? -Suno es una aplicaciÃ³n para crear mÃºsica,\r\n\r\n898\r\n00:54:24,558 --> 00:54:26,193\r\ntipo de mÃºsica sintÃ©tica. -EstÃ¡ bien.\r\n\r\n899\r\n00:54:26,293 --> 00:54:29,429\r\n-Y he estado disfrutando creando... -MÃºsica irlandesa.\r\n\r\n900\r\n00:54:29,563 --> 00:54:31,165\r\n-Por supuesto que lo probÃ©.\r\n\r\n901\r\n00:54:31,265 --> 00:54:34,201\r\nY el dubstep celta es algo que se puede hacer.\r\n\r\n902\r\n00:54:35,335 --> 00:54:37,237\r\n-FantÃ¡stico. Vale. Tiene sentido.\r\n\r\n903\r\n00:54:37,371 --> 00:54:39,239\r\nComo si pudiera hacer eso,\r\n\r\n904\r\n00:54:39,473 --> 00:54:42,442\r\nEntonces, por supuesto, podrÃ­a aprender el lenguaje de la vida.\r\n\r\n905\r\n00:54:43,477 --> 00:54:45,012\r\nPor supuesto que pueden aprender--\r\n\r\n906\r\n00:54:45,112 --> 00:54:48,415\r\nY si un modelo de lenguaje puede entender el sonido\r\n\r\n907\r\n00:54:48,515 --> 00:54:52,586\r\nque es una serie temporal secuencial, es una secuencia.\r\n\r\n908\r\n00:54:52,686 --> 00:54:57,057\r\nÂ¿Por quÃ© no puede aprender la articulaciÃ³n robÃ³tica, que es una secuencia?\r\n\r\n909\r\n00:54:57,191 --> 00:55:00,260\r\nSÃ³lo hay que descubrir cÃ³mo tokenizarlo.\r\n\r\n910\r\n00:55:00,360 --> 00:55:04,097\r\nY entonces surgiÃ³ la idea de que, de repente, \"Oh, oye, mira, escucha,\r\n\r\n911\r\n00:55:04,198 --> 00:55:08,001\r\nTambiÃ©n podrÃ­a aprender SQL, podrÃ­a aprender ABAP, puedo aprender Lightning,\r\n\r\n912\r\n00:55:08,101 --> 00:55:10,671\r\nPuedo aprender todos estos lenguajes propietarios, puedo aprender Verilog,\r\n\r\n913\r\n00:55:10,804 --> 00:55:14,842\r\nPuedo aprender, Â¿verdad? Entonces, de repente, te diste cuenta, espera un segundo,\r\n\r\n914\r\n00:55:14,942 --> 00:55:18,412\r\nPuedo poner un copiloto encima de cada herramienta del planeta.\r\n\r\n915\r\n00:55:18,512 --> 00:55:21,582\r\n-Y hasta este punto, NVIDIA es una gran IA,\r\n\r\n916\r\n00:55:21,682 --> 00:55:26,019\r\nÂ¿El futuro es uno de 100.000 modelos o de 100 millones de modelos?\r\n\r\n917\r\n00:55:26,119 --> 00:55:27,988\r\nÂ¿O el futuro serÃ¡ el de un solo modelo?\r\n\r\n918\r\n00:55:28,121 --> 00:55:29,990\r\nY son sÃ³lo un modelo que hace todas las cosas.\r\n\r\n919\r\n00:55:30,224 --> 00:55:37,631\r\n-Creo que serÃ­a genial tener sÃºper modelos.\r\n\r\n920\r\n00:55:37,731 --> 00:55:40,033\r\nque te ayudan a razonar sobre las cosas, en general.\r\n\r\n921\r\n00:55:40,133 --> 00:55:47,207\r\nPero, para nosotros, para todas las empresas que tienen necesidades muy especÃ­ficas,\r\n\r\n922\r\n00:55:47,307 --> 00:55:49,042\r\nexperiencia especÃ­fica del dominio,\r\n\r\n923\r\n00:55:49,176 --> 00:55:51,912\r\nVamos a tener que entrenar nuestros propios modelos.\r\n\r\n924\r\n00:55:52,045 --> 00:55:54,581\r\nY la razÃ³n de esto es porque tenemos un lenguaje propietario.\r\n\r\n925\r\n00:55:54,681 --> 00:55:58,552\r\nEsa diferencia entre el 99% y el 99,3%\r\n\r\n926\r\n00:55:58,652 --> 00:56:01,355\r\nEs la diferencia entre la vida y la muerte para nosotros.\r\n\r\n927\r\n00:56:01,455 --> 00:56:03,190\r\nY por eso es demasiado valioso para nosotros.\r\n\r\n928\r\n00:56:03,290 --> 00:56:06,393\r\nPara usted no es diferente a la detecciÃ³n de fraude: es demasiado importante para usted.\r\n\r\n929\r\n00:56:06,493 --> 00:56:09,463\r\n-Esa ha sido exactamente nuestra experiencia. -SÃ­. Es demasiado importante para ti.\r\n\r\n930\r\n00:56:09,563 --> 00:56:12,466\r\nPor muy bueno que sea el modelo general,\r\n\r\n931\r\n00:56:12,599 --> 00:56:15,235\r\nVas a querer tomar eso y afinarlo, Â¿verdad?\r\n\r\n932\r\n00:56:15,369 --> 00:56:19,373\r\nMejÃ³ralo hasta la perfecciÃ³n porque es demasiado importante para ti. SÃ­.\r\n\r\n933\r\n00:56:19,506 --> 00:56:22,142\r\n-Y pronto nos quedaremos sin tiempo.\r\n\r\n934\r\n00:56:22,242 --> 00:56:24,678\r\nY hay un montÃ³n de preguntas que aÃºn no he respondido.\r\n\r\n935\r\n00:56:24,778 --> 00:56:28,548\r\nHe ejercido poca disciplina en lo que respecta a la gestiÃ³n del tiempo.\r\n\r\n936\r\n00:56:28,682 --> 00:56:33,253\r\nAsÃ­ que hay un montÃ³n de cosas que creo que me dijeron que definitivamente tenÃ­a que preguntarte,\r\n\r\n937\r\n00:56:33,353 --> 00:56:36,723\r\npero hay una pareja a la que realmente querÃ­a preguntarle y somos los Ãºnicos que estamos aquÃ­ arriba.\r\n\r\n938\r\n00:56:36,857 --> 00:56:42,496\r\nEntonces, Â¿Lisa Sue es tu prima hermana?\r\n\r\n939\r\n00:56:42,596 --> 00:56:44,598\r\n-SÃ­. SÃ­. Es estupenda. Es increÃ­ble.\r\n\r\n940\r\n00:56:44,731 --> 00:56:48,302\r\n-Y AMD ahora es... -Ella es la CEO de AMD, por cierto.\r\n\r\n941\r\n00:56:48,402 --> 00:56:52,072\r\n-SÃ­, y AMD ahora es uno de sus competidores en el espacio de las GPU.\r\n\r\n942\r\n00:56:52,472 --> 00:56:53,507\r\n-No.\r\n\r\n943\r\n00:56:55,842 --> 00:56:57,844\r\nSomos familia.\r\n\r\n944\r\n00:56:57,945 --> 00:57:01,448\r\n-EstÃ¡ bien. -Todos estamos en la industria.\r\n\r\n945\r\n00:57:01,548 --> 00:57:02,683\r\n-Uno de sus socios en la industria.\r\n\r\n946\r\n00:57:02,783 --> 00:57:04,651\r\n-SÃ­, sÃ­. Compramos de AMD.\r\n\r\n947\r\n00:57:04,818 --> 00:57:06,353\r\n-Â¿QuÃ© estÃ¡ pasando en el agua?\r\n\r\n948\r\n00:57:06,486 --> 00:57:09,456\r\nÂ¿CÃ³mo terminamos con dos de ellos?\r\n\r\n949\r\n00:57:09,556 --> 00:57:14,227\r\nÂ¿PodrÃ­a decirse que las dos empresas de GPU mÃ¡s importantes estÃ¡n dirigidas por parientes cercanos?\r\n\r\n950\r\n00:57:14,328 --> 00:57:15,329\r\nÂ¿QuÃ© estÃ¡ sucediendo?\r\n\r\n951\r\n00:57:17,731 --> 00:57:20,534\r\n-Tienes que mantenerlo cerca de la familia.\r\n\r\n952\r\n00:57:20,634 --> 00:57:23,370\r\nNo, no tengo idea de cÃ³mo sucediÃ³.\r\n\r\n953\r\n00:57:23,470 --> 00:57:25,038\r\nNo crecimos juntos\r\n\r\n954\r\n00:57:25,138 --> 00:57:27,374\r\n-Y eso lo hace aÃºn mÃ¡s interesante, Â¿verdad?\r\n\r\n955\r\n00:57:27,507 --> 00:57:30,944\r\n-SÃ­, sÃ­. Ni siquiera nos conocÃ­amos hasta que ella estaba en IBM.\r\n\r\n956\r\n00:57:31,044 --> 00:57:35,716\r\nY su carrera es increÃ­ble. Y ella es realmente extraordinaria.\r\n\r\n957\r\n00:57:38,218 --> 00:57:40,620\r\n-Creo que esta cuestiÃ³n requiere mÃ¡s estudio.\r\n\r\n958\r\n00:57:40,721 --> 00:57:45,525\r\nAsÃ­ que usted ha estado operando en Silicon Valley desde principios de los aÃ±os 90.\r\n\r\n959\r\n00:57:45,659 --> 00:57:47,694\r\n-SÃ­.\r\n\r\n960\r\n00:57:47,794 --> 00:57:49,896\r\n-Â¿CÃ³mo ha cambiado la cultura de Silicon Valley en este tiempo?\r\n\r\n961\r\n00:57:52,132 --> 00:57:53,233\r\n-Oh, vaya.\r\n\r\n962\r\n00:57:55,369 --> 00:57:56,870\r\nHace mucho que no pienso en esto.\r\n\r\n963\r\n00:57:57,004 --> 00:58:03,677\r\nSupongo que, probablemente, de muchas maneras... Bueno, aquÃ­ hay una.\r\n\r\n964\r\n00:58:04,011 --> 00:58:11,184\r\nCuando comencÃ© con NVIDIA, tenÃ­a 29 aÃ±os y...\r\n\r\n965\r\n00:58:12,252 --> 00:58:19,826\r\nTenÃ­a 29 aÃ±os y tenÃ­a acnÃ© y,\r\n\r\n966\r\n00:58:19,926 --> 00:58:24,097\r\nVe a hablar con tu... ve a contratar bufetes de abogados.\r\n\r\n967\r\n00:58:24,197 --> 00:58:28,869\r\ny VCs y me saliÃ³ un grano grande en la frente\r\n\r\n968\r\n00:58:28,969 --> 00:58:33,140\r\ny no tengo uno hoy, asÃ­ que me siento cÃ³modo hablando de ello,\r\n\r\n969\r\n00:58:33,240 --> 00:58:34,941\r\npero podrÃ­a, podrÃ­a pasar.\r\n\r\n970\r\n00:58:35,075 --> 00:58:38,678\r\nY de todos modos, te sientes bastante inseguro.\r\n\r\n971\r\n00:58:38,812 --> 00:58:43,617\r\nPorque la mayorÃ­a de los directores ejecutivos de aquella Ã©poca vestÃ­an traje y eran muy competentes.\r\n\r\n972\r\n00:58:43,750 --> 00:58:47,120\r\nY suenan como adultos y usan palabras grandes.\r\n\r\n973\r\n00:58:47,220 --> 00:58:50,090\r\ny hablan de negocios y cosas asÃ­.\r\n\r\n974\r\n00:58:50,190 --> 00:58:53,560\r\nY, entonces, cuando eres joven, te sientes bastante intimidado.\r\n\r\n975\r\n00:58:53,660 --> 00:58:55,762\r\nEstÃ¡s rodeado de un grupo de adultos.\r\n\r\n976\r\n00:58:55,896 --> 00:59:00,333\r\nBueno, ahora si no tienes acnÃ©,\r\n\r\n977\r\n00:59:00,834 --> 00:59:02,903\r\nNo creo que merezcas empezar una empresa.\r\n\r\n978\r\n00:59:08,008 --> 00:59:11,411\r\nEsa es una gran diferencia. El acnÃ©.\r\n\r\n979\r\n00:59:11,511 --> 00:59:14,047\r\nLa moraleja del discurso de Jensen.\r\n\r\n980\r\n00:59:14,181 --> 00:59:20,287\r\nLo que realmente significa es que hemos permitido que los jÃ³venes sean extraordinarios.\r\n\r\n981\r\n00:59:20,387 --> 00:59:23,090\r\nCreo que la generaciÃ³n joven de directores ejecutivos,\r\n\r\n982\r\n00:59:23,423 --> 00:59:26,493\r\nEl tipo de cosas que ustedes saben a una edad tan temprana,\r\n\r\n983\r\n00:59:26,593 --> 00:59:27,627\r\nEs realmente bastante extraordinario.\r\n\r\n984\r\n00:59:27,727 --> 00:59:30,697\r\nQuiero decir que me llevÃ³ dÃ©cadas aprenderlo.\r\n\r\n985\r\n00:59:30,831 --> 00:59:32,365\r\n-Ãšltima pregunta.\r\n\r\n986\r\n00:59:33,233 --> 00:59:36,036\r\nâ€”Eso fue un cumplido. Mira quÃ© rÃ¡pido cambiÃ³...\r\n\r\n987\r\n00:59:37,838 --> 00:59:40,340\r\nNo decÃ­a que tuvieras acnÃ©. Solo decÃ­a que eras inteligente.\r\n\r\n988\r\n00:59:43,310 --> 00:59:46,513\r\n-NVIDIA tiene una capitalizaciÃ³n de mercado de aproximadamente 2 billones de dÃ³lares.\r\n\r\n989\r\n00:59:46,613 --> 00:59:53,653\r\nY ahora estÃ¡s a un paso de Apple y Microsoft.\r\n\r\n990\r\n00:59:53,753 --> 00:59:56,590\r\nAcabo de comprobarlo y lo tienen.\r\n\r\n991\r\n00:59:56,690 --> 01:00:02,863\r\n220.000 y 160.000 empleados, respectivamente.\r\n\r\n992\r\n01:00:02,963 --> 01:00:06,766\r\nNVIDIA tiene 28.000 empleados, por lo que,\r\n\r\n993\r\n01:00:06,900 --> 01:00:10,770\r\nMenos de una quinta parte del mÃ¡s pequeÃ±o de los dos que hay allÃ­.\r\n\r\n994\r\n01:00:11,404 --> 01:00:15,709\r\nY luego dijiste cuando estÃ¡bamos charlando detrÃ¡s del escenario,\r\n\r\n995\r\n01:00:15,809 --> 01:00:22,516\r\nY anotÃ© esto: \"Se puede lograr la excelencia operativa a travÃ©s del proceso,\r\n\r\n996\r\n01:00:22,649 --> 01:00:25,785\r\nPero el arte sÃ³lo se logra con permanencia.\"\r\n\r\n997\r\n01:00:26,553 --> 01:00:32,058\r\nY NVIDIA es considerablemente mÃ¡s pequeÃ±a que cualquiera de los otros gigantes.\r\n\r\n998\r\n01:00:32,192 --> 01:00:34,761\r\nY usted parece pensar que la permanencia en el cargo realmente importa.\r\n\r\n999\r\n01:00:34,861 --> 01:00:37,164\r\nY supongo que la artesanÃ­a realmente importa.\r\n\r\n1000\r\n01:00:37,264 --> 01:00:41,401\r\nÂ¿Quieres decir algo mÃ¡s?\r\n\r\n1001\r\n01:00:43,470 --> 01:00:48,341\r\n-Creo que se podrÃ­an hacer cosas extraordinarias... creo que se podrÃ­an hacer muchas cosas buenas...\r\n\r\n1002\r\n01:00:48,441 --> 01:00:52,612\r\nLas cosas buenas se hacen con excelencia operativa.\r\n\r\n1003\r\n01:00:52,712 --> 01:00:58,318\r\nPero no se pueden lograr cosas extraordinarias sÃ³lo mediante la excelencia operativa.\r\n\r\n1004\r\n01:00:58,451 --> 01:01:01,855\r\nY la razÃ³n de esto es porque muchas de las grandes cosas\r\n\r\n1005\r\n01:01:01,955 --> 01:01:05,659\r\nen su trabajo y los productos que elabora,\r\n\r\n1006\r\n01:01:05,759 --> 01:01:10,564\r\nla empresa que has creado, las organizaciones que has nutrido,\r\n\r\n1007\r\n01:01:10,830 --> 01:01:13,867\r\nSe necesita cuidado amoroso.\r\n\r\n1008\r\n01:01:13,967 --> 01:01:15,936\r\nNi siquiera puedes expresarlo con palabras.\r\n\r\n1009\r\n01:01:16,069 --> 01:01:18,972\r\nÂ¿CÃ³mo poner amor y cariÃ±o en un correo electrÃ³nico?\r\n\r\n1010\r\n01:01:19,105 --> 01:01:22,342\r\nY que la gente diga: \"Oh, sÃ© exactamente quÃ© hacer\".\r\n\r\n1011\r\n01:01:22,442 --> 01:01:27,480\r\nNo puedes poner eso en un proceso de negocios, amor y cuidado.\r\n\r\n1012\r\n01:01:28,481 --> 01:01:31,184\r\n-Â¿Es Â«amor y cuidadoÂ» un lema de NVIDIA?\r\n\r\n1013\r\n01:01:31,685 --> 01:01:37,991\r\n-Bueno, yo uso el Amor con bastante abundancia, y el Cuidado lo uso con bastante abundancia.\r\n\r\n1014\r\n01:01:38,124 --> 01:01:41,061\r\n-En Stripe hablamos mucho de artesanÃ­a y belleza.\r\n\r\n1015\r\n01:01:41,161 --> 01:01:43,430\r\n-SÃ­, claro. Tienes que usar estas palabras porque...\r\n\r\n1016\r\n01:01:43,563 --> 01:01:47,834\r\nEn muchos sentidos no hay otras palabras para describirlo.\r\n\r\n1017\r\n01:01:48,301 --> 01:01:52,239\r\nNo puedes ponerlos en nÃºmeros ni escribirlos en las especificaciones del producto.\r\n\r\n1018\r\n01:01:52,372 --> 01:01:53,707\r\nLa especificaciÃ³n del producto dice:\r\n\r\n1019\r\n01:01:53,840 --> 01:01:56,610\r\n\"Quiero que construyas algo grandioso que sea increÃ­blemente hermoso.\r\n\r\n1020\r\n01:01:56,710 --> 01:02:01,047\r\nEso es de gran, gran habilidad.\" No se pueden especificar estas cosas.\r\n\r\n1021\r\n01:02:01,147 --> 01:02:02,816\r\n-Pero estoy seguro de que hay gente en Stripe que piensa...\r\n\r\n1022\r\n01:02:02,916 --> 01:02:05,485\r\nPatrick siempre estÃ¡ hablando de artesanÃ­a y belleza.\r\n\r\n1023\r\n01:02:05,585 --> 01:02:07,921\r\n-Y es este tipo de-- -Nunca me quejo.\r\n\r\n1024\r\n01:02:08,021 --> 01:02:11,258\r\nSolo querÃ­a que lo supieras. Ni siquiera sÃ© cÃ³mo suena eso.\r\n\r\n1025\r\n01:02:11,491 --> 01:02:13,627\r\nâ€”Vale. Bueno, sÃ­. â€”Sigue parloteando. Adelante, adelante. SÃ­.\r\n\r\n1026\r\n01:02:13,760 --> 01:02:15,762\r\n-Eres mÃ¡s lÃºcido que yo.\r\n\r\n1027\r\n01:02:15,895 --> 01:02:17,430\r\nSÃ³lo balbuceo, pero, bueno,\r\n\r\n1028\r\n01:02:17,530 --> 01:02:20,166\r\n\"AsÃ­ que Patrick siempre estÃ¡ hablando de estas cosas de artesanÃ­a y belleza,\r\n\r\n1029\r\n01:02:20,267 --> 01:02:24,604\r\ny quiere que las cosas tengan ese carÃ¡cter particular e inefable,\r\n\r\n1030\r\n01:02:24,738 --> 01:02:27,907\r\npero no atiende directamente alguna necesidad del cliente, etcÃ©tera.\"\r\n\r\n1031\r\n01:02:28,041 --> 01:02:30,010\r\nComo si los clientes no vinieran a nosotros y nos dijeran:\r\n\r\n1032\r\n01:02:30,143 --> 01:02:31,678\r\n\"Quiero que el producto sea mÃ¡s bonito.\"\r\n\r\n1033\r\n01:02:31,778 --> 01:02:33,880\r\nDicen: \"Quiero que incluya X o Y\".\r\n\r\n1034\r\n01:02:34,114 --> 01:02:37,117\r\nY aÃºn asÃ­ creemos que la artesanÃ­a y la belleza realmente importan.\r\n\r\n1035\r\n01:02:37,250 --> 01:02:40,620\r\nParece que estÃ¡s llegando a algo similar. Â¿Por quÃ© crees que es importante?\r\n\r\n1036\r\n01:02:41,521 --> 01:02:45,125\r\n-En realidad tus clientes, aunque no lo digan,\r\n\r\n1037\r\n01:02:45,225 --> 01:02:50,430\r\nPuede que no tengan palabras para decirlo, pero cuando lo experimentan, lo saben.\r\n\r\n1038\r\n01:02:50,530 --> 01:02:54,167\r\nNo hay duda. Mira...\r\n\r\n1039\r\n01:02:55,001 --> 01:03:01,174\r\nEl trabajo de Stripe tiene belleza, tiene elegancia, tiene sencillez.\r\n\r\n1040\r\n01:03:01,341 --> 01:03:03,777\r\nLa sencillez no es simple, como ya sabÃ©is.\r\n\r\n1041\r\n01:03:03,877 --> 01:03:06,646\r\nSimplicidad y simple no son... lo mismo.\r\n\r\n1042\r\n01:03:06,746 --> 01:03:12,752\r\nY tiene elegancia y soluciona el problema, pero lo justo,\r\n\r\n1043\r\n01:03:12,852 --> 01:03:15,322\r\nTe pesa, pero no demasiado,\r\n\r\n1044\r\n01:03:15,422 --> 01:03:18,758\r\nY por eso es difÃ­cil encontrar ese equilibrio.\r\n\r\n1045\r\n01:03:18,858 --> 01:03:21,361\r\nY no puedes especificarlo, sÃ³lo vas sintiendo el camino.\r\n\r\n1046\r\n01:03:21,461 --> 01:03:24,664\r\nY cuando tienes un equipo que estÃ¡ contigo,\r\n\r\n1047\r\n01:03:24,764 --> 01:03:27,567\r\nque se siente como estÃ¡n juntos,\r\n\r\n1048\r\n01:03:27,667 --> 01:03:32,172\r\nDe muchas maneras hemos codificado, hemos codificado,\r\n\r\n1049\r\n01:03:32,672 --> 01:03:37,444\r\nLa magia de la empresa de una manera que ninguna palabra puede describir.\r\n\r\n1050\r\n01:03:37,544 --> 01:03:39,913\r\nY no quieres perder eso. No quieres perder eso.\r\n\r\n1051\r\n01:03:40,013 --> 01:03:43,283\r\nQuerrÃ¡s tomar eso y llevarlo al siguiente nivel la prÃ³xima vez.\r\n\r\n1052\r\n01:03:43,383 --> 01:03:45,552\r\nY por eso no quiero reiniciar.\r\n\r\n1053\r\n01:03:45,652 --> 01:03:50,123\r\nNo me gusta trabajar con gente nueva por esa razÃ³n.\r\n\r\n1054\r\n01:03:50,223 --> 01:03:54,160\r\nPorque he codificado, he encarnado, he depositado tanto dolor,\r\n\r\n1055\r\n01:03:54,361 --> 01:03:57,464\r\nsufrimiento, alegrÃ­a, conocimiento. Correcto.\r\n\r\n1056\r\n01:03:57,597 --> 01:03:59,065\r\nToda esa experiencia, experiencia de vida.\r\n\r\n1057\r\n01:03:59,165 --> 01:04:01,368\r\nLo has codificado en todas las personas con las que has trabajado.\r\n\r\n1058\r\n01:04:01,501 --> 01:04:04,170\r\nQuieres seguir adelante. Quieres llevarlo al siguiente nivel.\r\n\r\n1059\r\n01:04:04,304 --> 01:04:08,708\r\nY esa es realmente la razÃ³n por la que creo profundamente en la titularidad.\r\n\r\n1060\r\n01:04:09,109 --> 01:04:13,346\r\nY gracias a eso, los equipos pequeÃ±os pueden hacer grandes cosas.\r\n\r\n1061\r\n01:04:13,446 --> 01:04:16,516\r\nY NVIDIA es una especie de equipo pequeÃ±o con 28.000 personas,\r\n\r\n1062\r\n01:04:16,649 --> 01:04:20,854\r\nLa gente cree que rendimos mucho mÃ¡s de lo que podemos por ese motivo.\r\n\r\n1063\r\n01:04:20,987 --> 01:04:24,290\r\nY es increÃ­ble lo que habÃ©is hecho.\r\n\r\n1064\r\n01:04:24,391 --> 01:04:26,760\r\ny quÃ© increÃ­blemente pequeÃ±o eres,\r\n\r\n1065\r\n01:04:26,860 --> 01:04:31,531\r\n7.000 personas apoyando un billÃ³n de dÃ³lares en\r\n\r\n1066\r\n01:04:31,631 --> 01:04:36,269\r\necosistema, industria y economÃ­a\r\n\r\n1067\r\n01:04:36,369 --> 01:04:38,304\r\nY quiÃ©n sabe hasta dÃ³nde podÃ©is llegar.\r\n\r\n1068\r\n01:04:38,405 --> 01:04:39,606\r\nAsÃ­ que estoy muy orgulloso de ti.\r\n\r\n1069\r\n01:04:39,706 --> 01:04:41,207\r\n-Jensen. Gracias.","category":"Talks","date":1767413807536},
 {"id":"2wK06mCJWHo","title":"A Conversation with Jensen Huang","srtEn":"1\n00:00:02,610 --> 00:00:07,800\n[Music]\n\n2\n00:00:08,240 --> 00:00:12,320\nHi, I'm Millie with the Special\n\n3\n00:00:10,160 --> 00:00:14,080\nCompetitive Studies Project. In this\n\n4\n00:00:12,320 --> 00:00:16,560\nweek's episode of Memos to the\n\n5\n00:00:14,080 --> 00:00:19,439\nPresident, I've had the opportunity to\n\n6\n00:00:16,560 --> 00:00:22,080\nchat with Jensen Wong, founder and CEO\n\n7\n00:00:19,439 --> 00:00:24,160\nof Nvidia. We cover all things related\n\n8\n00:00:22,080 --> 00:00:28,519\nto AI. I hope you'll enjoy the\n\n9\n00:00:24,160 --> 00:00:28,519\nconversation as much as I did.\n\n10\n00:00:32,480 --> 00:00:36,000\nHi, welcome back to Memos to the\n\n11\n00:00:34,160 --> 00:00:38,640\nPresident. It's really an honor to sit\n\n12\n00:00:36,000 --> 00:00:41,280\ndown today with a special guest, Jensen\n\n13\n00:00:38,640 --> 00:00:43,200\nWong, founder and CEO of Nvidia. This\n\n14\n00:00:41,280 --> 00:00:45,280\nwas a big week for Nvidia. So, I'm going\n\n15\n00:00:43,200 --> 00:00:48,399\nto turn over to Jensen to talk about how\n\n16\n00:00:45,280 --> 00:00:50,320\ndoes it feel this week to be the founder\n\n17\n00:00:48,399 --> 00:00:51,120\nand CEO of Nvidia. Jensen, welcome to my\n\n18\n00:00:50,320 --> 00:00:52,640\npodcast.\n\n19\n00:00:51,120 --> 00:00:55,280\n>> Thank you, Elie. It's an incredible\n\n20\n00:00:52,640 --> 00:00:56,559\nhonor. Wow. Crazy week, right?\n\n21\n00:00:55,280 --> 00:00:58,320\n>> It's an incredible week for Nvidia.\n\n22\n00:00:56,559 --> 00:01:00,079\nYeah, it's crazy week. I think the um\n\n23\n00:00:58,320 --> 00:01:04,000\nwell, first of all, I it hasn't really\n\n24\n00:01:00,079 --> 00:01:06,560\nsunk in yet. Um but let's see what\n\n25\n00:01:04,000 --> 00:01:08,159\nthoughts comes to mind. Well, you know,\n\n26\n00:01:06,560 --> 00:01:11,280\nin a lot of ways, somebody asked me\n\n27\n00:01:08,159 --> 00:01:14,640\nyesterday about and and and mentioned to\n\n28\n00:01:11,280 --> 00:01:16,320\nme uh that I'm an immigrant, and in\n\n29\n00:01:14,640 --> 00:01:16,960\nfact, I am an immigrant. Talking about\n\n30\n00:01:16,320 --> 00:01:18,000\nimmigration,\n\n31\n00:01:16,960 --> 00:01:19,600\n>> same here.\n\n32\n00:01:18,000 --> 00:01:23,439\n>> And the two of us are both immigrants.\n\n33\n00:01:19,600 --> 00:01:26,000\nAnd and uh America stands for\n\n34\n00:01:23,439 --> 00:01:27,680\nthe uh the land of the American dream.\n\n35\n00:01:26,000 --> 00:01:30,080\nAnd this is the place where immigrants\n\n36\n00:01:27,680 --> 00:01:32,880\ncome to to build build a life and build\n\n37\n00:01:30,080 --> 00:01:34,720\na company. In a lot of ways, you know,\n\n38\n00:01:32,880 --> 00:01:36,159\nwhat I'm experiencing is probably the\n\n39\n00:01:34,720 --> 00:01:37,759\nultimate American dream.\n\n40\n00:01:36,159 --> 00:01:39,280\n>> Absolutely. you know, to come come to\n\n41\n00:01:37,759 --> 00:01:41,840\nUnited States when I was seven years\n\n42\n00:01:39,280 --> 00:01:43,439\nold, not I guess eight years old and and\n\n43\n00:01:41,840 --> 00:01:45,439\num\n\n44\n00:01:43,439 --> 00:01:48,240\nand and having the opportunity to found\n\n45\n00:01:45,439 --> 00:01:53,119\na company with good friends 33 years\n\n46\n00:01:48,240 --> 00:01:56,240\nago, be the uh the CEO today after 33\n\n47\n00:01:53,119 --> 00:01:58,240\nyears and um having many of the founders\n\n48\n00:01:56,240 --> 00:02:00,960\nthat are that were there with me at the\n\n49\n00:01:58,240 --> 00:02:02,960\nvery beginning still here at NVIDIA um\n\n50\n00:02:00,960 --> 00:02:05,280\npursuing a dream that we've had that\n\n51\n00:02:02,960 --> 00:02:06,640\nthat took three decades to accomplish\n\n52\n00:02:05,280 --> 00:02:09,520\n>> with a lot of ups and downs. with\n\n53\n00:02:06,640 --> 00:02:12,319\nenormous amount of ups and downs and you\n\n54\n00:02:09,520 --> 00:02:15,440\nknow so in a lot of ways uh this\n\n55\n00:02:12,319 --> 00:02:16,959\nmilestone what is happening to us it's\n\n56\n00:02:15,440 --> 00:02:20,000\nkind of hard to internalize it's kind of\n\n57\n00:02:16,959 --> 00:02:21,680\nhard to take in but it also represents\n\n58\n00:02:20,000 --> 00:02:24,080\nsomething that's really important you\n\n59\n00:02:21,680 --> 00:02:27,120\nknow we uh we wanted to reinvent the\n\n60\n00:02:24,080 --> 00:02:28,800\ncomputer the computer as we know it uh\n\n61\n00:02:27,120 --> 00:02:30,800\nreally has been largely defined for\n\n62\n00:02:28,800 --> 00:02:34,959\nabout six decades since the IBM system\n\n63\n00:02:30,800 --> 00:02:36,720\n360 IBM as you know was was uh the the\n\n64\n00:02:34,959 --> 00:02:40,239\nlargest company in the world of of their\n\n65\n00:02:36,720 --> 00:02:43,040\ntime and and the blueprint that they put\n\n66\n00:02:40,239 --> 00:02:44,480\ntogether for computing was basically\n\n67\n00:02:43,040 --> 00:02:47,360\nit's the same blueprint that has been\n\n68\n00:02:44,480 --> 00:02:49,360\nplayed out in the last six decades.\n\n69\n00:02:47,360 --> 00:02:51,599\nEverything from the architecture of the\n\n70\n00:02:49,360 --> 00:02:54,400\nsystems, the way the separation between\n\n71\n00:02:51,599 --> 00:02:57,280\nsoftware and hardware and architecture\n\n72\n00:02:54,400 --> 00:03:00,080\ncompatibility and you know application\n\n73\n00:02:57,280 --> 00:03:01,680\ncompatibility, full family lineup, you\n\n74\n00:03:00,080 --> 00:03:04,959\nknow, all of the things that they they\n\n75\n00:03:01,680 --> 00:03:07,120\ndescribed uh largely describes the the\n\n76\n00:03:04,959 --> 00:03:09,360\ncomputer industry today and and in order\n\n77\n00:03:07,120 --> 00:03:12,000\nand the opportunity to reinvent that and\n\n78\n00:03:09,360 --> 00:03:14,159\ntake it to the next level and now being\n\n79\n00:03:12,000 --> 00:03:16,319\nthe platform for artificial intelligence\n\n80\n00:03:14,159 --> 00:03:17,840\nis really a dream come true.\n\n81\n00:03:16,319 --> 00:03:20,800\nYeah, really extraordinary time.\n\n82\n00:03:17,840 --> 00:03:23,200\n>> You talk about the AI waves\n\n83\n00:03:20,800 --> 00:03:24,560\n>> and I and I really like how you divide\n\n84\n00:03:23,200 --> 00:03:24,959\nthem into different categories.\n\n85\n00:03:24,560 --> 00:03:27,840\n>> Yeah.\n\n86\n00:03:24,959 --> 00:03:31,360\n>> Can you describe where we're today in in\n\n87\n00:03:27,840 --> 00:03:35,280\nterms of the wave and how we got here?\n\n88\n00:03:31,360 --> 00:03:37,599\n>> Um 20 2012 uh uh we saw the same moment\n\n89\n00:03:35,280 --> 00:03:40,799\nas everybody else. We had the we had the\n\n90\n00:03:37,599 --> 00:03:42,319\nthe inside track in the sense that uh we\n\n91\n00:03:40,799 --> 00:03:44,640\nalways believed that CUDA was going to\n\n92\n00:03:42,319 --> 00:03:47,200\nenable a new class of applications and\n\n93\n00:03:44,640 --> 00:03:50,799\nwe were always looking out for it. And\n\n94\n00:03:47,200 --> 00:03:53,120\nso when when um Alexet came along um\n\n95\n00:03:50,799 --> 00:03:56,640\nbuilt on top of CUDA, our GPUs made it\n\n96\n00:03:53,120 --> 00:03:59,120\npossible uh to train AlexNet and for\n\n97\n00:03:56,640 --> 00:04:01,599\nAlex Net to achieve such extraordinary\n\n98\n00:03:59,120 --> 00:04:04,879\nresults in computer vision, achieve the\n\n99\n00:04:01,599 --> 00:04:07,120\nlevel of capability that um computer\n\n100\n00:04:04,879 --> 00:04:09,120\nscientists specializing in computer\n\n101\n00:04:07,120 --> 00:04:11,760\nvision could not achieve over four\n\n102\n00:04:09,120 --> 00:04:15,200\ndecades. You know, for three people to\n\n103\n00:04:11,760 --> 00:04:17,919\nto do something like that. um uh it's\n\n104\n00:04:15,200 --> 00:04:20,000\njust an extraordinary feat. And so we\n\n105\n00:04:17,919 --> 00:04:21,680\ntook the opportunity and we we looked at\n\n106\n00:04:20,000 --> 00:04:23,680\nwhat is it that we're looking at? You\n\n107\n00:04:21,680 --> 00:04:26,240\nknow, what what's going on here? Is this\n\n108\n00:04:23,680 --> 00:04:29,360\nis Alec Alex al Alexnet a breakthrough\n\n109\n00:04:26,240 --> 00:04:31,440\nin computer vision or is it a bigger\n\n110\n00:04:29,360 --> 00:04:34,080\nidea than that? And of course, as we\n\n111\n00:04:31,440 --> 00:04:36,080\nknow, computer vision is a pillar of\n\n112\n00:04:34,080 --> 00:04:37,680\nartificial intelligence. You know,\n\n113\n00:04:36,080 --> 00:04:39,280\nwithout computer vision, without speech\n\n114\n00:04:37,680 --> 00:04:41,840\nlanguage understanding, it's hard to\n\n115\n00:04:39,280 --> 00:04:43,520\nhave intelligence. And and so we we\n\n116\n00:04:41,840 --> 00:04:45,440\nrealized that this of course was a part\n\n117\n00:04:43,520 --> 00:04:46,880\nof artificial intelligence. But is it a\n\n118\n00:04:45,440 --> 00:04:49,840\nbigger idea than that? And we came to\n\n119\n00:04:46,880 --> 00:04:53,680\nthe conclusion that what AlexNet and\n\n120\n00:04:49,840 --> 00:04:56,880\ndeep learning showed is that it is now\n\n121\n00:04:53,680 --> 00:05:00,000\nfinally possible if we had enough data,\n\n122\n00:04:56,880 --> 00:05:01,600\nenough computing scale. And of course we\n\n123\n00:05:00,000 --> 00:05:03,520\nhave these deep learning models that are\n\n124\n00:05:01,600 --> 00:05:07,039\nquite scalable\n\n125\n00:05:03,520 --> 00:05:10,000\num that we might be able to apply\n\n126\n00:05:07,039 --> 00:05:12,240\ncomputers to solve problems uh that were\n\n127\n00:05:10,000 --> 00:05:14,720\nimpossible to describe using human\n\n128\n00:05:12,240 --> 00:05:17,280\nengineered feature and using principled\n\n129\n00:05:14,720 --> 00:05:19,360\nalgorithms. And so we we um we got\n\n130\n00:05:17,280 --> 00:05:21,039\nexcited from that perspective. We also\n\n131\n00:05:19,360 --> 00:05:23,840\ngot excited because\n\n132\n00:05:21,039 --> 00:05:26,400\n>> because when you when you reason through\n\n133\n00:05:23,840 --> 00:05:29,280\num deep learning and and the training of\n\n134\n00:05:26,400 --> 00:05:31,199\nAlexNet and where it could go, we\n\n135\n00:05:29,280 --> 00:05:32,400\nrealized that the entire computing\n\n136\n00:05:31,199 --> 00:05:34,160\nplatform is going to change,\n\n137\n00:05:32,400 --> 00:05:35,680\n>> right? processors are going to change,\n\n138\n00:05:34,160 --> 00:05:37,600\nthe internet connect's going to change,\n\n139\n00:05:35,680 --> 00:05:39,440\nthe networking is going to change, the\n\n140\n00:05:37,600 --> 00:05:41,759\nsoftware stack on top of it, how you\n\n141\n00:05:39,440 --> 00:05:44,080\ndevelop the software, the methodology of\n\n142\n00:05:41,759 --> 00:05:46,080\nsoftware inside companies and of course\n\n143\n00:05:44,080 --> 00:05:48,000\nthe many industries that we might be\n\n144\n00:05:46,080 --> 00:05:50,479\nable to create was going to completely\n\n145\n00:05:48,000 --> 00:05:53,440\nchange and so we went about doing that.\n\n146\n00:05:50,479 --> 00:05:54,800\nWe re reset our company essentially. Now\n\n147\n00:05:53,440 --> 00:05:58,240\nthe the waves that you were talking\n\n148\n00:05:54,800 --> 00:06:00,639\nabout after we did that um we dedicated\n\n149\n00:05:58,240 --> 00:06:02,639\nourselves to uh creating new libraries\n\n150\n00:06:00,639 --> 00:06:05,759\ncalled KDNN\n\n151\n00:06:02,639 --> 00:06:09,520\nuh creating creating uh AI frameworks uh\n\n152\n00:06:05,759 --> 00:06:12,639\ncalled Mega Core um Megatron core uh to\n\n153\n00:06:09,520 --> 00:06:15,680\nuh inventing MVLink and tensor cores and\n\n154\n00:06:12,639 --> 00:06:17,840\nthe different numerical formats and uh\n\n155\n00:06:15,680 --> 00:06:21,199\nwe led to uh the creation of a system we\n\n156\n00:06:17,840 --> 00:06:23,039\ncall DGX1 our first AI supercomput. I\n\n157\n00:06:21,199 --> 00:06:24,400\npersonally delivered it to a startup in\n\n158\n00:06:23,039 --> 00:06:27,919\nSan Francisco which turned out to have\n\n159\n00:06:24,400 --> 00:06:30,560\nbeen open AI and and so anyways that\n\n160\n00:06:27,919 --> 00:06:33,199\njourney uh could really be captured in\n\n161\n00:06:30,560 --> 00:06:36,160\nseveral ways. Since 2012, the first\n\n162\n00:06:33,199 --> 00:06:38,160\nthing that happened, AI took off. Uh,\n\n163\n00:06:36,160 --> 00:06:39,840\ndeep learning kept advancing. The amount\n\n164\n00:06:38,160 --> 00:06:42,479\nof data we had, the amount of compute we\n\n165\n00:06:39,840 --> 00:06:44,240\nhad kept growing, kept scaling. And it\n\n166\n00:06:42,479 --> 00:06:47,680\nled to the first wave, which is really\n\n167\n00:06:44,240 --> 00:06:49,440\ndescribed as, if I could, perception. We\n\n168\n00:06:47,680 --> 00:06:50,479\nwe solved perception. It became\n\n169\n00:06:49,440 --> 00:06:52,240\nsuperhuman.\n\n170\n00:06:50,479 --> 00:06:54,319\n>> Computer vision became superhuman.\n\n171\n00:06:52,240 --> 00:06:56,800\nLanguage understanding or speech\n\n172\n00:06:54,319 --> 00:06:58,960\nrecognition became superhuman. The\n\n173\n00:06:56,800 --> 00:07:00,560\nsecond the second phase is uh\n\n174\n00:06:58,960 --> 00:07:03,919\ngenerative.\n\n175\n00:07:00,560 --> 00:07:06,560\nuh we can now not only understand\n\n176\n00:07:03,919 --> 00:07:09,199\ninformation but we can translate and\n\n177\n00:07:06,560 --> 00:07:11,120\ngenerate information. So text to text,\n\n178\n00:07:09,199 --> 00:07:13,680\ntext to images,\n\n179\n00:07:11,120 --> 00:07:15,840\n>> images to text, text to video.\n\n180\n00:07:13,680 --> 00:07:18,400\n>> And so if you could do text to video,\n\n181\n00:07:15,840 --> 00:07:20,319\nyou know what else can't can you do? So\n\n182\n00:07:18,400 --> 00:07:22,880\nsecond phase was generative AI. This\n\n183\n00:07:20,319 --> 00:07:25,120\nthird third wave is the wave that we're\n\n184\n00:07:22,880 --> 00:07:28,319\nin today, you know, really deeply\n\n185\n00:07:25,120 --> 00:07:28,880\nsolidly into which is uh reasoning AI.\n\n186\n00:07:28,319 --> 00:07:32,080\nMhm.\n\n187\n00:07:28,880 --> 00:07:36,240\n>> This is where an AI could apply\n\n188\n00:07:32,080 --> 00:07:39,599\nprinciples and knowledge, maybe some\n\n189\n00:07:36,240 --> 00:07:43,919\ncommon sense and use techniques like\n\n190\n00:07:39,599 --> 00:07:47,039\nchain of thought, trees of thought um to\n\n191\n00:07:43,919 --> 00:07:49,759\nuh break down the problem into multiple\n\n192\n00:07:47,039 --> 00:07:52,319\nsteps, reason about how to solve the\n\n193\n00:07:49,759 --> 00:07:54,479\nproblem, the larger goal step by step.\n\n194\n00:07:52,319 --> 00:07:57,360\nIt might even do some research, read\n\n195\n00:07:54,479 --> 00:07:59,199\nsome documents, read an archive paper\n\n196\n00:07:57,360 --> 00:08:00,879\nbefore it answers the question.\n\n197\n00:07:59,199 --> 00:08:05,759\n>> And so the third wave that we're in\n\n198\n00:08:00,879 --> 00:08:08,479\ntoday, which is reasoning, is a very big\n\n199\n00:08:05,759 --> 00:08:10,560\npart of seeing the acceleration of AI\n\n200\n00:08:08,479 --> 00:08:12,400\nbecoming AI. That's this the fact that\n\n201\n00:08:10,560 --> 00:08:15,039\nwe're we're doing reasoning AI is the\n\n202\n00:08:12,400 --> 00:08:17,520\nreason why why um uh people are starting\n\n203\n00:08:15,039 --> 00:08:18,240\nto say, you know, we're near uh general\n\n204\n00:08:17,520 --> 00:08:20,879\nintelligence,\n\n205\n00:08:18,240 --> 00:08:24,160\n>> right? And then the third the the next\n\n206\n00:08:20,879 --> 00:08:26,960\nwave after after uh uh after reasoning\n\n207\n00:08:24,160 --> 00:08:28,479\nAI is physical AI. This is where AI\n\n208\n00:08:26,960 --> 00:08:30,800\nknows how to now interact with the\n\n209\n00:08:28,479 --> 00:08:34,479\nphysical world. Has physical world\n\n210\n00:08:30,800 --> 00:08:36,959\ncommon sense like um object permanence,\n\n211\n00:08:34,479 --> 00:08:39,200\nfriction, inertia,\n\n212\n00:08:36,959 --> 00:08:41,360\ncause and effect, and you know, all of\n\n213\n00:08:39,200 --> 00:08:43,599\nthese types of types of common sense\n\n214\n00:08:41,360 --> 00:08:44,399\nthat children have, you know, puppies\n\n215\n00:08:43,599 --> 00:08:46,080\nhave.\n\n216\n00:08:44,399 --> 00:08:47,839\n>> You know, now we're going to AI is going\n\n217\n00:08:46,080 --> 00:08:49,760\nto have those things. And as a result of\n\n218\n00:08:47,839 --> 00:08:52,000\nthat, the the collection of all these\n\n219\n00:08:49,760 --> 00:08:53,519\ncapabilities, we should be able to see,\n\n220\n00:08:52,000 --> 00:08:54,640\nyou know, the next wave, which is\n\n221\n00:08:53,519 --> 00:08:55,519\nprobably robotics,\n\n222\n00:08:54,640 --> 00:08:58,880\n>> right? Yeah.\n\n223\n00:08:55,519 --> 00:09:00,800\n>> Um, you know, you've built the digital\n\n224\n00:08:58,880 --> 00:09:04,720\ninfrastructure for the way we live in\n\n225\n00:09:00,800 --> 00:09:07,360\ntoday. Uh, you're also working and, um,\n\n226\n00:09:04,720 --> 00:09:09,440\nhave a vision for the AI factories. Can\n\n227\n00:09:07,360 --> 00:09:11,519\nyou unpack what does that mean? Yeah.\n\n228\n00:09:09,440 --> 00:09:14,399\n>> How does that transform today's data\n\n229\n00:09:11,519 --> 00:09:17,680\ncenter for the future? the semiconductor\n\n230\n00:09:14,399 --> 00:09:19,200\nindustry, TSMC, uh the the uh computer\n\n231\n00:09:17,680 --> 00:09:20,959\necosystem\n\n232\n00:09:19,200 --> 00:09:24,160\nuh that that then create these\n\n233\n00:09:20,959 --> 00:09:27,120\ncomputers, Nvidia today, we represent,\n\n234\n00:09:24,160 --> 00:09:29,279\nif you will, the digital ecosystem, the\n\n235\n00:09:27,120 --> 00:09:31,200\ndigital infrastructure of the world, the\n\n236\n00:09:29,279 --> 00:09:32,480\ncomputing infrastructure.\n\n237\n00:09:31,200 --> 00:09:34,560\n>> And on top of that computing\n\n238\n00:09:32,480 --> 00:09:37,279\ninfrastructure realized this thing\n\n239\n00:09:34,560 --> 00:09:40,880\ncalled artificial intelligence.\n\n240\n00:09:37,279 --> 00:09:43,760\nAnd what is interesting about about um\n\n241\n00:09:40,880 --> 00:09:46,720\nabout the uh the last industry is that\n\n242\n00:09:43,760 --> 00:09:49,200\nwe the the the digital infrastructure\n\n243\n00:09:46,720 --> 00:09:51,839\nthe computer industry uh enabled\n\n244\n00:09:49,200 --> 00:09:54,720\nsoftware and that represents you know\n\n245\n00:09:51,839 --> 00:09:56,959\nabout a trillion dollars of industry.\n\n246\n00:09:54,720 --> 00:09:59,440\nYou you u uh use the computer\n\n247\n00:09:56,959 --> 00:10:01,440\ninfrastructures to write the software\n\n248\n00:09:59,440 --> 00:10:04,800\nbut then you deploy the software into\n\n249\n00:10:01,440 --> 00:10:07,920\nthings like you know phones, smartphones\n\n250\n00:10:04,800 --> 00:10:11,040\nand and so the software industry was not\n\n251\n00:10:07,920 --> 00:10:12,880\nvery large you know it's a you know call\n\n252\n00:10:11,040 --> 00:10:15,200\nit a half trillion dollars right\n\n253\n00:10:12,880 --> 00:10:17,760\n>> and the hardware industry not very large\n\n254\n00:10:15,200 --> 00:10:19,920\ncall it a half trillion dollars and all\n\n255\n00:10:17,760 --> 00:10:22,640\nof a sudden this industry the computer\n\n256\n00:10:19,920 --> 00:10:24,880\nindustry enabled artificial intelligence\n\n257\n00:10:22,640 --> 00:10:26,959\nand what's really interesting is\n\n258\n00:10:24,880 --> 00:10:28,800\nartificial intelligence is both this\n\n259\n00:10:26,959 --> 00:10:30,880\nrevolutionary technology that we just\n\n260\n00:10:28,800 --> 00:10:33,279\ntalked about\n\n261\n00:10:30,880 --> 00:10:35,279\nand it's because of its perception and\n\n262\n00:10:33,279 --> 00:10:37,279\nreasoning capability. You can use it to\n\n263\n00:10:35,279 --> 00:10:39,120\nsolve problems in just about every\n\n264\n00:10:37,279 --> 00:10:40,959\nsingle industry because every industry\n\n265\n00:10:39,120 --> 00:10:43,600\nthat we know at the pill at the\n\n266\n00:10:40,959 --> 00:10:48,000\nfoundation of it is intelligence and now\n\n267\n00:10:43,600 --> 00:10:50,079\nwe can create intelligence at in\n\n268\n00:10:48,000 --> 00:10:52,399\nincredible scales and of course that's\n\n269\n00:10:50,079 --> 00:10:54,079\ngoing to revolutionize every industry.\n\n270\n00:10:52,399 --> 00:10:56,079\nUm\n\n271\n00:10:54,079 --> 00:10:58,640\nthat's the technology perspective. What\n\n272\n00:10:56,079 --> 00:11:01,279\nabout the industrial perspective? In\n\n273\n00:10:58,640 --> 00:11:02,640\norder to produce these AI uh what is\n\n274\n00:11:01,279 --> 00:11:05,279\nactually coming out of it? What's coming\n\n275\n00:11:02,640 --> 00:11:08,000\nout of these models is tokens and these\n\n276\n00:11:05,279 --> 00:11:11,279\ntokens are formulated into words and\n\n277\n00:11:08,000 --> 00:11:13,920\nnumbers and symbols and could be in the\n\n278\n00:11:11,279 --> 00:11:18,480\nfuture chemicals and proteins for drug\n\n279\n00:11:13,920 --> 00:11:22,399\ndiscovery. It could be actuator motions\n\n280\n00:11:18,480 --> 00:11:26,399\nto uh to um drive a self-driving car uh\n\n281\n00:11:22,399 --> 00:11:29,519\nor animate a robot. And so so these\n\n282\n00:11:26,399 --> 00:11:31,760\nthese tokens that are coming out are\n\n283\n00:11:29,519 --> 00:11:34,480\nreformulated\n\n284\n00:11:31,760 --> 00:11:36,720\nreconstituted into intelligence of\n\n285\n00:11:34,480 --> 00:11:38,640\ndifferent kinds.\n\n286\n00:11:36,720 --> 00:11:41,040\nBut what what it takes to generate these\n\n287\n00:11:38,640 --> 00:11:42,480\ntokens at the scale that we need to\n\n288\n00:11:41,040 --> 00:11:46,160\nsupport all these industries and\n\n289\n00:11:42,480 --> 00:11:48,640\neverybody using AI are these large data\n\n290\n00:11:46,160 --> 00:11:50,079\ncenters and I and I stopped calling it a\n\n291\n00:11:48,640 --> 00:11:51,360\ndata center because in fact it's not a\n\n292\n00:11:50,079 --> 00:11:53,040\ndata center. It's not about it's not\n\n293\n00:11:51,360 --> 00:11:55,760\nabout the classical data centers are\n\n294\n00:11:53,040 --> 00:11:57,839\nretrieving data. It's a new type of data\n\n295\n00:11:55,760 --> 00:12:00,000\ncenter and its job is singular to\n\n296\n00:11:57,839 --> 00:12:02,240\nproduce tokens and that's why I call it\n\n297\n00:12:00,000 --> 00:12:05,120\nan AI factory. Yeah.\n\n298\n00:12:02,240 --> 00:12:07,120\n>> And you used also token per meter uh uh\n\n299\n00:12:05,120 --> 00:12:08,240\non how these AI factories will be\n\n300\n00:12:07,120 --> 00:12:09,920\nutilized going forward.\n\n301\n00:12:08,240 --> 00:12:12,800\n>> Exactly. And and what's what's really\n\n302\n00:12:09,920 --> 00:12:16,880\ninteresting about that is is um the last\n\n303\n00:12:12,800 --> 00:12:19,200\nindustry of factories creating electrons\n\n304\n00:12:16,880 --> 00:12:19,519\nwas the power generation industry,\n\n305\n00:12:19,200 --> 00:12:21,839\n>> right?\n\n306\n00:12:19,519 --> 00:12:24,560\n>> It represented 30% of the world's\n\n307\n00:12:21,839 --> 00:12:29,519\neconomy at one at one time. what is\n\n308\n00:12:24,560 --> 00:12:32,880\nproduced out of it is uh monetized at\n\n309\n00:12:29,519 --> 00:12:35,040\nyou know kilowatt hours per dollar. Now\n\n310\n00:12:32,880 --> 00:12:35,760\nwe have these ideas called mega tokens\n\n311\n00:12:35,040 --> 00:12:36,240\nper dollar\n\n312\n00:12:35,760 --> 00:12:39,120\n>> right\n\n313\n00:12:36,240 --> 00:12:41,680\n>> and it comes out as electrons again you\n\n314\n00:12:39,120 --> 00:12:44,959\nknow and so in instead of pure electrons\n\n315\n00:12:41,680 --> 00:12:46,880\nnow it's value added electrons and we\n\n316\n00:12:44,959 --> 00:12:48,399\ncall them tokens and they're we're going\n\n317\n00:12:46,880 --> 00:12:51,040\nto create essentially a whole new\n\n318\n00:12:48,399 --> 00:12:52,399\nindustry and this industry needs energy\n\n319\n00:12:51,040 --> 00:12:54,720\nwhich is the reason why President\n\n320\n00:12:52,399 --> 00:12:57,839\nTrump's pro- energy and and energy\n\n321\n00:12:54,720 --> 00:13:00,800\ngrowth uh initiative is is so timely\n\n322\n00:12:57,839 --> 00:13:03,200\nbecause at the exact time when America\n\n323\n00:13:00,800 --> 00:13:07,920\nwants to be great at AI I and wants to\n\n324\n00:13:03,200 --> 00:13:09,920\nbe a world leader in the AI ecosystem.\n\n325\n00:13:07,920 --> 00:13:11,600\nWithout the energy that's necessary to\n\n326\n00:13:09,920 --> 00:13:13,760\ncreate these AI factories, we wouldn't\n\n327\n00:13:11,600 --> 00:13:16,480\nbe able to do that. So the the\n\n328\n00:13:13,760 --> 00:13:19,200\nconfluence of President Trump's vision\n\n329\n00:13:16,480 --> 00:13:22,240\nand his his his\n\n330\n00:13:19,200 --> 00:13:25,120\ndrive to enable energy growth in our\n\n331\n00:13:22,240 --> 00:13:27,600\nnation and the confluence of the the\n\n332\n00:13:25,120 --> 00:13:30,480\nreadiness of the technology and the\n\n333\n00:13:27,600 --> 00:13:32,320\nreadiness of the world for AI, it all\n\n334\n00:13:30,480 --> 00:13:34,399\nhappened at exactly the right time, you\n\n335\n00:13:32,320 --> 00:13:36,000\nknow, and so so this this is going to\n\n336\n00:13:34,399 --> 00:13:36,560\nit's going to enable a new industry in\n\n337\n00:13:36,000 --> 00:13:39,680\nfront of us.\n\n338\n00:13:36,560 --> 00:13:41,839\n>> Um just a a question on that enabling of\n\n339\n00:13:39,680 --> 00:13:44,079\nnew industries. A lot of people are\n\n340\n00:13:41,839 --> 00:13:46,079\nnervous about jobs. Yeah. Um how is this\n\n341\n00:13:44,079 --> 00:13:48,160\ngoing to impact the workforce? This is\n\n342\n00:13:46,079 --> 00:13:51,200\nnot the first time that you know a\n\n343\n00:13:48,160 --> 00:13:53,519\nmassive transformative technology you\n\n344\n00:13:51,200 --> 00:13:56,320\nknow comes to our lives and it creates\n\n345\n00:13:53,519 --> 00:13:59,040\nnew job but it also you know makes\n\n346\n00:13:56,320 --> 00:14:01,440\nimpact on people losing their jobs. How\n\n347\n00:13:59,040 --> 00:14:06,680\ndo you see this transformation happening\n\n348\n00:14:01,440 --> 00:14:06,680\nin the workforce? New technologies\n\n349\n00:14:06,720 --> 00:14:11,920\nand productivity\n\n350\n00:14:09,519 --> 00:14:13,839\ndrives the growth of industries and it\n\n351\n00:14:11,920 --> 00:14:17,279\ncreates jobs.\n\n352\n00:14:13,839 --> 00:14:19,680\nIn the case of electricity,\n\n353\n00:14:17,279 --> 00:14:21,760\nit's a new technology\n\n354\n00:14:19,680 --> 00:14:23,600\nand as I mentioned earlier, the energy\n\n355\n00:14:21,760 --> 00:14:26,480\nproduction\n\n356\n00:14:23,600 --> 00:14:29,760\nindustry represented 30% at one time of\n\n357\n00:14:26,480 --> 00:14:32,240\nthe world's economy. Not only did the\n\n358\n00:14:29,760 --> 00:14:33,920\nenergy production was a large industry\n\n359\n00:14:32,240 --> 00:14:35,519\nin itself creating these power\n\n360\n00:14:33,920 --> 00:14:38,399\ngeneration plants of all different types\n\n361\n00:14:35,519 --> 00:14:41,440\nall around the world, it also enabled\n\n362\n00:14:38,399 --> 00:14:45,279\nnew applications. Electricity enabled\n\n363\n00:14:41,440 --> 00:14:48,320\nlight bulbs, dishwashers, refrigerators,\n\n364\n00:14:45,279 --> 00:14:50,480\nright? Laundry machines, all kinds of\n\n365\n00:14:48,320 --> 00:14:52,240\nnew applications were created and all\n\n366\n00:14:50,480 --> 00:14:54,399\nthose applications created a new\n\n367\n00:14:52,240 --> 00:14:57,199\nindustry, created jobs.\n\n368\n00:14:54,399 --> 00:14:59,279\nNow the last industrial revolution which\n\n369\n00:14:57,199 --> 00:15:03,040\nUnited States is you know right in the\n\n370\n00:14:59,279 --> 00:15:06,079\nmiddle of was the information industrial\n\n371\n00:15:03,040 --> 00:15:08,639\nrevolution and that digital revolution\n\n372\n00:15:06,079 --> 00:15:12,000\nenabled productivity to grow in the last\n\n373\n00:15:08,639 --> 00:15:14,560\nthree decades or so productivity has\n\n374\n00:15:12,000 --> 00:15:17,360\ngrown about 80%.\n\n375\n00:15:14,560 --> 00:15:20,160\nAlong with it employment went up by 80%.\n\n376\n00:15:17,360 --> 00:15:23,360\nAnd so so when productivity goes up\n\n377\n00:15:20,160 --> 00:15:25,120\nemployment goes up. Now why is that?\n\n378\n00:15:23,360 --> 00:15:27,760\nIn fact, you could say that when\n\n379\n00:15:25,120 --> 00:15:30,079\nproductivity goes up, employment would\n\n380\n00:15:27,760 --> 00:15:33,199\ngo down because you could do more with\n\n381\n00:15:30,079 --> 00:15:36,000\nfewer people, right? But that's because\n\n382\n00:15:33,199 --> 00:15:37,839\nthat lacks imagination.\n\n383\n00:15:36,000 --> 00:15:40,160\nIf your company, let's just take it from\n\n384\n00:15:37,839 --> 00:15:43,120\na company's perspective. If a company\n\n385\n00:15:40,160 --> 00:15:46,399\nhas no new ideas and it's literally\n\n386\n00:15:43,120 --> 00:15:48,480\ndoing one thing and one thing only, when\n\n387\n00:15:46,399 --> 00:15:50,720\nour productivity goes up, we need fewer\n\n388\n00:15:48,480 --> 00:15:53,120\npeople to do it. But if you look at\n\n389\n00:15:50,720 --> 00:15:54,800\nNvidia, we have so many ideas. is we\n\n390\n00:15:53,120 --> 00:15:58,880\ndon't have enough time or people to go\n\n391\n00:15:54,800 --> 00:16:01,519\ndo it. The backlog of great ideas that\n\n392\n00:15:58,880 --> 00:16:03,519\nwe would love to go try\n\n393\n00:16:01,519 --> 00:16:05,839\nnew markets and new applications we like\n\n394\n00:16:03,519 --> 00:16:07,360\nto go create the backlog of that is\n\n395\n00:16:05,839 --> 00:16:09,839\nincredible.\n\n396\n00:16:07,360 --> 00:16:12,160\nNow, if I just had more people, more\n\n397\n00:16:09,839 --> 00:16:13,680\ntime, you know, considering how fast how\n\n398\n00:16:12,160 --> 00:16:15,360\nhow hard we're already working, I'm\n\n399\n00:16:13,680 --> 00:16:17,040\nalready working. If I just had more\n\n400\n00:16:15,360 --> 00:16:19,680\npeople, if I had more time, if I were\n\n401\n00:16:17,040 --> 00:16:21,040\nmore productive, we do more. Our company\n\n402\n00:16:19,680 --> 00:16:23,120\nwould be able to offer more things,\n\n403\n00:16:21,040 --> 00:16:25,600\nwould be be able to invent new ideas\n\n404\n00:16:23,120 --> 00:16:27,839\nthat created new industries. And so the\n\n405\n00:16:25,600 --> 00:16:31,519\nthe real the real theme is that are you\n\n406\n00:16:27,839 --> 00:16:34,320\na do you do you are you a hopeful person\n\n407\n00:16:31,519 --> 00:16:37,839\noptimistic person and you believe in in\n\n408\n00:16:34,320 --> 00:16:39,680\nidea creation or are you somebody who\n\n409\n00:16:37,839 --> 00:16:41,199\nbelieves you know there are no new ideas\n\n410\n00:16:39,680 --> 00:16:44,000\nleft\n\n411\n00:16:41,199 --> 00:16:46,320\nand quite frankly we're just working and\n\n412\n00:16:44,000 --> 00:16:48,959\nif we could just do that work more\n\n413\n00:16:46,320 --> 00:16:50,959\nproductively we'll be out of work right\n\n414\n00:16:48,959 --> 00:16:53,759\nI don't I think there's so much work to\n\n415\n00:16:50,959 --> 00:16:55,600\ndo there are so many ideas to pursue\n\n416\n00:16:53,759 --> 00:16:58,320\nthat if If I can do work more\n\n417\n00:16:55,600 --> 00:17:02,160\neffectively, I would simply do more. And\n\n418\n00:16:58,320 --> 00:17:06,640\nso I think that that that that\n\n419\n00:17:02,160 --> 00:17:07,600\noptimistic view is not naive. It's\n\n420\n00:17:06,640 --> 00:17:08,240\nactually history,\n\n421\n00:17:07,600 --> 00:17:10,720\n>> right?\n\n422\n00:17:08,240 --> 00:17:13,280\n>> History would suggest that humanity has\n\n423\n00:17:10,720 --> 00:17:15,839\na lot more ideas to go pursue. We have a\n\n424\n00:17:13,280 --> 00:17:17,839\nlot more challenges to go address. If we\n\n425\n00:17:15,839 --> 00:17:21,199\nshould just have more productivity, we\n\n426\n00:17:17,839 --> 00:17:23,360\ncan go get to it much faster. Well, we\n\n427\n00:17:21,199 --> 00:17:24,959\none one thing if I could just one thing\n\n428\n00:17:23,360 --> 00:17:27,520\nthat I would like to add\n\n429\n00:17:24,959 --> 00:17:29,360\n>> and this is this is um this is something\n\n430\n00:17:27,520 --> 00:17:30,799\nthat that you and I have spoken about\n\n431\n00:17:29,360 --> 00:17:36,000\nbefore.\n\n432\n00:17:30,799 --> 00:17:37,840\n>> Um it is vital that everyone engages AI\n\n433\n00:17:36,000 --> 00:17:40,000\nright away.\n\n434\n00:17:37,840 --> 00:17:42,080\nEvery adult,\n\n435\n00:17:40,000 --> 00:17:44,880\nevery working person, not working\n\n436\n00:17:42,080 --> 00:17:46,559\nperson, every child should address and\n\n437\n00:17:44,880 --> 00:17:49,120\nengage AI right away. And the reason for\n\n438\n00:17:46,559 --> 00:17:51,360\nthat is because AI is the greatest\n\n439\n00:17:49,120 --> 00:17:52,160\nequalization equalizing force.\n\n440\n00:17:51,360 --> 00:17:54,960\n>> That's a good point.\n\n441\n00:17:52,160 --> 00:17:58,080\n>> It is the first time in history that a\n\n442\n00:17:54,960 --> 00:18:01,840\ntechnology as incredible as artificial\n\n443\n00:17:58,080 --> 00:18:04,240\nintelligence is useful for someone who\n\n444\n00:18:01,840 --> 00:18:07,600\nknows how to program software, whether\n\n445\n00:18:04,240 --> 00:18:08,960\nyou program C++ or Python or you have no\n\n446\n00:18:07,600 --> 00:18:11,039\nidea how to use a computer.\n\n447\n00:18:08,960 --> 00:18:12,880\n>> Right? This is the very first time in\n\n448\n00:18:11,039 --> 00:18:14,720\nhistory that all of a sudden that\n\n449\n00:18:12,880 --> 00:18:16,880\ncomputer is easy to use. If you don't\n\n450\n00:18:14,720 --> 00:18:19,360\nknow how to use AI, just open up the\n\n451\n00:18:16,880 --> 00:18:20,160\nwebsite, go to Chad GPT, go to Gemini\n\n452\n00:18:19,360 --> 00:18:20,960\nPro, just say\n\n453\n00:18:20,160 --> 00:18:21,760\n>> ask a simple question.\n\n454\n00:18:20,960 --> 00:18:24,000\n>> Yeah. Yeah.\n\n455\n00:18:21,760 --> 00:18:26,080\n>> And you could even say, \"I have no idea\n\n456\n00:18:24,000 --> 00:18:26,640\nhow to use AI. Can you teach me how to\n\n457\n00:18:26,080 --> 00:18:27,520\nuse AI?\"\n\n458\n00:18:26,640 --> 00:18:29,440\n>> That's true.\n\n459\n00:18:27,520 --> 00:18:31,440\n>> And if you don't know how to type, hit\n\n460\n00:18:29,440 --> 00:18:32,160\nthe microphone button and speak to us.\n\n461\n00:18:31,440 --> 00:18:34,080\n>> Gordon. Yeah.\n\n462\n00:18:32,160 --> 00:18:37,200\n>> And if you don't understand English, you\n\n463\n00:18:34,080 --> 00:18:39,120\ncan speak whatever language you like.\n\n464\n00:18:37,200 --> 00:18:42,160\n>> It is an extraordinary thing.\n\n465\n00:18:39,120 --> 00:18:44,160\n>> It is an extraordinary thing. And I also\n\n466\n00:18:42,160 --> 00:18:45,440\nthink it's incredible that if the AI\n\n467\n00:18:44,160 --> 00:18:48,720\ndoesn't know that language, you tell the\n\n468\n00:18:45,440 --> 00:18:50,640\nAI go learn that language, right?\n\n469\n00:18:48,720 --> 00:18:52,640\nAnd so so I think I think everybody\n\n470\n00:18:50,640 --> 00:18:55,039\nneeds to to engage AI. It is the\n\n471\n00:18:52,640 --> 00:18:57,280\ngreatest equalization\n\n472\n00:18:55,039 --> 00:18:59,600\num uh equalization force that we have\n\n473\n00:18:57,280 --> 00:19:01,039\never known and it's going to empower\n\n474\n00:18:59,600 --> 00:19:02,720\nit's going to enable it's going to lift\n\n475\n00:19:01,039 --> 00:19:04,640\nsociety of all you know everywhere.\n\n476\n00:19:02,720 --> 00:19:06,480\n>> I agree with you. Switching now to\n\n477\n00:19:04,640 --> 00:19:08,000\nWashington. We're you're in Washington\n\n478\n00:19:06,480 --> 00:19:09,280\nas I said you had an incredible week.\n\n479\n00:19:08,000 --> 00:19:09,840\nYou also met with the president\n\n480\n00:19:09,280 --> 00:19:11,039\nyesterday.\n\n481\n00:19:09,840 --> 00:19:13,760\n>> Yes. first question.\n\n482\n00:19:11,039 --> 00:19:16,080\n>> So, it's incredible. Pro- innovation,\n\n483\n00:19:13,760 --> 00:19:17,919\npro growth,\n\n484\n00:19:16,080 --> 00:19:19,520\npro- energy,\n\n485\n00:19:17,919 --> 00:19:25,039\npro\n\n486\n00:19:19,520 --> 00:19:28,559\nuh industry, wants us to take AI by the\n\n487\n00:19:25,039 --> 00:19:31,600\nhorns and be the world leader, continue\n\n488\n00:19:28,559 --> 00:19:33,440\nto be the world leader. Uh so proud uh\n\n489\n00:19:31,600 --> 00:19:35,520\nof our country, so proud of our\n\n490\n00:19:33,440 --> 00:19:38,080\ncompanies, so proud of our people. Uh\n\n491\n00:19:35,520 --> 00:19:39,760\njust it's always Yeah. I every time I\n\n492\n00:19:38,080 --> 00:19:41,120\nmeet him, every time I'm I'm with him, I\n\n493\n00:19:39,760 --> 00:19:41,520\ncome back, you know, completely fired\n\n494\n00:19:41,120 --> 00:19:44,960\nup.\n\n495\n00:19:41,520 --> 00:19:47,679\n>> Uh so I have three questions for you for\n\n496\n00:19:44,960 --> 00:19:49,919\nthree different audiences. Number one is\n\n497\n00:19:47,679 --> 00:19:51,440\n>> you obviously met the president\n\n498\n00:19:49,919 --> 00:19:54,480\n>> and this is Memos to the President\n\n499\n00:19:51,440 --> 00:19:56,720\npodcast. What is your advice to him\n\n500\n00:19:54,480 --> 00:19:59,600\nabout what are the things we have to do\n\n501\n00:19:56,720 --> 00:20:01,679\nnow to stay ahead?\n\n502\n00:19:59,600 --> 00:20:04,480\nHe wants America.\n\n503\n00:20:01,679 --> 00:20:06,880\nWell, first of all, he recognizes that\n\n504\n00:20:04,480 --> 00:20:08,960\nthe computer industry\n\n505\n00:20:06,880 --> 00:20:12,640\nand one that I have the great honor to\n\n506\n00:20:08,960 --> 00:20:16,640\nbe part of. The computer industry is\n\n507\n00:20:12,640 --> 00:20:19,039\nAmerica's national treasure.\n\n508\n00:20:16,640 --> 00:20:20,640\nIn no other industry do we lead the\n\n509\n00:20:19,039 --> 00:20:23,520\nworld\n\n510\n00:20:20,640 --> 00:20:27,520\nto the level and scale of the computer\n\n511\n00:20:23,520 --> 00:20:30,320\nindustry. You can't find another one.\n\n512\n00:20:27,520 --> 00:20:32,159\nWe lost the telecommunications industry.\n\n513\n00:20:30,320 --> 00:20:34,720\nThere is no way we're going to lose the\n\n514\n00:20:32,159 --> 00:20:35,360\nAmerican computing industry and this\n\n515\n00:20:34,720 --> 00:20:37,039\ncomputer industry.\n\n516\n00:20:35,360 --> 00:20:39,360\n>> Talk about 5G because I think we have\n\n517\n00:20:37,039 --> 00:20:40,000\nhad this conversation. We lost the 5G\n\n518\n00:20:39,360 --> 00:20:42,400\nwave.\n\n519\n00:20:40,000 --> 00:20:44,240\n>> We lost the 5G wave. We lost it through\n\n520\n00:20:42,400 --> 00:20:46,000\ntechnology. We lost it through policy.\n\n521\n00:20:44,240 --> 00:20:46,799\nWe lost it through bad strategic\n\n522\n00:20:46,000 --> 00:20:49,280\nthinking.\n\n523\n00:20:46,799 --> 00:20:51,120\n>> Um it is incredible what happened and we\n\n524\n00:20:49,280 --> 00:20:52,799\nsimply cannot allow that to happen.\n\n525\n00:20:51,120 --> 00:20:53,840\n>> And we're still struggling to regain\n\n526\n00:20:52,799 --> 00:20:55,600\nthat territory.\n\n527\n00:20:53,840 --> 00:20:57,280\n>> It's going to be it's going to be rough,\n\n528\n00:20:55,600 --> 00:20:58,960\n>> right? It's going to be super rough. We\n\n529\n00:20:57,280 --> 00:21:01,679\nhave an opportunity with 6G, right?\n\n530\n00:20:58,960 --> 00:21:04,559\n>> Because 6G. That's right. Because of AI\n\n531\n00:21:01,679 --> 00:21:06,799\nalso. And so we are going to go do our\n\n532\n00:21:04,559 --> 00:21:07,919\nbest to help our country regain\n\n533\n00:21:06,799 --> 00:21:10,880\ntechnology leadership and\n\n534\n00:21:07,919 --> 00:21:13,360\ntelecommunications. But back on AI, he\n\n535\n00:21:10,880 --> 00:21:15,520\nwants he wants America to uh be the\n\n536\n00:21:13,360 --> 00:21:17,840\nworld's best. Of course, he wants her\n\n537\n00:21:15,520 --> 00:21:21,120\nwould to continue to lead the world. In\n\n538\n00:21:17,840 --> 00:21:23,600\norder to lead the world in AI because AI\n\n539\n00:21:21,120 --> 00:21:25,039\nis fundamentally about computing and\n\n540\n00:21:23,600 --> 00:21:26,720\ncomputing is fundamentally about\n\n541\n00:21:25,039 --> 00:21:29,440\ndevelopers.\n\n542\n00:21:26,720 --> 00:21:33,039\nThe first job of leadership of a\n\n543\n00:21:29,440 --> 00:21:36,000\ncomputing platform which AI is also is\n\n544\n00:21:33,039 --> 00:21:38,640\nto win all developers.\n\n545\n00:21:36,000 --> 00:21:41,360\nThe first job of any platform is to win\n\n546\n00:21:38,640 --> 00:21:43,600\nall developers. Later when we talk about\n\n547\n00:21:41,360 --> 00:21:46,320\n5G, I can show you exactly the same\n\n548\n00:21:43,600 --> 00:21:48,880\nthing. We had a policy that caused us to\n\n549\n00:21:46,320 --> 00:21:51,120\nlose all developers. We need to have a\n\n550\n00:21:48,880 --> 00:21:53,919\npolicy that enables us to win all\n\n551\n00:21:51,120 --> 00:21:56,799\ndevelopers. 50% of the world's AI\n\n552\n00:21:53,919 --> 00:21:59,120\ndevelopers are in China.\n\n553\n00:21:56,799 --> 00:22:01,520\nAI developers are all over the world.\n\n554\n00:21:59,120 --> 00:22:04,880\nTheir AI developers now come out growing\n\n555\n00:22:01,520 --> 00:22:07,440\nup in Africa, in Latin America, in\n\n556\n00:22:04,880 --> 00:22:09,280\nSoutheast Asia, in the Middle East. AI\n\n557\n00:22:07,440 --> 00:22:11,360\ndevelopers are everywhere. The reason\n\n558\n00:22:09,280 --> 00:22:14,559\nwhy AI developers are everywhere is\n\n559\n00:22:11,360 --> 00:22:17,120\nbecause every country, every industry,\n\n560\n00:22:14,559 --> 00:22:18,799\nevery company needs to have intelligence\n\n561\n00:22:17,120 --> 00:22:21,520\nand wants to engage artificial\n\n562\n00:22:18,799 --> 00:22:23,679\nintelligence. But it starts with 50% are\n\n563\n00:22:21,520 --> 00:22:25,360\nin China and we need to win those\n\n564\n00:22:23,679 --> 00:22:28,880\ndevelopers. And so I think the first\n\n565\n00:22:25,360 --> 00:22:31,280\nthing he that that I would I would um\n\n566\n00:22:28,880 --> 00:22:33,120\ncontinue to say every time I I can\n\n567\n00:22:31,280 --> 00:22:36,559\nbecause the technology is not easy to\n\n568\n00:22:33,120 --> 00:22:39,520\nunderstand is if we want America to lead\n\n569\n00:22:36,559 --> 00:22:42,240\nthe AI revolution and continue to be the\n\n570\n00:22:39,520 --> 00:22:45,360\nworld leader the first thing we need is\n\n571\n00:22:42,240 --> 00:22:47,919\nevery AI developer to build on American\n\n572\n00:22:45,360 --> 00:22:48,480\ntech stack. The second thing I would say\n\n573\n00:22:47,919 --> 00:22:50,960\nis that\n\n574\n00:22:48,480 --> 00:22:53,280\n>> I think you've also said\n\n575\n00:22:50,960 --> 00:22:54,559\num you want to set the global standard\n\n576\n00:22:53,280 --> 00:22:55,919\nfor the technology stack.\n\n577\n00:22:54,559 --> 00:22:58,480\n>> That's right. The American company\n\n578\n00:22:55,919 --> 00:23:00,159\nshould be those that set the standard.\n\n579\n00:22:58,480 --> 00:23:01,200\n>> The American tech stack should be the\n\n580\n00:23:00,159 --> 00:23:03,120\nglobal standard.\n\n581\n00:23:01,200 --> 00:23:05,600\n>> Just as the American dollar is the\n\n582\n00:23:03,120 --> 00:23:08,880\nglobal standard by which every country\n\n583\n00:23:05,600 --> 00:23:11,520\nbuilds on, we should want the American\n\n584\n00:23:08,880 --> 00:23:15,919\ntech stack to be the tech stack that the\n\n585\n00:23:11,520 --> 00:23:17,840\nAI stack that everyone builds on. Now\n\n586\n00:23:15,919 --> 00:23:20,720\nthe tech stack starts with chips and\n\n587\n00:23:17,840 --> 00:23:22,960\nsystems. It is not just the AI models on\n\n588\n00:23:20,720 --> 00:23:25,360\ntop. There are many AI models on top.\n\n589\n00:23:22,960 --> 00:23:26,640\nThere's incredible models of all kinds.\n\n590\n00:23:25,360 --> 00:23:29,440\nSome of them are open source, some of\n\n591\n00:23:26,640 --> 00:23:32,080\nthem are closed, some of for physics,\n\n592\n00:23:29,440 --> 00:23:34,799\nsome of them are for uh quantum, some of\n\n593\n00:23:32,080 --> 00:23:37,520\nthem are for communications. The AI\n\n594\n00:23:34,799 --> 00:23:39,600\nmodels are of all different types. The\n\n595\n00:23:37,520 --> 00:23:42,320\nthings that you do uh your initiative\n\n596\n00:23:39,600 --> 00:23:44,000\ncalled AI plus I just deeply love. AI\n\n597\n00:23:42,320 --> 00:23:46,880\nfor science, that model is obviously\n\n598\n00:23:44,000 --> 00:23:50,159\ndifferent than a chatbot. AI for quantum\n\n599\n00:23:46,880 --> 00:23:52,159\nobviously different. AI for 5G and 6G,\n\n600\n00:23:50,159 --> 00:23:53,919\nobviously different, right? And so AI\n\n601\n00:23:52,159 --> 00:23:57,360\nfor robotics, obviously different. And\n\n602\n00:23:53,919 --> 00:24:00,480\nso all these different models are all AI\n\n603\n00:23:57,360 --> 00:24:03,120\nmodels and they should all be built\n\n604\n00:24:00,480 --> 00:24:04,960\n>> on the American tech stack. And so the\n\n605\n00:24:03,120 --> 00:24:10,320\nsecond thing that I would advocate is\n\n606\n00:24:04,960 --> 00:24:13,600\nthat AI diffusion should not be to limit\n\n607\n00:24:10,320 --> 00:24:16,480\nAmerican tech stack to the world. AI\n\n608\n00:24:13,600 --> 00:24:19,039\ndiffusion should be about maximizing the\n\n609\n00:24:16,480 --> 00:24:21,520\nAmerican tech stack all over the world\n\n610\n00:24:19,039 --> 00:24:24,480\nso that every AI developer in the world\n\n611\n00:24:21,520 --> 00:24:26,880\nbuilds on the American standard. And as\n\n612\n00:24:24,480 --> 00:24:30,240\nwe know about the computing ecosystem,\n\n613\n00:24:26,880 --> 00:24:33,600\nthe virtual cycle is incredible. the\n\n614\n00:24:30,240 --> 00:24:35,760\nmore your technology is everywhere, the\n\n615\n00:24:33,600 --> 00:24:37,440\nmore developers you're gonna have, the\n\n616\n00:24:35,760 --> 00:24:38,880\nmore developers you're going to have,\n\n617\n00:24:37,440 --> 00:24:40,400\nthe more your technology is going to be\n\n618\n00:24:38,880 --> 00:24:41,679\neverywhere. And so this positive\n\n619\n00:24:40,400 --> 00:24:43,279\nfeedback system is\n\n620\n00:24:41,679 --> 00:24:45,840\n>> I will just piggyback on this topic\n\n621\n00:24:43,279 --> 00:24:46,320\nbecause you talk a lot about sovereign\n\n622\n00:24:45,840 --> 00:24:47,120\nAI.\n\n623\n00:24:46,320 --> 00:24:49,840\n>> Yeah.\n\n624\n00:24:47,120 --> 00:24:52,159\n>> What do you mean by that? How does a\n\n625\n00:24:49,840 --> 00:24:54,640\ncountry build a sovereign AI? Why does\n\n626\n00:24:52,159 --> 00:24:56,720\nit need it for? You know, obviously the\n\n627\n00:24:54,640 --> 00:24:58,240\nway Europeans will build sovereign AI is\n\n628\n00:24:56,720 --> 00:25:00,159\ngoing to be different than how African\n\n629\n00:24:58,240 --> 00:25:02,240\ncountries are big. But you're traveling\n\n630\n00:25:00,159 --> 00:25:04,159\naround the world and advocating for\n\n631\n00:25:02,240 --> 00:25:05,120\nsovereign AI. So can you unpack your\n\n632\n00:25:04,159 --> 00:25:07,200\nvision about this?\n\n633\n00:25:05,120 --> 00:25:09,919\n>> I'm advocating for the American tech\n\n634\n00:25:07,200 --> 00:25:11,919\nstack to be the tech stack that every\n\n635\n00:25:09,919 --> 00:25:14,080\ncountry builds on. That's what I'm\n\n636\n00:25:11,919 --> 00:25:15,520\nadvocating for. And the reason the\n\n637\n00:25:14,080 --> 00:25:17,840\nreason why every country needs to build\n\n638\n00:25:15,520 --> 00:25:19,279\ntheir own tech stack, their own AI, is\n\n639\n00:25:17,840 --> 00:25:20,960\nbecause even though they could use\n\n640\n00:25:19,279 --> 00:25:23,919\nAmerican AIS, there's no question. And\n\n641\n00:25:20,960 --> 00:25:26,159\nthey should um every country should use\n\n642\n00:25:23,919 --> 00:25:29,520\nOpen AI. Every c country should use\n\n643\n00:25:26,159 --> 00:25:31,440\nGemini. um Google's Gemini and every\n\n644\n00:25:29,520 --> 00:25:34,240\ncountry should use Grock, you know, and\n\n645\n00:25:31,440 --> 00:25:36,640\nand so these are incredible models and\n\n646\n00:25:34,240 --> 00:25:38,799\nso every country should use them, but\n\n647\n00:25:36,640 --> 00:25:42,080\nthey should also build their own\n\n648\n00:25:38,799 --> 00:25:44,960\nindigenous AI stack and their AI AI\n\n649\n00:25:42,080 --> 00:25:47,600\nmodels and that AI model is trained on\n\n650\n00:25:44,960 --> 00:25:49,520\ntheir language, their history, their the\n\n651\n00:25:47,600 --> 00:25:52,480\nknowledge of their society, their\n\n652\n00:25:49,520 --> 00:25:55,279\nculture, their values. It's not sensible\n\n653\n00:25:52,480 --> 00:25:58,240\nthat one western company will be able to\n\n654\n00:25:55,279 --> 00:26:00,559\ncapture and somehow appre deeply\n\n655\n00:25:58,240 --> 00:26:02,640\nappreciate the values of every country\n\n656\n00:26:00,559 --> 00:26:04,799\nand every religion and every background\n\n657\n00:26:02,640 --> 00:26:07,039\nand every you know society around the\n\n658\n00:26:04,799 --> 00:26:09,120\nworld. And so each one of them should be\n\n659\n00:26:07,039 --> 00:26:11,919\nable to build something on their own.\n\n660\n00:26:09,120 --> 00:26:14,559\nAnd that AI model will work with other\n\n661\n00:26:11,919 --> 00:26:18,240\nindustrial AI open AI models or maybe\n\n662\n00:26:14,559 --> 00:26:20,240\neven private um uh corporate models or\n\n663\n00:26:18,240 --> 00:26:22,640\nyou know specific\n\n664\n00:26:20,240 --> 00:26:24,799\nindustrial science models or whatever it\n\n665\n00:26:22,640 --> 00:26:26,159\nis. But all of these models are going to\n\n666\n00:26:24,799 --> 00:26:28,480\ninteract. They should be able to build\n\n667\n00:26:26,159 --> 00:26:29,919\ntheir own and but still we want them to\n\n668\n00:26:28,480 --> 00:26:32,080\nbuild on American tech stack.\n\n669\n00:26:29,919 --> 00:26:34,480\n>> Yeah. You've been pretty vocal about the\n\n670\n00:26:32,080 --> 00:26:36,640\nUS China tech competition. So I want to\n\n671\n00:26:34,480 --> 00:26:38,080\nget your views about how you view the\n\n672\n00:26:36,640 --> 00:26:40,000\ncompetition. you call them a peer\n\n673\n00:26:38,080 --> 00:26:41,919\ncompetitor, not a nearper but a peer\n\n674\n00:26:40,000 --> 00:26:43,760\ncompetitor that have serious products,\n\n675\n00:26:41,919 --> 00:26:47,200\nserious companies. Where do you think\n\n676\n00:26:43,760 --> 00:26:52,559\nthe competition stands now?\n\n677\n00:26:47,200 --> 00:26:55,840\nUm f first of all uh China is our\n\n678\n00:26:52,559 --> 00:26:57,600\ncompetitor and adversary not our enemy.\n\n679\n00:26:55,840 --> 00:27:00,559\nAnd the reason for that is because we\n\n680\n00:26:57,600 --> 00:27:02,159\nhave we have deep interconnections and\n\n681\n00:27:00,559 --> 00:27:04,960\ninterdependencies between the two\n\n682\n00:27:02,159 --> 00:27:08,240\ncountries.\n\n683\n00:27:04,960 --> 00:27:10,400\nAmerica is incredible. Our\n\n684\n00:27:08,240 --> 00:27:12,720\ntechnology leadership\n\n685\n00:27:10,400 --> 00:27:14,400\nis extraordinary.\n\n686\n00:27:12,720 --> 00:27:17,440\nThe computer industry by which I have\n\n687\n00:27:14,400 --> 00:27:20,400\nthe honor to serve\n\n688\n00:27:17,440 --> 00:27:24,000\nis the most talented, deeply capable\n\n689\n00:27:20,400 --> 00:27:27,200\ntech industry the world's ever seen.\n\n690\n00:27:24,000 --> 00:27:29,760\nAnd I expect us to retain\n\n691\n00:27:27,200 --> 00:27:33,360\nour leadership position for decades to\n\n692\n00:27:29,760 --> 00:27:36,799\ncome. And I welcome competition.\n\n693\n00:27:33,360 --> 00:27:39,520\nLet's go. you know, competitors, come\n\n694\n00:27:36,799 --> 00:27:42,640\non, let's go play. That's the American\n\n695\n00:27:39,520 --> 00:27:46,799\nspirit. The the competitive spirit that\n\n696\n00:27:42,640 --> 00:27:49,200\nwe have um isn't lost.\n\n697\n00:27:46,799 --> 00:27:53,120\nAnd we need the opportunity as the\n\n698\n00:27:49,200 --> 00:27:55,440\nAmerican industry to go fight for\n\n699\n00:27:53,120 --> 00:27:59,039\nAmerican leadership.\n\n700\n00:27:55,440 --> 00:28:01,840\nAnd at a time when when\n\n701\n00:27:59,039 --> 00:28:03,840\ncompanies countries around the world all\n\n702\n00:28:01,840 --> 00:28:06,320\nhave capabilities, frankly, we're\n\n703\n00:28:03,840 --> 00:28:08,480\ninterdependent and we depend on the\n\n704\n00:28:06,320 --> 00:28:10,320\ncapabilities of many countries. You\n\n705\n00:28:08,480 --> 00:28:12,480\nknow, the deeper you go, the more you\n\n706\n00:28:10,320 --> 00:28:14,080\nrealized, you realize there are things\n\n707\n00:28:12,480 --> 00:28:16,399\nin Europe that we depend on. There are\n\n708\n00:28:14,080 --> 00:28:17,919\nthings in in Japan we depend on. There\n\n709\n00:28:16,399 --> 00:28:19,600\nare things in Southeast Asia we depend\n\n710\n00:28:17,919 --> 00:28:21,840\non. There's things in Latin America we\n\n711\n00:28:19,600 --> 00:28:24,320\ndepend on. You know, every country has\n\n712\n00:28:21,840 --> 00:28:26,399\ntheir specialty and their capabilities.\n\n713\n00:28:24,320 --> 00:28:28,559\nAnd China of course has formidable\n\n714\n00:28:26,399 --> 00:28:31,279\ncapabilities. Their technology companies\n\n715\n00:28:28,559 --> 00:28:33,679\nare formidable. Huawei is formidable.\n\n716\n00:28:31,279 --> 00:28:38,159\nBYD is formidable. These are incredible\n\n717\n00:28:33,679 --> 00:28:41,440\ncompanies. Their their\n\n718\n00:28:38,159 --> 00:28:47,279\nnational pride in manufacturing\n\n719\n00:28:41,440 --> 00:28:49,760\nand uh deep really deep and broad you\n\n720\n00:28:47,279 --> 00:28:51,760\nknow scale of manufacturing expertise\n\n721\n00:28:49,760 --> 00:28:54,640\ncan be undermined. It's not about labor.\n\n722\n00:28:51,760 --> 00:28:55,039\nIt's technology plus craft and labor\n\n723\n00:28:54,640 --> 00:28:55,520\nscale,\n\n724\n00:28:55,039 --> 00:28:57,279\n>> right?\n\n725\n00:28:55,520 --> 00:29:00,080\n>> The combination of those three things\n\n726\n00:28:57,279 --> 00:29:03,279\ntogether is just extraordinary. It's\n\n727\n00:29:00,080 --> 00:29:06,799\nit's something to to witness. And so we\n\n728\n00:29:03,279 --> 00:29:10,159\nwe need to realize that that um we are\n\n729\n00:29:06,799 --> 00:29:12,480\nnow in a interdependent world. Uh and so\n\n730\n00:29:10,159 --> 00:29:15,760\nwhat do we do? uh the things that we\n\n731\n00:29:12,480 --> 00:29:18,000\nshould do uh one\n\n732\n00:29:15,760 --> 00:29:20,080\nwhat President Trump's\n\n733\n00:29:18,000 --> 00:29:24,960\ninitiative President Trump's initiative\n\n734\n00:29:20,080 --> 00:29:30,080\non uh localizing or re reindustrializing\n\n735\n00:29:24,960 --> 00:29:33,919\nAmerica is just a fantastic and and\n\n736\n00:29:30,080 --> 00:29:36,480\nvisionary and timely initiative. We need\n\n737\n00:29:33,919 --> 00:29:38,960\nto we need to be world class at the\n\n738\n00:29:36,480 --> 00:29:41,120\ntechnology of manufacturing, the craft\n\n739\n00:29:38,960 --> 00:29:43,760\nof manufacturing and the labor scale of\n\n740\n00:29:41,120 --> 00:29:46,640\nmanufacturing. Again, that entire part\n\n741\n00:29:43,760 --> 00:29:49,279\nof our ecosystem is somewhat lagging and\n\n742\n00:29:46,640 --> 00:29:51,200\nwe've lost our passion for it. Uh maybe\n\n743\n00:29:49,279 --> 00:29:52,960\nit's not it maybe it's because back in\n\n744\n00:29:51,200 --> 00:29:55,679\nthe old days it was more about labor\n\n745\n00:29:52,960 --> 00:29:57,840\nthan it was about technology. Um but now\n\n746\n00:29:55,679 --> 00:29:59,600\nit's deeply technical and it's something\n\n747\n00:29:57,840 --> 00:30:02,559\nthat we could really get passionate\n\n748\n00:29:59,600 --> 00:30:04,960\nbehind. And so I I think this whole area\n\n749\n00:30:02,559 --> 00:30:07,760\nof manufacturing so that we could reduce\n\n750\n00:30:04,960 --> 00:30:10,480\nour dependency on many countries around\n\n751\n00:30:07,760 --> 00:30:12,880\nthe world reduce the temperature there\n\n752\n00:30:10,480 --> 00:30:14,640\num have more capabilities ourselves.\n\n753\n00:30:12,880 --> 00:30:16,559\nIt's great for our national security.\n\n754\n00:30:14,640 --> 00:30:20,080\nIt's great for our industries. It's\n\n755\n00:30:16,559 --> 00:30:22,399\ngreat for um for job creation. It's\n\n756\n00:30:20,080 --> 00:30:24,799\ngreat for our culture, frankly. It's\n\n757\n00:30:22,399 --> 00:30:26,880\ngreat for our society overall. And so I\n\n758\n00:30:24,799 --> 00:30:29,360\nI love that vision, President Trump's\n\n759\n00:30:26,880 --> 00:30:31,760\nvision of re-industrializing America.\n\n760\n00:30:29,360 --> 00:30:33,279\nAnd so I think that we need to do that.\n\n761\n00:30:31,760 --> 00:30:35,440\nMeanwhile,\n\n762\n00:30:33,279 --> 00:30:38,880\nwe have to get extra we have to stay\n\n763\n00:30:35,440 --> 00:30:41,600\nextraordinarily excellent in uh areas\n\n764\n00:30:38,880 --> 00:30:44,240\nlike artificial intelligence and and AI\n\n765\n00:30:41,600 --> 00:30:47,440\ncomputing and and the the tech stack so\n\n766\n00:30:44,240 --> 00:30:51,440\nthat we could be a partner to every\n\n767\n00:30:47,440 --> 00:30:53,360\ncountry in the world and um uh and and\n\n768\n00:30:51,440 --> 00:30:56,000\nmake a contribution to every country in\n\n769\n00:30:53,360 --> 00:30:58,159\nthe world so that we could have uh this\n\n770\n00:30:56,000 --> 00:31:01,200\ncontinued you know interdependency of\n\n771\n00:30:58,159 --> 00:31:02,640\neach other and uh uh you know drive our\n\n772\n00:31:01,200 --> 00:31:04,559\nindustry straight forward.\n\n773\n00:31:02,640 --> 00:31:07,279\n>> Last question. Jensen, you talk about\n\n774\n00:31:04,559 --> 00:31:10,640\nregaining strategic confidence. Is this\n\n775\n00:31:07,279 --> 00:31:12,960\nwhat that means in your words, leading,\n\n776\n00:31:10,640 --> 00:31:16,480\nkeep inventing? And by this end of this\n\n777\n00:31:12,960 --> 00:31:18,240\ndecade, American technologies have built\n\n778\n00:31:16,480 --> 00:31:20,399\nthe global infrastructure both on the\n\n779\n00:31:18,240 --> 00:31:20,880\ntech stack hardware, but as well as\n\n780\n00:31:20,399 --> 00:31:22,960\nsoftware.\n\n781\n00:31:20,880 --> 00:31:25,600\n>> Yeah, exactly. you know, most of the\n\n782\n00:31:22,960 --> 00:31:27,200\ntime most of the time regulation um\n\n783\n00:31:25,600 --> 00:31:31,520\npolicies\n\n784\n00:31:27,200 --> 00:31:34,640\nuh uh tends to be tends to tends to\n\n785\n00:31:31,520 --> 00:31:36,559\nfocus too much on limiting and\n\n786\n00:31:34,640 --> 00:31:38,720\nrestricting.\n\n787\n00:31:36,559 --> 00:31:42,640\nUm and that that's that is that is fine\n\n788\n00:31:38,720 --> 00:31:46,320\nto do. Um, I just want to remind remind\n\n789\n00:31:42,640 --> 00:31:47,840\nuh ourselves that America is\n\n790\n00:31:46,320 --> 00:31:50,000\nextraordinary\n\n791\n00:31:47,840 --> 00:31:51,360\nand that the companies here and I I have\n\n792\n00:31:50,000 --> 00:31:53,440\nthe benefit of working with companies\n\n793\n00:31:51,360 --> 00:31:55,279\nall over the world. Nvidia is obviously\n\n794\n00:31:53,440 --> 00:31:57,600\na global company. We have we have\n\n795\n00:31:55,279 --> 00:32:00,080\nbusinesses all over the world. Um, I can\n\n796\n00:31:57,600 --> 00:32:03,600\nattest that this country is\n\n797\n00:32:00,080 --> 00:32:06,640\nextraordinary has extraordinary\n\n798\n00:32:03,600 --> 00:32:12,399\nwork ethics. Frankly, I think Americans\n\n799\n00:32:06,640 --> 00:32:15,440\nwork, if not as hard as any hardworking\n\n800\n00:32:12,399 --> 00:32:16,960\nculture in the world, but I consider\n\n801\n00:32:15,440 --> 00:32:19,279\nmany American companies and many\n\n802\n00:32:16,960 --> 00:32:22,480\nindustries, we work the hardest of any\n\n803\n00:32:19,279 --> 00:32:25,840\nindustry. And so so Americans work, we\n\n804\n00:32:22,480 --> 00:32:29,039\nhave work ethics, we have incredible\n\n805\n00:32:25,840 --> 00:32:30,559\num uh\n\n806\n00:32:29,039 --> 00:32:32,480\nfoundation\n\n807\n00:32:30,559 --> 00:32:36,000\nuh for supporting industry and\n\n808\n00:32:32,480 --> 00:32:38,480\nsupporting startups. And uh it's a it's\n\n809\n00:32:36,000 --> 00:32:39,840\nstill the world's best place for\n\n810\n00:32:38,480 --> 00:32:41,519\nimmigrants to come.\n\n811\n00:32:39,840 --> 00:32:43,120\n>> Yeah.\n\n812\n00:32:41,519 --> 00:32:44,880\nIt's still the world's best place for\n\n813\n00:32:43,120 --> 00:32:47,840\nimmigrants to come to get a great\n\n814\n00:32:44,880 --> 00:32:50,320\neducation and have the opportunity with\n\n815\n00:32:47,840 --> 00:32:53,919\nall the ecosystem around us to build a\n\n816\n00:32:50,320 --> 00:32:56,080\ngreat company. I saw it firsthand.\n\n817\n00:32:53,919 --> 00:32:59,120\nYou know, no one has enjoyed the\n\n818\n00:32:56,080 --> 00:33:01,039\nAmerican dream and saw it personally in\n\n819\n00:32:59,120 --> 00:33:03,919\nmy lifetime\n\n820\n00:33:01,039 --> 00:33:06,080\nthan I have. You know, if if there's a\n\n821\n00:33:03,919 --> 00:33:07,919\nbook that's called the American dream,\n\n822\n00:33:06,080 --> 00:33:10,399\nI'm I might be one of the chapters,\n\n823\n00:33:07,919 --> 00:33:12,960\nright? you know, and so so this is this\n\n824\n00:33:10,399 --> 00:33:16,880\nis I embody the American dream and and\n\n825\n00:33:12,960 --> 00:33:21,840\nso I I I walk with extraordinary pride\n\n826\n00:33:16,880 --> 00:33:24,399\nand great and gratitude um and uh uh and\n\n827\n00:33:21,840 --> 00:33:28,880\nrecognizing what the magic of the of\n\n828\n00:33:24,399 --> 00:33:32,640\nAmerica and uh and also uh a great\n\n829\n00:33:28,880 --> 00:33:35,360\nconfidence about what we can do and and\n\n830\n00:33:32,640 --> 00:33:38,320\nso I think that that whatever policies\n\n831\n00:33:35,360 --> 00:33:41,039\nare created are developed\n\n832\n00:33:38,320 --> 00:33:42,640\nis to realize, you know, and this is no\n\n833\n00:33:41,039 --> 00:33:44,480\ndifferent than than companies developing\n\n834\n00:33:42,640 --> 00:33:46,960\nstrategies. Before you develop\n\n835\n00:33:44,480 --> 00:33:49,039\nstrategies about the adversary or the\n\n836\n00:33:46,960 --> 00:33:52,159\ncompetition, the first thing you have to\n\n837\n00:33:49,039 --> 00:33:55,120\ndo is know thyself.\n\n838\n00:33:52,159 --> 00:33:58,559\nAnd the the strategies that you deploy\n\n839\n00:33:55,120 --> 00:34:00,720\nwhen you're in defense versus the\n\n840\n00:33:58,559 --> 00:34:04,480\nstrategies that you deploy and policies\n\n841\n00:34:00,720 --> 00:34:06,880\nyou deploy when you are in offense are\n\n842\n00:34:04,480 --> 00:34:08,879\nrelated, not the same. And so it's\n\n843\n00:34:06,880 --> 00:34:11,280\nreally important to s get a sense of\n\n844\n00:34:08,879 --> 00:34:14,560\nwhat our what our national capabilities\n\n845\n00:34:11,280 --> 00:34:17,359\nare and especially in the field of\n\n846\n00:34:14,560 --> 00:34:19,440\nartificial intelligence and computing uh\n\n847\n00:34:17,359 --> 00:34:21,440\nto recognize what an extraordinary\n\n848\n00:34:19,440 --> 00:34:24,879\nindustry we've created somehow over the\n\n849\n00:34:21,440 --> 00:34:27,679\nyears and it is our national treasure.\n\n850\n00:34:24,879 --> 00:34:31,200\nWe should do everything we can to\n\n851\n00:34:27,679 --> 00:34:33,440\nfurther this capability to nurture this\n\n852\n00:34:31,200 --> 00:34:36,560\ncapability to protect this capability\n\n853\n00:34:33,440 --> 00:34:38,240\nand enhance it. And so I can't tell you\n\n854\n00:34:36,560 --> 00:34:41,599\nhow proud it, you know, here I am in\n\n855\n00:34:38,240 --> 00:34:44,079\nWashington DC, uh, and, uh, our nation's\n\n856\n00:34:41,599 --> 00:34:46,320\ncapital. Uh, you it's hard, it's hard\n\n857\n00:34:44,079 --> 00:34:50,320\nnot to feel patriotic after you see the\n\n858\n00:34:46,320 --> 00:34:52,960\npresident and and, um, but it it is a a\n\n859\n00:34:50,320 --> 00:34:56,000\ngreat reminder what an amazing country\n\n860\n00:34:52,960 --> 00:34:58,160\nwe've somehow built over the years and\n\n861\n00:34:56,000 --> 00:35:00,320\nwhat an industry that has been that has\n\n862\n00:34:58,160 --> 00:35:02,320\nemerged from it as a result. And we have\n\n863\n00:35:00,320 --> 00:35:04,079\nevery reason to be proud and every\n\n864\n00:35:02,320 --> 00:35:06,800\nreason to be confident. On that\n\n865\n00:35:04,079 --> 00:35:09,520\nincredible uh last note, thank you for\n\n866\n00:35:06,800 --> 00:35:11,040\nreally um being a guest on our show.\n\n867\n00:35:09,520 --> 00:35:12,800\nTruly appreciate. We value the\n\n868\n00:35:11,040 --> 00:35:14,400\npartnership we have with Nvidia and I\n\n869\n00:35:12,800 --> 00:35:15,599\nlook forward to many more conversation\n\n870\n00:35:14,400 --> 00:35:19,470\nwith you. Thank you, Jensen.\n\n871\n00:35:15,599 --> 00:35:22,720\n>> Thank you. It's great to be here.\n\n872\n00:35:19,470 --> 00:35:24,320\n[Music]\n\n873\n00:35:22,720 --> 00:35:27,520\n>> Thank you for watching my interview with\n\n874\n00:35:24,320 --> 00:35:29,520\nthe Nvidia CEO Jensen Wong. I hope you\n\n875\n00:35:27,520 --> 00:35:32,320\nenjoyed the conversation as much as I\n\n876\n00:35:29,520 --> 00:35:33,680\ndid. While I have you, I also wanted to\n\n877\n00:35:32,320 --> 00:35:35,760\nlet you know that the special\n\n878\n00:35:33,680 --> 00:35:39,359\ncompetitive studies project is\n\n879\n00:35:35,760 --> 00:35:42,560\norganizing an allday summit on AI plus\n\n880\n00:35:39,359 --> 00:35:45,040\nscience on July 23rd here in Washington\n\n881\n00:35:42,560 --> 00:35:47,359\nDC. So, please register if you're\n\n882\n00:35:45,040 --> 00:35:49,040\ninterested in attending and being part\n\n883\n00:35:47,359 --> 00:35:52,640\nof amazing conversations we have\n\n884\n00:35:49,040 --> 00:35:55,440\nplanned. Lastly, in March of this year,\n\n885\n00:35:52,640 --> 00:35:58,079\nwe launched a Genai course for national\n\n886\n00:35:55,440 --> 00:36:00,240\nsecurity in partnership with Corsera.\n\n887\n00:35:58,079 --> 00:36:02,400\nIt's an incredible course. More than\n\n888\n00:36:00,240 --> 00:36:04,480\n3,000 people have enrolled. You can be\n\n889\n00:36:02,400 --> 00:36:09,640\none of them if you start doing that\n\n890\n00:36:04,480 --> 00:36:09,640\ntoday. Hope you'll enjoy it. Thank you.\n\n","srtEs":"1\n00:00:02,610 --> 00:00:07,800\n[MÃºsica]\n\n2\n00:00:08,240 --> 00:00:12,320\nHola, soy Millie con el especial.\n\n3\n00:00:10,160 --> 00:00:14,080\nProyecto de Estudios Competitivos. en esto\n\n4\n00:00:12,320 --> 00:00:16,560\nepisodio de la semana de Memos to the\n\n5\n00:00:14,080 --> 00:00:19,439\nPresidente, he tenido la oportunidad de\n\n6\n00:00:16,560 --> 00:00:22,080\ncharle con Jensen Wong, fundador y director ejecutivo\n\n7\n00:00:19,439 --> 00:00:24,160\nde Nvidia. Cubrimos todo lo relacionado\n\n8\n00:00:22,080 --> 00:00:28,519\na la IA. Espero que disfrutes el\n\n9\n00:00:24,160 --> 00:00:28,519\nconversaciÃ³n tanto como yo.\n\n10\n00:00:32,480 --> 00:00:36,000\nHola, bienvenido de nuevo a Memos to the\n\n11\n00:00:34,160 --> 00:00:38,640\nPresidente. Es realmente un honor sentarse\n\n12\n00:00:36,000 --> 00:00:41,280\nhoy con un invitado especial, Jensen\n\n13\n00:00:38,640 --> 00:00:43,200\nWong, fundador y director ejecutivo de Nvidia. Este\n\n14\n00:00:41,280 --> 00:00:45,280\nFue una gran semana para Nvidia. Entonces, me voy\n\n15\n00:00:43,200 --> 00:00:48,399\npasarle la palabra a Jensen para hablar sobre cÃ³mo\n\n16\n00:00:45,280 --> 00:00:50,320\nÂ¿Se siente esta semana ser el fundador?\n\n17\n00:00:48,399 --> 00:00:51,120\ny director ejecutivo de Nvidia. Jensen, bienvenido a mi\n\n18\n00:00:50,320 --> 00:00:52,640\npodcast.\n\n19\n00:00:51,120 --> 00:00:55,280\n>> Gracias, Elie. es un increible\n\n20\n00:00:52,640 --> 00:00:56,559\nhonor. Guau. Semana loca, Â¿verdad?\n\n21\n00:00:55,280 --> 00:00:58,320\n>> Es una semana increÃ­ble para Nvidia.\n\n22\n00:00:56,559 --> 00:01:00,079\nSÃ­, es una semana loca. creo que um\n\n23\n00:00:58,320 --> 00:01:04,000\nBueno, primero que nada, en realidad no lo ha sido.\n\n24\n00:01:00,079 --> 00:01:06,560\nhundido todavÃ­a. Um pero veamos que\n\n25\n00:01:04,000 --> 00:01:08,159\nMe vienen pensamientos a la mente. Bueno, ya sabes,\n\n26\n00:01:06,560 --> 00:01:11,280\nde muchas maneras, alguien me preguntÃ³\n\n27\n00:01:08,159 --> 00:01:14,640\nayer sobre y y y mencionado a\n\n28\n00:01:11,280 --> 00:01:16,320\nyo uh que soy inmigrante, y en\n\n29\n00:01:14,640 --> 00:01:16,960\nDe hecho, soy un inmigrante. hablando de\n\n30\n00:01:16,320 --> 00:01:18,000\ninmigraciÃ³n,\n\n31\n00:01:16,960 --> 00:01:19,600\n>> lo mismo aquÃ­.\n\n32\n00:01:18,000 --> 00:01:23,439\n>> Y los dos somos inmigrantes.\n\n33\n00:01:19,600 --> 00:01:26,000\nY uh, Estados Unidos representa\n\n34\n00:01:23,439 --> 00:01:27,680\nLa uh, la tierra del sueÃ±o americano.\n\n35\n00:01:26,000 --> 00:01:30,080\nY este es el lugar donde los inmigrantes\n\n36\n00:01:27,680 --> 00:01:32,880\nvenir a construir construir una vida y construir\n\n37\n00:01:30,080 --> 00:01:34,720\nuna empresa. En muchos sentidos, ya sabes,\n\n38\n00:01:32,880 --> 00:01:36,159\nlo que estoy experimentando es probablemente el\n\n39\n00:01:34,720 --> 00:01:37,759\nEl sueÃ±o americano definitivo.\n\n40\n00:01:36,159 --> 00:01:39,280\n>> Absolutamente. ya sabes, venir venir a\n\n41\n00:01:37,759 --> 00:01:41,840\nEstados Unidos cuando tenÃ­a siete aÃ±os.\n\n42\n00:01:39,280 --> 00:01:43,439\nviejo, no creo que tenga ocho aÃ±os y y\n\n43\n00:01:41,840 --> 00:01:45,439\nuno\n\n44\n00:01:43,439 --> 00:01:48,240\ny tener la oportunidad de fundar\n\n45\n00:01:45,439 --> 00:01:53,119\nuna empresa con buenos amigos 33 aÃ±os\n\n46\n00:01:48,240 --> 00:01:56,240\nhace, ser el uh el CEO hoy despuÃ©s de 33\n\n47\n00:01:53,119 --> 00:01:58,240\naÃ±os y um teniendo muchos de los fundadores\n\n48\n00:01:56,240 --> 00:02:00,960\nque estaban ahÃ­ conmigo en el\n\n49\n00:01:58,240 --> 00:02:02,960\nmuy empezando todavÃ­a aquÃ­ en NVIDIA um\n\n50\n00:02:00,960 --> 00:02:05,280\npersiguiendo un sueÃ±o que hemos tenido que\n\n51\n00:02:02,960 --> 00:02:06,640\nque tardÃ³ tres dÃ©cadas en lograrse\n\n52\n00:02:05,280 --> 00:02:09,520\n>> con muchos altibajos. con\n\n53\n00:02:06,640 --> 00:02:12,319\nenorme cantidad de altibajos y tÃº\n\n54\n00:02:09,520 --> 00:02:15,440\nLo sÃ© de muchas maneras uh esto\n\n55\n00:02:12,319 --> 00:02:16,959\nhito lo que nos estÃ¡ pasando es\n\n56\n00:02:15,440 --> 00:02:20,000\nun poco difÃ­cil de internalizar, es un poco\n\n57\n00:02:16,959 --> 00:02:21,680\ndifÃ­cil de asimilar pero tambiÃ©n representa\n\n58\n00:02:20,000 --> 00:02:24,080\nalgo que es realmente importante para ti\n\n59\n00:02:21,680 --> 00:02:27,120\nSÃ© que querÃ­amos reinventar el\n\n60\n00:02:24,080 --> 00:02:28,800\ncomputadora la computadora tal como la conocemos uh\n\n61\n00:02:27,120 --> 00:02:30,800\nrealmente ha sido definido en gran medida para\n\n62\n00:02:28,800 --> 00:02:34,959\nAproximadamente seis dÃ©cadas desde el sistema IBM.\n\n63\n00:02:30,800 --> 00:02:36,720\n360 IBM como sabes era, eh, el\n\n64\n00:02:34,959 --> 00:02:40,239\nempresa mÃ¡s grande del mundo de sus\n\n65\n00:02:36,720 --> 00:02:43,040\nel tiempo y el plano que le pusieron\n\n66\n00:02:40,239 --> 00:02:44,480\njuntos para la informÃ¡tica era bÃ¡sicamente\n\n67\n00:02:43,040 --> 00:02:47,360\nes el mismo plano que ha sido\n\n68\n00:02:44,480 --> 00:02:49,360\ndesarrollado en las Ãºltimas seis dÃ©cadas.\n\n69\n00:02:47,360 --> 00:02:51,599\nTodo, desde la arquitectura del\n\n70\n00:02:49,360 --> 00:02:54,400\nsistemas, la forma en que la separaciÃ³n entre\n\n71\n00:02:51,599 --> 00:02:57,280\nsoftware y hardware y arquitectura\n\n72\n00:02:54,400 --> 00:03:00,080\ncompatibilidad y ya sabes aplicaciÃ³n\n\n73\n00:02:57,280 --> 00:03:01,680\ncompatibilidad, alineaciÃ³n familiar completa, tÃº\n\n74\n00:03:00,080 --> 00:03:04,959\nSabes, todas las cosas que ellos\n\n75\n00:03:01,680 --> 00:03:07,120\ndescrito uh describe en gran medida el\n\n76\n00:03:04,959 --> 00:03:09,360\nindustria informÃ¡tica hoy y en orden\n\n77\n00:03:07,120 --> 00:03:12,000\ny la oportunidad de reinventar eso y\n\n78\n00:03:09,360 --> 00:03:14,159\nllevarlo al siguiente nivel y ahora ser\n\n79\n00:03:12,000 --> 00:03:16,319\nla plataforma para la inteligencia artificial\n\n80\n00:03:14,159 --> 00:03:17,840\nEs realmente un sueÃ±o hecho realidad.\n\n81\n00:03:16,319 --> 00:03:20,800\nSÃ­, un momento realmente extraordinario.\n\n82\n00:03:17,840 --> 00:03:23,200\n>> Hablas de las ondas de IA.\n\n83\n00:03:20,800 --> 00:03:24,560\n>> y a mi y a mi nos gusta mucho como divides\n\n84\n00:03:23,200 --> 00:03:24,959\nclasificarlos en diferentes categorÃ­as.\n\n85\n00:03:24,560 --> 00:03:27,840\n>> SÃ­.\n\n86\n00:03:24,959 --> 00:03:31,360\n>> Â¿Puedes describir dÃ³nde nos encontramos hoy?\n\n87\n00:03:27,840 --> 00:03:35,280\nÂ¿TÃ©rminos de la ola y cÃ³mo llegamos aquÃ­?\n\n88\n00:03:31,360 --> 00:03:37,599\n>> Um 20 2012 uh uh vimos el mismo momento\n\n89\n00:03:35,280 --> 00:03:40,799\ncomo todos los demÃ¡s. Tuvimos el tuvimos el\n\n90\n00:03:37,599 --> 00:03:42,319\nla pista interior en el sentido de que uh nosotros\n\n91\n00:03:40,799 --> 00:03:44,640\nSiempre creÃ­ que CUDA iba a\n\n92\n00:03:42,319 --> 00:03:47,200\npermitir una nueva clase de aplicaciones y\n\n93\n00:03:44,640 --> 00:03:50,799\nsiempre estÃ¡bamos atentos a ello. Y\n\n94\n00:03:47,200 --> 00:03:53,120\nentonces cuando cuando um Alexet vino um\n\n95\n00:03:50,799 --> 00:03:56,640\nConstruido sobre CUDA, nuestras GPU lo hicieron\n\n96\n00:03:53,120 --> 00:03:59,120\nposible uh para entrenar a AlexNet y para\n\n97\n00:03:56,640 --> 00:04:01,599\nAlex Net para lograr algo tan extraordinario\n\n98\n00:03:59,120 --> 00:04:04,879\nresultados en visiÃ³n por computadora, lograr el\n\n99\n00:04:01,599 --> 00:04:07,120\nnivel de capacidad que una computadora\n\n100\n00:04:04,879 --> 00:04:09,120\ncientÃ­ficos especializados en informÃ¡tica\n\n101\n00:04:07,120 --> 00:04:11,760\nla visiÃ³n no pudo lograr mÃ¡s de cuatro\n\n102\n00:04:09,120 --> 00:04:15,200\ndÃ©cadas. Ya sabes, para que tres personas\n\n103\n00:04:11,760 --> 00:04:17,919\nhacer algo asÃ­. um uh es\n\n104\n00:04:15,200 --> 00:04:20,000\nsimplemente una hazaÃ±a extraordinaria. Y entonces nosotros\n\n105\n00:04:17,919 --> 00:04:21,680\naprovechÃ© la oportunidad y nos miramos\n\n106\n00:04:20,000 --> 00:04:23,680\nÂ¿QuÃ© es lo que estamos mirando? TÃº\n\n107\n00:04:21,680 --> 00:04:26,240\nÂ¿Sabes quÃ© estÃ¡ pasando aquÃ­? Es esto\n\n108\n00:04:23,680 --> 00:04:29,360\nÂ¿Es Alec Alex al Alexnet un gran avance?\n\n109\n00:04:26,240 --> 00:04:31,440\nen visiÃ³n por computadora o es mÃ¡s grande\n\n110\n00:04:29,360 --> 00:04:34,080\nidea que esa? Y por supuesto, como nosotros\n\n111\n00:04:31,440 --> 00:04:36,080\nYa sabes, la visiÃ³n por computadora es un pilar de\n\n112\n00:04:34,080 --> 00:04:37,680\ninteligencia artificial. Sabes,\n\n113\n00:04:36,080 --> 00:04:39,280\nsin visiÃ³n por computadora, sin habla\n\n114\n00:04:37,680 --> 00:04:41,840\ncomprensiÃ³n del idioma, es difÃ­cil\n\n115\n00:04:39,280 --> 00:04:43,520\ntener inteligencia. Y entonces nosotros nosotros\n\n116\n00:04:41,840 --> 00:04:45,440\nMe di cuenta de que esto, por supuesto, era parte\n\n117\n00:04:43,520 --> 00:04:46,880\nde inteligencia artificial. Â¿Pero es un\n\n118\n00:04:45,440 --> 00:04:49,840\nÂ¿Una idea mÃ¡s grande que esa? Y llegamos a\n\n119\n00:04:46,880 --> 00:04:53,680\nla conclusiÃ³n de que lo que AlexNet y\n\n120\n00:04:49,840 --> 00:04:56,880\nEl aprendizaje profundo demostrÃ³ es que ahora es\n\n121\n00:04:53,680 --> 00:05:00,000\nfinalmente posible si tuviÃ©ramos suficientes datos,\n\n122\n00:04:56,880 --> 00:05:01,600\nsuficiente escala informÃ¡tica. Y por supuesto nosotros\n\n123\n00:05:00,000 --> 00:05:03,520\ntener estos modelos de aprendizaje profundo que son\n\n124\n00:05:01,600 --> 00:05:07,039\nbastante escalable\n\n125\n00:05:03,520 --> 00:05:10,000\num que podrÃ­amos ser capaces de aplicar\n\n126\n00:05:07,039 --> 00:05:12,240\ncomputadoras para resolver problemas uh que eran\n\n127\n00:05:10,000 --> 00:05:14,720\nimposible de describir usando humanos\n\n128\n00:05:12,240 --> 00:05:17,280\ncaracterÃ­stica diseÃ±ada y utilizando principios\n\n129\n00:05:14,720 --> 00:05:19,360\nalgoritmos. Y entonces nosotros, um, tenemos\n\n130\n00:05:17,280 --> 00:05:21,039\nemocionado desde esa perspectiva. Nosotros tambiÃ©n\n\n131\n00:05:19,360 --> 00:05:23,840\nMe emocionÃ© porque\n\n132\n00:05:21,039 --> 00:05:26,400\n>> porque cuando razonas\n\n133\n00:05:23,840 --> 00:05:29,280\num aprendizaje profundo y la formaciÃ³n de\n\n134\n00:05:26,400 --> 00:05:31,199\nAlexNet y hacia dÃ³nde podrÃ­a llegar, nosotros\n\n135\n00:05:29,280 --> 00:05:32,400\nse dio cuenta de que toda la informÃ¡tica\n\n136\n00:05:31,199 --> 00:05:34,160\nla plataforma va a cambiar,\n\n137\n00:05:32,400 --> 00:05:35,680\n>> Â¿verdad? los procesadores van a cambiar,\n\n138\n00:05:34,160 --> 00:05:37,600\nla conexiÃ³n a internet va a cambiar,\n\n139\n00:05:35,680 --> 00:05:39,440\nlas redes van a cambiar, el\n\n140\n00:05:37,600 --> 00:05:41,759\npila de software encima de Ã©l, Â¿cÃ³mo\n\n141\n00:05:39,440 --> 00:05:44,080\ndesarrollar el software, la metodologÃ­a de\n\n142\n00:05:41,759 --> 00:05:46,080\nsoftware dentro de las empresas y por supuesto\n\n143\n00:05:44,080 --> 00:05:48,000\nlas muchas industrias en las que podrÃ­amos estar\n\n144\n00:05:46,080 --> 00:05:50,479\ncapaz de crear iba a completamente\n\n145\n00:05:48,000 --> 00:05:53,440\ncambiar y entonces nos pusimos a hacerlo.\n\n146\n00:05:50,479 --> 00:05:54,800\nBÃ¡sicamente, reiniciamos nuestra empresa. Ahora\n\n147\n00:05:53,440 --> 00:05:58,240\nLas olas de las que estabas hablando.\n\n148\n00:05:54,800 --> 00:06:00,639\nsobre despuÃ©s de que hicimos eso, um, dedicamos\n\n149\n00:05:58,240 --> 00:06:02,639\nNosotros mismos para crear nuevas bibliotecas.\n\n150\n00:06:00,639 --> 00:06:05,759\nllamado KDNN\n\n151\n00:06:02,639 --> 00:06:09,520\nuh creando creando uh marcos de IA uh\n\n152\n00:06:05,759 --> 00:06:12,639\nllamado Mega Core um Megatron core uh para\n\n153\n00:06:09,520 --> 00:06:15,680\nuh inventando MVLink y nÃºcleos tensoriales y\n\n154\n00:06:12,639 --> 00:06:17,840\nlos diferentes formatos numÃ©ricos y uh\n\n155\n00:06:15,680 --> 00:06:21,199\nllevamos a la creaciÃ³n de un sistema que\n\n156\n00:06:17,840 --> 00:06:23,039\nLlame a DGX1 nuestra primera supercomputadora de IA. I\n\n157\n00:06:21,199 --> 00:06:24,400\nLo entreguÃ© personalmente a una startup en\n\n158\n00:06:23,039 --> 00:06:27,919\nSan Francisco que resultÃ³ tener\n\n159\n00:06:24,400 --> 00:06:30,560\nHa sido IA abierta y de todos modos eso\n\n160\n00:06:27,919 --> 00:06:33,199\nEl viaje realmente podrÃ­a capturarse en\n\n161\n00:06:30,560 --> 00:06:36,160\nvarias maneras. Desde 2012, la primera\n\n162\n00:06:33,199 --> 00:06:38,160\nLo que sucediÃ³ fue que la IA despegÃ³. Oh,\n\n163\n00:06:36,160 --> 00:06:39,840\nEl aprendizaje profundo siguiÃ³ avanzando. La cantidad\n\n164\n00:06:38,160 --> 00:06:42,479\nde datos que tenÃ­amos, la cantidad de cÃ¡lculo que\n\n165\n00:06:39,840 --> 00:06:44,240\nhabÃ­a seguido creciendo, seguido escalando. y eso\n\n166\n00:06:42,479 --> 00:06:47,680\ncondujo a la primera ola, que es realmente\n\n167\n00:06:44,240 --> 00:06:49,440\ndescrito como, si pudiera, percepciÃ³n. Nosotros\n\n168\n00:06:47,680 --> 00:06:50,479\nresolvimos la percepciÃ³n. se convirtiÃ³\n\n169\n00:06:49,440 --> 00:06:52,240\nsobrehumano.\n\n170\n00:06:50,479 --> 00:06:54,319\n>> La visiÃ³n por computadora se volviÃ³ sobrehumana.\n\n171\n00:06:52,240 --> 00:06:56,800\nComprensiÃ³n del lenguaje o habla.\n\n172\n00:06:54,319 --> 00:06:58,960\nEl reconocimiento se volviÃ³ sobrehumano. El\n\n173\n00:06:56,800 --> 00:07:00,560\nsegundo la segunda fase es uh\n\n174\n00:06:58,960 --> 00:07:03,919\ngenerativo.\n\n175\n00:07:00,560 --> 00:07:06,560\nuh ahora no sÃ³lo podemos entender\n\n176\n00:07:03,919 --> 00:07:09,199\ninformaciÃ³n pero podemos traducir y\n\n177\n00:07:06,560 --> 00:07:11,120\ngenerar informaciÃ³n. AsÃ­ que texto a texto,\n\n178\n00:07:09,199 --> 00:07:13,680\ntexto a imÃ¡genes,\n\n179\n00:07:11,120 --> 00:07:15,840\n>> imÃ¡genes a texto, texto a video.\n\n180\n00:07:13,680 --> 00:07:18,400\n>> Y si pudieras convertir texto a video,\n\n181\n00:07:15,840 --> 00:07:20,319\nÂ¿Sabes quÃ© mÃ¡s no puedes hacer? Entonces\n\n182\n00:07:18,400 --> 00:07:22,880\nLa segunda fase fue la IA generativa. Este\n\n183\n00:07:20,319 --> 00:07:25,120\ntercera tercera ola es la ola que estamos\n\n184\n00:07:22,880 --> 00:07:28,319\nen hoy, ya sabes, muy profundamente\n\n185\n00:07:25,120 --> 00:07:28,880\nsÃ³lidamente en lo que estÃ¡ razonando la IA.\n\n186\n00:07:28,319 --> 00:07:32,080\nMmmm.\n\n187\n00:07:28,880 --> 00:07:36,240\n>> AquÃ­ es donde podrÃ­a aplicarse una IA\n\n188\n00:07:32,080 --> 00:07:39,599\nprincipios y conocimientos, tal vez algunos\n\n189\n00:07:36,240 --> 00:07:43,919\nsentido comÃºn y utilizar tÃ©cnicas como\n\n190\n00:07:39,599 --> 00:07:47,039\ncadena de pensamiento, Ã¡rboles de pensamiento um a\n\n191\n00:07:43,919 --> 00:07:49,759\nuh, divide el problema en mÃºltiples\n\n192\n00:07:47,039 --> 00:07:52,319\npasos, razonar sobre cÃ³mo resolver el\n\n193\n00:07:49,759 --> 00:07:54,479\nproblema, el objetivo mÃ¡s amplio paso a paso.\n\n194\n00:07:52,319 --> 00:07:57,360\nIncluso podrÃ­a investigar un poco, leer\n\n195\n00:07:54,479 --> 00:07:59,199\nalgunos documentos, leer un documento de archivo\n\n196\n00:07:57,360 --> 00:08:00,879\nantes de que responda la pregunta.\n\n197\n00:07:59,199 --> 00:08:05,759\n>> Y entonces la tercera ola en la que estamos\n\n198\n00:08:00,879 --> 00:08:08,479\nhoy, que es razonamiento, es un acontecimiento muy grande\n\n199\n00:08:05,759 --> 00:08:10,560\nparte de ver la aceleraciÃ³n de la IA\n\n200\n00:08:08,479 --> 00:08:12,400\nconvirtiÃ©ndose en IA. Ese es el hecho de que\n\n201\n00:08:10,560 --> 00:08:15,039\nestamos estamos haciendo razonamientos que la IA es el\n\n202\n00:08:12,400 --> 00:08:17,520\nrazÃ³n por la cual um uh la gente estÃ¡ empezando\n\n203\n00:08:15,039 --> 00:08:18,240\npara decir, ya sabes, estamos cerca del general\n\n204\n00:08:17,520 --> 00:08:20,879\ninteligencia,\n\n205\n00:08:18,240 --> 00:08:24,160\n>> Â¿verdad? Y luego el tercero el siguiente\n\n206\n00:08:20,879 --> 00:08:26,960\nola despuÃ©s despuÃ©s uh uh despuÃ©s del razonamiento\n\n207\n00:08:24,160 --> 00:08:28,479\nLa IA es IA fÃ­sica. AquÃ­ es donde la IA\n\n208\n00:08:26,960 --> 00:08:30,800\nahora sabe cÃ³mo interactuar con el\n\n209\n00:08:28,479 --> 00:08:34,479\nmundo fÃ­sico. tiene mundo fisico\n\n210\n00:08:30,800 --> 00:08:36,959\nsentido comÃºn como permanencia del objeto,\n\n211\n00:08:34,479 --> 00:08:39,200\nfricciÃ³n, inercia,\n\n212\n00:08:36,959 --> 00:08:41,360\ncausa y efecto, y ya sabes, todos\n\n213\n00:08:39,200 --> 00:08:43,599\neste tipo de tipos de sentido comÃºn\n\n214\n00:08:41,360 --> 00:08:44,399\nque los niÃ±os tienen, ya sabes, cachorros\n\n215\n00:08:43,599 --> 00:08:46,080\ntener.\n\n216\n00:08:44,399 --> 00:08:47,839\n>> Ya sabes, ahora vamos a la IA.\n\n217\n00:08:46,080 --> 00:08:49,760\ntener esas cosas. Y como resultado de\n\n218\n00:08:47,839 --> 00:08:52,000\neso, la colecciÃ³n de todos estos\n\n219\n00:08:49,760 --> 00:08:53,519\ncapacidades, deberÃ­amos poder ver,\n\n220\n00:08:52,000 --> 00:08:54,640\nya sabes, la prÃ³xima ola, que es\n\n221\n00:08:53,519 --> 00:08:55,519\nprobablemente robÃ³tica,\n\n222\n00:08:54,640 --> 00:08:58,880\n>> Â¿verdad? SÃ­.\n\n223\n00:08:55,519 --> 00:09:00,800\n>> Um, ya sabes, has construido lo digital\n\n224\n00:08:58,880 --> 00:09:04,720\ninfraestructura para la forma en que vivimos\n\n225\n00:09:00,800 --> 00:09:07,360\nhoy. Uh, tÃº tambiÃ©n estÃ¡s trabajando y, um,\n\n226\n00:09:04,720 --> 00:09:09,440\ntener una visiÃ³n para las fÃ¡bricas de IA. Poder\n\n227\n00:09:07,360 --> 00:09:11,519\ndesempacas Â¿quÃ© significa eso? SÃ­.\n\n228\n00:09:09,440 --> 00:09:14,399\n>> Â¿CÃ³mo transforma eso los datos actuales?\n\n229\n00:09:11,519 --> 00:09:17,680\ncentro para el futuro? el semiconductor\n\n230\n00:09:14,399 --> 00:09:19,200\nindustria, TSMC, uh, la computadora\n\n231\n00:09:17,680 --> 00:09:20,959\necosistema\n\n232\n00:09:19,200 --> 00:09:24,160\nuh eso entonces crea estos\n\n233\n00:09:20,959 --> 00:09:27,120\ncomputadoras, Nvidia hoy, representamos,\n\n234\n00:09:24,160 --> 00:09:29,279\npor asÃ­ decirlo, el ecosistema digital, el\n\n235\n00:09:27,120 --> 00:09:31,200\ninfraestructura digital del mundo, la\n\n236\n00:09:29,279 --> 00:09:32,480\ninfraestructura informÃ¡tica.\n\n237\n00:09:31,200 --> 00:09:34,560\n>> Y ademÃ¡s de esa informÃ¡tica\n\n238\n00:09:32,480 --> 00:09:37,279\nLa infraestructura se dio cuenta de esto.\n\n239\n00:09:34,560 --> 00:09:40,880\nllamada inteligencia artificial.\n\n240\n00:09:37,279 --> 00:09:43,760\nÂ¿Y quÃ© tiene de interesante acerca de um?\n\n241\n00:09:40,880 --> 00:09:46,720\nsobre la uh, la Ãºltima industria es esa\n\n242\n00:09:43,760 --> 00:09:49,200\nnosotros la infraestructura digital\n\n243\n00:09:46,720 --> 00:09:51,839\nla industria informÃ¡tica uh habilitada\n\n244\n00:09:49,200 --> 00:09:54,720\nsoftware y eso representa ya sabes\n\n245\n00:09:51,839 --> 00:09:56,959\nalrededor de un billÃ³n de dÃ³lares de industria.\n\n246\n00:09:54,720 --> 00:09:59,440\nTÃº, tÃº, usas la computadora.\n\n247\n00:09:56,959 --> 00:10:01,440\ninfraestructuras para escribir el software\n\n248\n00:09:59,440 --> 00:10:04,800\npero luego implementas el software en\n\n249\n00:10:01,440 --> 00:10:07,920\ncosas como usted sabe telÃ©fonos, telÃ©fonos inteligentes\n\n250\n00:10:04,800 --> 00:10:11,040\ny por eso la industria del software no era\n\n251\n00:10:07,920 --> 00:10:12,880\nmuy grande sabes que es una llamada ya sabes\n\n252\n00:10:11,040 --> 00:10:15,200\nEs medio billÃ³n de dÃ³lares, Â¿verdad?\n\n253\n00:10:12,880 --> 00:10:17,760\n>> y la industria del hardware no es muy grande\n\n254\n00:10:15,200 --> 00:10:19,920\nllÃ¡malo medio billÃ³n de dÃ³lares y todo\n\n255\n00:10:17,760 --> 00:10:22,640\nDe repente esta industria la computadora.\n\n256\n00:10:19,920 --> 00:10:24,880\ninteligencia artificial habilitada por la industria\n\n257\n00:10:22,640 --> 00:10:26,959\ny lo realmente interesante es\n\n258\n00:10:24,880 --> 00:10:28,800\nLa inteligencia artificial es a la vez esto.\n\n259\n00:10:26,959 --> 00:10:30,880\ntecnologÃ­a revolucionaria que acabamos de\n\n260\n00:10:28,800 --> 00:10:33,279\nhablÃ³ de\n\n261\n00:10:30,880 --> 00:10:35,279\ny es por su percepciÃ³n y\n\n262\n00:10:33,279 --> 00:10:37,279\ncapacidad de razonamiento. Puedes usarlo para\n\n263\n00:10:35,279 --> 00:10:39,120\nresolver problemas en casi todos\n\n264\n00:10:37,279 --> 00:10:40,959\nuna sola industria porque cada industria\n\n265\n00:10:39,120 --> 00:10:43,600\nque sabemos en la pastilla en el\n\n266\n00:10:40,959 --> 00:10:48,000\nSu fundamento es la inteligencia y ahora\n\n267\n00:10:43,600 --> 00:10:50,079\nPodemos crear inteligencia en\n\n268\n00:10:48,000 --> 00:10:52,399\nescalas increÃ­bles y por supuesto eso es\n\n269\n00:10:50,079 --> 00:10:54,079\nrevolucionarÃ¡ todas las industrias.\n\n270\n00:10:52,399 --> 00:10:56,079\nUno\n\n271\n00:10:54,079 --> 00:10:58,640\nesa es la perspectiva tecnolÃ³gica. QuÃ©\n\n272\n00:10:56,079 --> 00:11:01,279\nsobre la perspectiva industrial? En\n\n273\n00:10:58,640 --> 00:11:02,640\nPara producir estas IA, Â¿quÃ© es?\n\n274\n00:11:01,279 --> 00:11:05,279\nrealmente saliendo de esto? que viene\n\n275\n00:11:02,640 --> 00:11:08,000\nDe estos modelos hay tokens y estos.\n\n276\n00:11:05,279 --> 00:11:11,279\nLas fichas se formulan en palabras y\n\n277\n00:11:08,000 --> 00:11:13,920\nnÃºmeros y sÃ­mbolos y podrÃ­an estar en el\n\n278\n00:11:11,279 --> 00:11:18,480\nFuturos productos quÃ­micos y proteÃ­nas para fÃ¡rmacos.\n\n279\n00:11:13,920 --> 00:11:22,399\ndescubrimiento. PodrÃ­an ser movimientos del actuador.\n\n280\n00:11:18,480 --> 00:11:26,399\npara uh para um conducir un auto sin conductor uh\n\n281\n00:11:22,399 --> 00:11:29,519\no animar un robot. Y asÃ­ estos\n\n282\n00:11:26,399 --> 00:11:31,760\nestas fichas que estan saliendo son\n\n283\n00:11:29,519 --> 00:11:34,480\nreformulado\n\n284\n00:11:31,760 --> 00:11:36,720\nreconstituido en inteligencia de\n\n285\n00:11:34,480 --> 00:11:38,640\ndiferentes tipos.\n\n286\n00:11:36,720 --> 00:11:41,040\nPero, Â¿quÃ© se necesita para generar estos\n\n287\n00:11:38,640 --> 00:11:42,480\ntokens a la escala que necesitamos\n\n288\n00:11:41,040 --> 00:11:46,160\napoyar a todas estas industrias y\n\n289\n00:11:42,480 --> 00:11:48,640\ntodos los que usan IA son estos grandes datos\n\n290\n00:11:46,160 --> 00:11:50,079\ncentros y yo y yo dejamos de llamarlo un\n\n291\n00:11:48,640 --> 00:11:51,360\ncentro de datos porque en realidad no es un\n\n292\n00:11:50,079 --> 00:11:53,040\ncentro de datos. no se trata de no se trata\n\n293\n00:11:51,360 --> 00:11:55,760\nsobre los centros de datos clÃ¡sicos son\n\n294\n00:11:53,040 --> 00:11:57,839\nrecuperando datos. Es un nuevo tipo de datos.\n\n295\n00:11:55,760 --> 00:12:00,000\ncentro y su trabajo es singular para\n\n296\n00:11:57,839 --> 00:12:02,240\nproducir tokens y por eso lo llamo\n\n297\n00:12:00,000 --> 00:12:05,120\nuna fÃ¡brica de IA. SÃ­.\n\n298\n00:12:02,240 --> 00:12:07,120\n>> Y tambiÃ©n usaste token por metro uh uh\n\n299\n00:12:05,120 --> 00:12:08,240\nsobre cÃ³mo serÃ¡n estas fÃ¡bricas de IA\n\n300\n00:12:07,120 --> 00:12:09,920\nutilizado en el futuro.\n\n301\n00:12:08,240 --> 00:12:12,800\n>> Exactamente. Y quÃ© es lo que realmente\n\n302\n00:12:09,920 --> 00:12:16,880\nLo interesante es que es el Ãºltimo.\n\n303\n00:12:12,800 --> 00:12:19,200\nindustria de fÃ¡bricas que crean electrones\n\n304\n00:12:16,880 --> 00:12:19,519\nfue la industria de generaciÃ³n de energÃ­a,\n\n305\n00:12:19,200 --> 00:12:21,839\n>> Â¿verdad?\n\n306\n00:12:19,519 --> 00:12:24,560\n>> Representaba el 30% de la poblaciÃ³n mundial\n\n307\n00:12:21,839 --> 00:12:29,519\neconomÃ­a de uno en uno. quÃ© es\n\n308\n00:12:24,560 --> 00:12:32,880\nproducido a partir de Ã©l se monetiza en\n\n309\n00:12:29,519 --> 00:12:35,040\nya sabes kilovatios hora por dÃ³lar. Ahora\n\n310\n00:12:32,880 --> 00:12:35,760\ntenemos estas ideas llamadas mega tokens\n\n311\n00:12:35,040 --> 00:12:36,240\npor dolar\n\n312\n00:12:35,760 --> 00:12:39,120\n>> correcto\n\n313\n00:12:36,240 --> 00:12:41,680\n>> y sale como electrones otra vez tu\n\n314\n00:12:39,120 --> 00:12:44,959\nsaber y asÃ­ en lugar de electrones puros\n\n315\n00:12:41,680 --> 00:12:46,880\nahora son electrones de valor agregado y nosotros\n\n316\n00:12:44,959 --> 00:12:48,399\nLlÃ¡malos tokens y vamos.\n\n317\n00:12:46,880 --> 00:12:51,040\npara crear esencialmente un nuevo\n\n318\n00:12:48,399 --> 00:12:52,399\nLa industria y esta industria necesita energÃ­a.\n\n319\n00:12:51,040 --> 00:12:54,720\nÂ¿CuÃ¡l es la razÃ³n por la que el Presidente\n\n320\n00:12:52,399 --> 00:12:57,839\nLos pro-energÃ­a y energÃ­a de Trump\n\n321\n00:12:54,720 --> 00:13:00,800\nLa iniciativa de crecimiento es muy oportuna.\n\n322\n00:12:57,839 --> 00:13:03,200\nporque en el momento exacto en que AmÃ©rica\n\n323\n00:13:00,800 --> 00:13:07,920\nquiere ser excelente en AI I y quiere\n\n324\n00:13:03,200 --> 00:13:09,920\nSer un lÃ­der mundial en el ecosistema de IA.\n\n325\n00:13:07,920 --> 00:13:11,600\nSin la energÃ­a que es necesaria para\n\n326\n00:13:09,920 --> 00:13:13,760\ncrear estas fÃ¡bricas de IA, no lo harÃ­amos\n\n327\n00:13:11,600 --> 00:13:16,480\npoder hacer eso. Entonces el el\n\n328\n00:13:13,760 --> 00:13:19,200\nconfluencia de la visiÃ³n del presidente Trump\n\n329\n00:13:16,480 --> 00:13:22,240\ny su su su\n\n330\n00:13:19,200 --> 00:13:25,120\nimpulso para permitir el crecimiento energÃ©tico en nuestra\n\n331\n00:13:22,240 --> 00:13:27,600\nnaciÃ³n y la confluencia de los\n\n332\n00:13:25,120 --> 00:13:30,480\npreparaciÃ³n de la tecnologÃ­a y\n\n333\n00:13:27,600 --> 00:13:32,320\npreparaciÃ³n del mundo para la IA, todo\n\n334\n00:13:30,480 --> 00:13:34,399\nsucediÃ³ exactamente en el momento adecuado, usted\n\n335\n00:13:32,320 --> 00:13:36,000\nlo sÃ©, y asÃ­ esto va a ser\n\n336\n00:13:34,399 --> 00:13:36,560\nva a permitir una nueva industria en\n\n337\n00:13:36,000 --> 00:13:39,680\nfrente a nosotros.\n\n338\n00:13:36,560 --> 00:13:41,839\n>> Um solo una pregunta sobre esa habilitaciÃ³n de\n\n339\n00:13:39,680 --> 00:13:44,079\nnuevas industrias. Mucha gente esta\n\n340\n00:13:41,839 --> 00:13:46,079\nnervioso por el trabajo. SÃ­. Â¿CÃ³mo es esto?\n\n341\n00:13:44,079 --> 00:13:48,160\nÂ¿Va a impactar a la fuerza laboral? Esto es\n\n342\n00:13:46,079 --> 00:13:51,200\nNo es la primera vez que conoces a\n\n343\n00:13:48,160 --> 00:13:53,519\ntecnologÃ­a transformadora masiva que usted\n\n344\n00:13:51,200 --> 00:13:56,320\nsabemos llega a nuestras vidas y crea\n\n345\n00:13:53,519 --> 00:13:59,040\nnuevo trabajo pero tambiÃ©n sabes que hace\n\n346\n00:13:56,320 --> 00:14:01,440\nimpacto en las personas que pierden sus empleos. CÃ³mo\n\n347\n00:13:59,040 --> 00:14:06,680\nÂ¿Ves esta transformaciÃ³n sucediendo?\n\n348\n00:14:01,440 --> 00:14:06,680\nen la fuerza laboral? Nuevas tecnologÃ­as\n\n349\n00:14:06,720 --> 00:14:11,920\ny productividad\n\n350\n00:14:09,519 --> 00:14:13,839\nimpulsa el crecimiento de las industrias y\n\n351\n00:14:11,920 --> 00:14:17,279\ncrea empleos.\n\n352\n00:14:13,839 --> 00:14:19,680\nEn el caso de la electricidad,\n\n353\n00:14:17,279 --> 00:14:21,760\nes una nueva tecnologÃ­a\n\n354\n00:14:19,680 --> 00:14:23,600\ny como mencionÃ© antes, la energÃ­a\n\n355\n00:14:21,760 --> 00:14:26,480\nproducciÃ³n\n\n356\n00:14:23,600 --> 00:14:29,760\nLa industria representÃ³ en algÃºn momento el 30% de\n\n357\n00:14:26,480 --> 00:14:32,240\nla economÃ­a mundial. No sÃ³lo el\n\n358\n00:14:29,760 --> 00:14:33,920\nLa producciÃ³n de energÃ­a era una gran industria.\n\n359\n00:14:32,240 --> 00:14:35,519\nen sÃ­ mismo creando estos poderes\n\n360\n00:14:33,920 --> 00:14:38,399\nPlantas de generaciÃ³n de todo tipo.\n\n361\n00:14:35,519 --> 00:14:41,440\nen todo el mundo, tambiÃ©n permitiÃ³\n\n362\n00:14:38,399 --> 00:14:45,279\nnuevas aplicaciones. Electricidad habilitada\n\n363\n00:14:41,440 --> 00:14:48,320\nbombillas, lavavajillas, frigorÃ­ficos,\n\n364\n00:14:45,279 --> 00:14:50,480\nÂ¿bien? Lavadoras, todo tipo de\n\n365\n00:14:48,320 --> 00:14:52,240\nSe crearon nuevas aplicaciones y todo\n\n366\n00:14:50,480 --> 00:14:54,399\nesas aplicaciones crearon una nueva\n\n367\n00:14:52,240 --> 00:14:57,199\nindustria, creÃ³ empleos.\n\n368\n00:14:54,399 --> 00:14:59,279\nAhora la Ãºltima revoluciÃ³n industrial que\n\n369\n00:14:57,199 --> 00:15:03,040\nEstados Unidos, Â¿sabes?, justo en el\n\n370\n00:14:59,279 --> 00:15:06,079\nen medio estaba la industria de la informaciÃ³n\n\n371\n00:15:03,040 --> 00:15:08,639\nrevoluciÃ³n y esa revoluciÃ³n digital\n\n372\n00:15:06,079 --> 00:15:12,000\npermitiÃ³ que la productividad creciera en los Ãºltimos\n\n373\n00:15:08,639 --> 00:15:14,560\naproximadamente tres dÃ©cadas la productividad ha\n\n374\n00:15:12,000 --> 00:15:17,360\ncrecido alrededor del 80%.\n\n375\n00:15:14,560 --> 00:15:20,160\nAl mismo tiempo, el empleo aumentÃ³ un 80%.\n\n376\n00:15:17,360 --> 00:15:23,360\nY lo mismo ocurre cuando la productividad aumenta\n\n377\n00:15:20,160 --> 00:15:25,120\nel empleo aumenta. Ahora Â¿por quÃ© es eso?\n\n378\n00:15:23,360 --> 00:15:27,760\nDe hecho, se podrÃ­a decir que cuando\n\n379\n00:15:25,120 --> 00:15:30,079\nla productividad aumenta, el empleo\n\n380\n00:15:27,760 --> 00:15:33,199\nbaja porque podrÃ­as hacer mÃ¡s con\n\n381\n00:15:30,079 --> 00:15:36,000\nmenos gente, Â¿verdad? Pero eso es porque\n\n382\n00:15:33,199 --> 00:15:37,839\neso le falta imaginaciÃ³n.\n\n383\n00:15:36,000 --> 00:15:40,160\nSi es su empresa, tomemoslo de\n\n384\n00:15:37,839 --> 00:15:43,120\nla perspectiva de una empresa. si una empresa\n\n385\n00:15:40,160 --> 00:15:46,399\nno tiene nuevas ideas y es literalmente\n\n386\n00:15:43,120 --> 00:15:48,480\nhaciendo una cosa y sÃ³lo una cosa, cuando\n\n387\n00:15:46,399 --> 00:15:50,720\nnuestra productividad aumenta, necesitamos menos\n\n388\n00:15:48,480 --> 00:15:53,120\ngente para hacerlo. Pero si miras\n\n389\n00:15:50,720 --> 00:15:54,800\nNvidia, tenemos tantas ideas. somos nosotros\n\n390\n00:15:53,120 --> 00:15:58,880\nno tengo suficiente tiempo ni gente para ir\n\n391\n00:15:54,800 --> 00:16:01,519\nhazlo. La acumulaciÃ³n de grandes ideas que\n\n392\n00:15:58,880 --> 00:16:03,519\nnos encantarÃ­a ir a intentarlo\n\n393\n00:16:01,519 --> 00:16:05,839\nNuevos mercados y nuevas aplicaciones que nos gustan.\n\n394\n00:16:03,519 --> 00:16:07,360\npara ir a crear el trabajo pendiente de eso es\n\n395\n00:16:05,839 --> 00:16:09,839\nincreÃ­ble.\n\n396\n00:16:07,360 --> 00:16:12,160\nAhora, si tuviera mÃ¡s gente, mÃ¡s\n\n397\n00:16:09,839 --> 00:16:13,680\ntiempo, ya sabes, considerando lo rÃ¡pido que\n\n398\n00:16:12,160 --> 00:16:15,360\nque duro ya estamos trabajando, estoy\n\n399\n00:16:13,680 --> 00:16:17,040\nya funcionando. Si tuviera mÃ¡s\n\n400\n00:16:15,360 --> 00:16:19,680\ngente, si tuviera mÃ¡s tiempo, si fuera\n\n401\n00:16:17,040 --> 00:16:21,040\nMÃ¡s productivos, hacemos mÃ¡s. nuestra empresa\n\n402\n00:16:19,680 --> 00:16:23,120\npodrÃ­a ofrecer mÃ¡s cosas,\n\n403\n00:16:21,040 --> 00:16:25,600\nserÃ­a capaz de inventar nuevas ideas\n\n404\n00:16:23,120 --> 00:16:27,839\nque creÃ³ nuevas industrias. Y asÃ­ el\n\n405\n00:16:25,600 --> 00:16:31,519\nel verdadero el verdadero tema es que eres tÃº\n\n406\n00:16:27,839 --> 00:16:34,320\nÂ¿Eres una persona esperanzada?\n\n407\n00:16:31,519 --> 00:16:37,839\npersona optimista y crees en\n\n408\n00:16:34,320 --> 00:16:39,680\ncreaciÃ³n de ideas o eres alguien que\n\n409\n00:16:37,839 --> 00:16:41,199\ncree que sabes que no hay nuevas ideas\n\n410\n00:16:39,680 --> 00:16:44,000\nizquierda\n\n411\n00:16:41,199 --> 00:16:46,320\ny francamente solo estamos trabajando y\n\n412\n00:16:44,000 --> 00:16:48,959\nsi pudiÃ©ramos hacer ese trabajo mÃ¡s\n\n413\n00:16:46,320 --> 00:16:50,959\nproductivamente nos quedaremos sin trabajo\n\n414\n00:16:48,959 --> 00:16:53,759\nNo creo que haya mucho trabajo por hacer.\n\n415\n00:16:50,959 --> 00:16:55,600\nÂ¿Hay tantas ideas para seguir?\n\n416\n00:16:53,759 --> 00:16:58,320\neso si si puedo hacer trabajo mas\n\n417\n00:16:55,600 --> 00:17:02,160\nefectivamente, simplemente harÃ­a mÃ¡s. Y\n\n418\n00:16:58,320 --> 00:17:06,640\nentonces creo que eso eso eso\n\n419\n00:17:02,160 --> 00:17:07,600\nLa visiÃ³n optimista no es ingenua. Es\n\n420\n00:17:06,640 --> 00:17:08,240\nen realidad la historia,\n\n421\n00:17:07,600 --> 00:17:10,720\n>> Â¿verdad?\n\n422\n00:17:08,240 --> 00:17:13,280\n>> La historia sugerirÃ­a que la humanidad tiene\n\n423\n00:17:10,720 --> 00:17:15,839\nmuchas mÃ¡s ideas para seguir. tenemos un\n\n424\n00:17:13,280 --> 00:17:17,839\nmuchos mÃ¡s desafÃ­os que abordar. si nosotros\n\n425\n00:17:15,839 --> 00:17:21,199\ndeberÃ­amos tener mÃ¡s productividad, nosotros\n\n426\n00:17:17,839 --> 00:17:23,360\nPuedes llegar a Ã©l mucho mÃ¡s rÃ¡pido. Bueno, nosotros\n\n427\n00:17:21,199 --> 00:17:24,959\nuna cosa si pudiera solo una cosa\n\n428\n00:17:23,360 --> 00:17:27,520\nque me gustaria agregar\n\n429\n00:17:24,959 --> 00:17:29,360\n>> y esto es esto es um esto es algo\n\n430\n00:17:27,520 --> 00:17:30,799\neso de lo que tu y yo hemos hablado\n\n431\n00:17:29,360 --> 00:17:36,000\nantes.\n\n432\n00:17:30,799 --> 00:17:37,840\n>> Um, es vital que todos involucren la IA\n\n433\n00:17:36,000 --> 00:17:40,000\nde inmediato.\n\n434\n00:17:37,840 --> 00:17:42,080\nCada adulto,\n\n435\n00:17:40,000 --> 00:17:44,880\ncada persona que trabaja, no trabaja\n\n436\n00:17:42,080 --> 00:17:46,559\npersona, todo niÃ±o debe abordar y\n\n437\n00:17:44,880 --> 00:17:49,120\ninvolucra a la IA de inmediato. Y la razÃ³n de\n\n438\n00:17:46,559 --> 00:17:51,360\neso es porque la IA es la mejor\n\n439\n00:17:49,120 --> 00:17:52,160\nfuerza igualadora de ecualizaciÃ³n.\n\n440\n00:17:51,360 --> 00:17:54,960\n>> Ese es un buen punto.\n\n441\n00:17:52,160 --> 00:17:58,080\n>> Es la primera vez en la historia que un\n\n442\n00:17:54,960 --> 00:18:01,840\nTecnologÃ­a tan increÃ­ble como artificial.\n\n443\n00:17:58,080 --> 00:18:04,240\nLa inteligencia es Ãºtil para alguien que\n\n444\n00:18:01,840 --> 00:18:07,600\nsabe programar software, ya sea\n\n445\n00:18:04,240 --> 00:18:08,960\nprogramas en C++ o Python o no tienes\n\n446\n00:18:07,600 --> 00:18:11,039\nidea de cÃ³mo usar una computadora.\n\n447\n00:18:08,960 --> 00:18:12,880\n>> Â¿Verdad? Esta es la primera vez en\n\n448\n00:18:11,039 --> 00:18:14,720\nhistoria que de repente eso\n\n449\n00:18:12,880 --> 00:18:16,880\nLa computadora es fÃ¡cil de usar. Si no lo haces\n\n450\n00:18:14,720 --> 00:18:19,360\nsaber cÃ³mo usar la IA, simplemente abre el\n\n451\n00:18:16,880 --> 00:18:20,160\nsitio web, vaya a Chad GPT, vaya a Gemini\n\n452\n00:18:19,360 --> 00:18:20,960\nPro, solo di\n\n453\n00:18:20,160 --> 00:18:21,760\n>> haz una pregunta sencilla.\n\n454\n00:18:20,960 --> 00:18:24,000\n>> SÃ­. SÃ­.\n\n455\n00:18:21,760 --> 00:18:26,080\n>> E incluso podrÃ­as decir: \"No tengo idea\n\n456\n00:18:24,000 --> 00:18:26,640\ncÃ³mo utilizar la IA. Â¿Puedes enseÃ±arme cÃ³mo\n\n457\n00:18:26,080 --> 00:18:27,520\nÂ¿quiÃ©n mÃ¡s?\"\n\n458\n00:18:26,640 --> 00:18:29,440\n>> Eso es cierto.\n\n459\n00:18:27,520 --> 00:18:31,440\n>> Y si no sabes escribir, presiona\n\n460\n00:18:29,440 --> 00:18:32,160\nel botÃ³n del micrÃ³fono y habla con nosotros.\n\n461\n00:18:31,440 --> 00:18:34,080\n>> Gordon. SÃ­.\n\n462\n00:18:32,160 --> 00:18:37,200\n>> Y si no entiendes inglÃ©s,\n\n463\n00:18:34,080 --> 00:18:39,120\nPuedes hablar el idioma que quieras.\n\n464\n00:18:37,200 --> 00:18:42,160\n>> Es algo extraordinario.\n\n465\n00:18:39,120 --> 00:18:44,160\n>> Es algo extraordinario. y yo tambiÃ©n\n\n466\n00:18:42,160 --> 00:18:45,440\nCreo que es increÃ­ble que si la IA\n\n467\n00:18:44,160 --> 00:18:48,720\nno sabe ese idioma, le dices al\n\n468\n00:18:45,440 --> 00:18:50,640\nAI voy a aprender ese idioma, Â¿verdad?\n\n469\n00:18:48,720 --> 00:18:52,640\nY asÃ­ creo que creo que todos\n\n470\n00:18:50,640 --> 00:18:55,039\nnecesita involucrar a la IA. es el\n\n471\n00:18:52,640 --> 00:18:57,280\nmayor ecualizaciÃ³n\n\n472\n00:18:55,039 --> 00:18:59,600\num uh fuerza de ecualizaciÃ³n que tenemos\n\n473\n00:18:57,280 --> 00:19:01,039\njamÃ¡s conocido y va a potenciar\n\n474\n00:18:59,600 --> 00:19:02,720\nva a permitir va a levantar\n\n475\n00:19:01,039 --> 00:19:04,640\nsociedad de todos los que conoces en todas partes.\n\n476\n00:19:02,720 --> 00:19:06,480\n>> Estoy de acuerdo contigo. Cambiando ahora a\n\n477\n00:19:04,640 --> 00:19:08,000\nWashington. Estamos, estÃ¡s en Washington.\n\n478\n00:19:06,480 --> 00:19:09,280\nComo dije, pasaste una semana increÃ­ble.\n\n479\n00:19:08,000 --> 00:19:09,840\nTambiÃ©n te reuniste con el presidente.\n\n480\n00:19:09,280 --> 00:19:11,039\nayer.\n\n481\n00:19:09,840 --> 00:19:13,760\n>> SÃ­. primera pregunta.\n\n482\n00:19:11,039 --> 00:19:16,080\n>> Entonces, es increÃ­ble. Pro-innovaciÃ³n,\n\n483\n00:19:13,760 --> 00:19:17,919\ncrecimiento profesional,\n\n484\n00:19:16,080 --> 00:19:19,520\nproenergÃ­a,\n\n485\n00:19:17,919 --> 00:19:25,039\npro\n\n486\n00:19:19,520 --> 00:19:28,559\nuh, la industria quiere que tomemos la IA por el\n\n487\n00:19:25,039 --> 00:19:31,600\ncuernos y ser el lÃ­der mundial, continÃºa\n\n488\n00:19:28,559 --> 00:19:33,440\nser el lÃ­der mundial. Uh, tan orgulloso\n\n489\n00:19:31,600 --> 00:19:35,520\nde nuestro paÃ­s, muy orgullosos de nuestra\n\n490\n00:19:33,440 --> 00:19:38,080\nempresas, muy orgullosos de nuestra gente. Oh\n\n491\n00:19:35,520 --> 00:19:39,760\nsimplemente siempre es sÃ­. yo cada vez que\n\n492\n00:19:38,080 --> 00:19:41,120\nconocerlo, cada vez que estoy con el, yo\n\n493\n00:19:39,760 --> 00:19:41,520\nVuelve, ya sabes, completamente despedido.\n\n494\n00:19:41,120 --> 00:19:44,960\narriba.\n\n495\n00:19:41,520 --> 00:19:47,679\n>> Uh, entonces tengo tres preguntas para ti.\n\n496\n00:19:44,960 --> 00:19:49,919\ntres pÃºblicos diferentes. El numero uno es\n\n497\n00:19:47,679 --> 00:19:51,440\n>> obviamente conociste al presidente\n\n498\n00:19:49,919 --> 00:19:54,480\n>> y estos son memorandos para el presidente\n\n499\n00:19:51,440 --> 00:19:56,720\npodcast. Â¿CuÃ¡l es tu consejo para Ã©l?\n\n500\n00:19:54,480 --> 00:19:59,600\nsobre cuÃ¡les son las cosas que tenemos que hacer\n\n501\n00:19:56,720 --> 00:20:01,679\nahora para seguir adelante?\n\n502\n00:19:59,600 --> 00:20:04,480\nQuiere Estados Unidos.\n\n503\n00:20:01,679 --> 00:20:06,880\nBueno, primero que nada, reconoce que\n\n504\n00:20:04,480 --> 00:20:08,960\nla industria informÃ¡tica\n\n505\n00:20:06,880 --> 00:20:12,640\ny uno que tengo el gran honor de\n\n506\n00:20:08,960 --> 00:20:16,640\nser parte de. La industria informÃ¡tica es\n\n507\n00:20:12,640 --> 00:20:19,039\nEl tesoro nacional de Estados Unidos.\n\n508\n00:20:16,640 --> 00:20:20,640\nEn ninguna otra industria lideramos el\n\n509\n00:20:19,039 --> 00:20:23,520\nmundo\n\n510\n00:20:20,640 --> 00:20:27,520\nal nivel y escala de la computadora\n\n511\n00:20:23,520 --> 00:20:30,320\nindustria. No puedes encontrar otro.\n\n512\n00:20:27,520 --> 00:20:32,159\nPerdimos la industria de las telecomunicaciones.\n\n513\n00:20:30,320 --> 00:20:34,720\nNo hay manera de que vayamos a perder el\n\n514\n00:20:32,159 --> 00:20:35,360\nindustria informÃ¡tica estadounidense y esto\n\n515\n00:20:34,720 --> 00:20:37,039\nindustria informÃ¡tica.\n\n516\n00:20:35,360 --> 00:20:39,360\n>> Habla de 5G porque creo que tenemos\n\n517\n00:20:37,039 --> 00:20:40,000\nTuve esta conversaciÃ³n. Perdimos el 5G\n\n518\n00:20:39,360 --> 00:20:42,400\nola.\n\n519\n00:20:40,000 --> 00:20:44,240\n>> Perdimos la ola 5G. Lo perdimos\n\n520\n00:20:42,400 --> 00:20:46,000\ntecnologÃ­a. Lo perdimos a travÃ©s de la polÃ­tica.\n\n521\n00:20:44,240 --> 00:20:46,799\nLo perdimos por una mala estrategia.\n\n522\n00:20:46,000 --> 00:20:49,280\npensamiento.\n\n523\n00:20:46,799 --> 00:20:51,120\n>> Um es increÃ­ble lo que pasÃ³ y nosotros\n\n524\n00:20:49,280 --> 00:20:52,799\nSimplemente no puedo permitir que eso suceda.\n\n525\n00:20:51,120 --> 00:20:53,840\n>> Y todavÃ­a estamos luchando por recuperar\n\n526\n00:20:52,799 --> 00:20:55,600\nese territorio.\n\n527\n00:20:53,840 --> 00:20:57,280\n>> Va a ser difÃ­cil,\n\n528\n00:20:55,600 --> 00:20:58,960\n>> Â¿verdad? Va a ser muy duro. Nosotros\n\n529\n00:20:57,280 --> 00:21:01,679\nTienes una oportunidad con el 6G, Â¿verdad?\n\n530\n00:20:58,960 --> 00:21:04,559\n>> Porque 6G. AsÃ­ es. Debido a la IA\n\n531\n00:21:01,679 --> 00:21:06,799\ntambiÃ©n. Y entonces vamos a hacer nuestro\n\n532\n00:21:04,559 --> 00:21:07,919\nmejor para ayudar a nuestro paÃ­s a recuperarse\n\n533\n00:21:06,799 --> 00:21:10,880\nliderazgo tecnolÃ³gico y\n\n534\n00:21:07,919 --> 00:21:13,360\ntelecomunicaciones. Pero volviendo a la IA, Ã©l\n\n535\n00:21:10,880 --> 00:21:15,520\nquiere que Estados Unidos sea el\n\n536\n00:21:13,360 --> 00:21:17,840\nlos mejores del mundo. Por supuesto, Ã©l la quiere.\n\n537\n00:21:15,520 --> 00:21:21,120\ncontinuarÃ­a liderando el mundo. En\n\n538\n00:21:17,840 --> 00:21:23,600\npara liderar el mundo en IA porque la IA\n\n539\n00:21:21,120 --> 00:21:25,039\ntrata fundamentalmente de informÃ¡tica y\n\n540\n00:21:23,600 --> 00:21:26,720\nLa informÃ¡tica se trata fundamentalmente de\n\n541\n00:21:25,039 --> 00:21:29,440\ndesarrolladores.\n\n542\n00:21:26,720 --> 00:21:33,039\nEl primer trabajo de liderazgo de una\n\n543\n00:21:29,440 --> 00:21:36,000\nplataforma informÃ¡tica que tambiÃ©n es la IA\n\n544\n00:21:33,039 --> 00:21:38,640\npara ganar a todos los desarrolladores.\n\n545\n00:21:36,000 --> 00:21:41,360\nEl primer trabajo de cualquier plataforma es ganar\n\n546\n00:21:38,640 --> 00:21:43,600\ntodos los desarrolladores. MÃ¡s tarde cuando hablamos de\n\n547\n00:21:41,360 --> 00:21:46,320\n5G, puedo mostrarte exactamente lo mismo\n\n548\n00:21:43,600 --> 00:21:48,880\ncosa. TenÃ­amos una polÃ­tica que nos hizo\n\n549\n00:21:46,320 --> 00:21:51,120\nperder a todos los desarrolladores. Necesitamos tener un\n\n550\n00:21:48,880 --> 00:21:53,919\npolÃ­tica que nos permita ganarlo todo\n\n551\n00:21:51,120 --> 00:21:56,799\ndesarrolladores. 50% de la IA del mundo\n\n552\n00:21:53,919 --> 00:21:59,120\nLos desarrolladores estÃ¡n en China.\n\n553\n00:21:56,799 --> 00:22:01,520\nLos desarrolladores de IA estÃ¡n en todo el mundo.\n\n554\n00:21:59,120 --> 00:22:04,880\nSus desarrolladores de IA ahora estÃ¡n creciendo\n\n555\n00:22:01,520 --> 00:22:07,440\nen Ãfrica, en AmÃ©rica Latina, en\n\n556\n00:22:04,880 --> 00:22:09,280\nSudeste AsiÃ¡tico, en Medio Oriente. AI\n\n557\n00:22:07,440 --> 00:22:11,360\nLos desarrolladores estÃ¡n en todas partes. La razÃ³n\n\n558\n00:22:09,280 --> 00:22:14,559\nLa razÃ³n por la que los desarrolladores de IA estÃ¡n en todas partes es\n\n559\n00:22:11,360 --> 00:22:17,120\nporque cada paÃ­s, cada industria,\n\n560\n00:22:14,559 --> 00:22:18,799\ntoda empresa necesita tener inteligencia\n\n561\n00:22:17,120 --> 00:22:21,520\ny quiere involucrarse artificial\n\n562\n00:22:18,799 --> 00:22:23,679\ninteligencia. Pero comienza con el 50% son\n\n563\n00:22:21,520 --> 00:22:25,360\nen China y necesitamos ganar esos\n\n564\n00:22:23,679 --> 00:22:28,880\ndesarrolladores. Y entonces creo que la primera\n\n565\n00:22:25,360 --> 00:22:31,280\ncosa el que eso lo harÃ­a lo harÃ­a um\n\n566\n00:22:28,880 --> 00:22:33,120\nsigo diciendo cada vez que puedo\n\n567\n00:22:31,280 --> 00:22:36,559\nporque la tecnologÃ­a no es fÃ¡cil de\n\n568\n00:22:33,120 --> 00:22:39,520\nentender es si queremos que Estados Unidos lidere\n\n569\n00:22:36,559 --> 00:22:42,240\nla revoluciÃ³n de la IA y seguir siendo el\n\n570\n00:22:39,520 --> 00:22:45,360\nlÃ­der mundial lo primero que necesitamos es\n\n571\n00:22:42,240 --> 00:22:47,919\ncada desarrollador de IA se base en Estados Unidos\n\n572\n00:22:45,360 --> 00:22:48,480\npila de tecnologÃ­a. La segunda cosa que dirÃ­a\n\n573\n00:22:47,919 --> 00:22:50,960\nes eso\n\n574\n00:22:48,480 --> 00:22:53,280\n>> Creo que tambiÃ©n has dicho\n\n575\n00:22:50,960 --> 00:22:54,559\num quieres establecer el estÃ¡ndar global\n\n576\n00:22:53,280 --> 00:22:55,919\npara la pila de tecnologÃ­a.\n\n577\n00:22:54,559 --> 00:22:58,480\n>> AsÃ­ es. la empresa americana\n\n578\n00:22:55,919 --> 00:23:00,159\ndeberÃ­an ser aquellos que establezcan la norma.\n\n579\n00:22:58,480 --> 00:23:01,200\n>> La pila tecnolÃ³gica estadounidense deberÃ­a ser la\n\n580\n00:23:00,159 --> 00:23:03,120\nestÃ¡ndar mundial.\n\n581\n00:23:01,200 --> 00:23:05,600\n>> AsÃ­ como el dÃ³lar americano es el\n\n582\n00:23:03,120 --> 00:23:08,880\nestÃ¡ndar global por el cual cada paÃ­s\n\n583\n00:23:05,600 --> 00:23:11,520\nse basa en, deberÃ­amos querer que los estadounidenses\n\n584\n00:23:08,880 --> 00:23:15,919\npila de tecnologÃ­a para ser la pila de tecnologÃ­a que el\n\n585\n00:23:11,520 --> 00:23:17,840\nPila de IA sobre la que todos construyen. Ahora\n\n586\n00:23:15,919 --> 00:23:20,720\nla pila tecnolÃ³gica comienza con chips y\n\n587\n00:23:17,840 --> 00:23:22,960\nsistemas. No se trata sÃ³lo de los modelos de IA\n\n588\n00:23:20,720 --> 00:23:25,360\narriba. Hay muchos modelos de IA ademÃ¡s.\n\n589\n00:23:22,960 --> 00:23:26,640\nHay modelos increÃ­bles de todo tipo.\n\n590\n00:23:25,360 --> 00:23:29,440\nAlgunos de ellos son de cÃ³digo abierto, otros\n\n591\n00:23:26,640 --> 00:23:32,080\nestÃ¡n cerrados, algunos de fÃ­sica,\n\n592\n00:23:29,440 --> 00:23:34,799\nalgunos de ellos son para uh cuÃ¡nticos, algunos de\n\n593\n00:23:32,080 --> 00:23:37,520\nSon para comunicaciones. La IA\n\n594\n00:23:34,799 --> 00:23:39,600\nLos modelos son de todos los tipos diferentes. El\n\n595\n00:23:37,520 --> 00:23:42,320\ncosas que haces uh tu iniciativa\n\n596\n00:23:39,600 --> 00:23:44,000\nSe llama AI plus y me encanta. AI\n\n597\n00:23:42,320 --> 00:23:46,880\npara la ciencia, ese modelo es obviamente\n\n598\n00:23:44,000 --> 00:23:50,159\ndiferente a un chatbot. IA para lo cuÃ¡ntico\n\n599\n00:23:46,880 --> 00:23:52,159\nobviamente diferente. IA para 5G y 6G,\n\n600\n00:23:50,159 --> 00:23:53,919\nobviamente diferente, Â¿verdad? Y entonces la IA\n\n601\n00:23:52,159 --> 00:23:57,360\npara la robÃ³tica, obviamente diferente. Y\n\n602\n00:23:53,919 --> 00:24:00,480\nentonces todos estos modelos diferentes son todos IA\n\n603\n00:23:57,360 --> 00:24:03,120\nmodelos y todos deberÃ­an ser construidos\n\n604\n00:24:00,480 --> 00:24:04,960\n>> en la pila tecnolÃ³gica estadounidense. Y asÃ­ el\n\n605\n00:24:03,120 --> 00:24:10,320\nLa segunda cosa que recomendarÃ­a es\n\n606\n00:24:04,960 --> 00:24:13,600\nque la difusiÃ³n de la IA no deberÃ­a limitarse\n\n607\n00:24:10,320 --> 00:24:16,480\nPila de tecnologÃ­a estadounidense para el mundo. AI\n\n608\n00:24:13,600 --> 00:24:19,039\nLa difusiÃ³n debe consistir en maximizar la\n\n609\n00:24:16,480 --> 00:24:21,520\nPila de tecnologÃ­a estadounidense en todo el mundo\n\n610\n00:24:19,039 --> 00:24:24,480\npara que todos los desarrolladores de IA del mundo\n\n611\n00:24:21,520 --> 00:24:26,880\nse basa en el estÃ¡ndar americano. y como\n\n612\n00:24:24,480 --> 00:24:30,240\nsabemos sobre el ecosistema informÃ¡tico,\n\n613\n00:24:26,880 --> 00:24:33,600\nEl ciclo virtual es increÃ­ble. el\n\n614\n00:24:30,240 --> 00:24:35,760\nCuanto mÃ¡s tu tecnologÃ­a estÃ© en todas partes, mÃ¡s\n\n615\n00:24:33,600 --> 00:24:37,440\nmÃ¡s desarrolladores tendrÃ¡s, el\n\n616\n00:24:35,760 --> 00:24:38,880\nmÃ¡s desarrolladores vas a tener,\n\n617\n00:24:37,440 --> 00:24:40,400\ncuanto mÃ¡s serÃ¡ tu tecnologÃ­a\n\n618\n00:24:38,880 --> 00:24:41,679\nen todos lados. Y entonces esto positivo\n\n619\n00:24:40,400 --> 00:24:43,279\nEl sistema de retroalimentaciÃ³n es\n\n620\n00:24:41,679 --> 00:24:45,840\n>> Me limitarÃ© a aprovechar este tema\n\n621\n00:24:43,279 --> 00:24:46,320\nporque hablas mucho de soberano\n\n622\n00:24:45,840 --> 00:24:47,120\nAI.\n\n623\n00:24:46,320 --> 00:24:49,840\n>> SÃ­.\n\n624\n00:24:47,120 --> 00:24:52,159\n>> Â¿QuÃ© quieres decir con eso? Â¿CÃ³mo funciona un\n\n625\n00:24:49,840 --> 00:24:54,640\nÂ¿QuÃ© paÃ­s construirÃ¡ una IA soberana? Â¿Por quÃ©\n\n626\n00:24:52,159 --> 00:24:56,720\nÂ¿Lo necesita? Ya sabes, obviamente el\n\n627\n00:24:54,640 --> 00:24:58,240\nLa forma en que los europeos construirÃ¡n una IA soberana es\n\n628\n00:24:56,720 --> 00:25:00,159\nva a ser diferente de lo africano\n\n629\n00:24:58,240 --> 00:25:02,240\nLos paÃ­ses son grandes. pero estÃ¡s viajando\n\n630\n00:25:00,159 --> 00:25:04,159\nen todo el mundo y abogar por\n\n631\n00:25:02,240 --> 00:25:05,120\nIA soberana. Entonces, Â¿puedes desempacar tu\n\n632\n00:25:04,159 --> 00:25:07,200\nvisiÃ³n sobre esto?\n\n633\n00:25:05,120 --> 00:25:09,919\n>> Estoy defendiendo la tecnologÃ­a estadounidense\n\n634\n00:25:07,200 --> 00:25:11,919\npila para ser la pila tecnolÃ³gica que cada\n\n635\n00:25:09,919 --> 00:25:14,080\nel paÃ­s sigue construyendo. eso es lo que soy\n\n636\n00:25:11,919 --> 00:25:15,520\nabogando por. Y la razÃ³n por la que\n\n637\n00:25:14,080 --> 00:25:17,840\nrazÃ³n por la cual cada paÃ­s necesita construir\n\n638\n00:25:15,520 --> 00:25:19,279\nsu propia pila tecnolÃ³gica, su propia IA, es\n\n639\n00:25:17,840 --> 00:25:20,960\nporque aunque podrÃ­an usar\n\n640\n00:25:19,279 --> 00:25:23,919\nAIS americano, no hay duda. Y\n\n641\n00:25:20,960 --> 00:25:26,159\ndeberÃ­an um cada paÃ­s deberÃ­a usar\n\n642\n00:25:23,919 --> 00:25:29,520\nIA abierta. Cada paÃ­s c deberÃ­a utilizar\n\n643\n00:25:26,159 --> 00:25:31,440\nGÃ©minis. um GÃ©minis de Google y todos\n\n644\n00:25:29,520 --> 00:25:34,240\nEl paÃ­s deberÃ­a usar Grock, ya sabes, y\n\n645\n00:25:31,440 --> 00:25:36,640\ny estos son modelos increÃ­bles y\n\n646\n00:25:34,240 --> 00:25:38,799\nentonces todos los paÃ­ses deberÃ­an usarlos, pero\n\n647\n00:25:36,640 --> 00:25:42,080\nTambiÃ©n deberÃ­an construir el suyo.\n\n648\n00:25:38,799 --> 00:25:44,960\nPila de IA autÃ³ctona y su IA AI\n\n649\n00:25:42,080 --> 00:25:47,600\nmodelos y ese modelo de IA estÃ¡ entrenado en\n\n650\n00:25:44,960 --> 00:25:49,520\nsu idioma, su historia, su\n\n651\n00:25:47,600 --> 00:25:52,480\nconocimiento de su sociedad, su\n\n652\n00:25:49,520 --> 00:25:55,279\ncultura, sus valores. no es sensato\n\n653\n00:25:52,480 --> 00:25:58,240\nque una empresa occidental podrÃ¡\n\n654\n00:25:55,279 --> 00:26:00,559\ncapturar y de alguna manera apreciar profundamente\n\n655\n00:25:58,240 --> 00:26:02,640\napreciar los valores de cada paÃ­s\n\n656\n00:26:00,559 --> 00:26:04,799\ny cada religiÃ³n y cada trasfondo\n\n657\n00:26:02,640 --> 00:26:07,039\ny toda la sociedad que conoces alrededor del\n\n658\n00:26:04,799 --> 00:26:09,120\nmundo. Y asÃ­ cada uno de ellos deberÃ­a ser\n\n659\n00:26:07,039 --> 00:26:11,919\ncapaz de construir algo por sÃ­ solo.\n\n660\n00:26:09,120 --> 00:26:14,559\nY ese modelo de IA funcionarÃ¡ con otros\n\n661\n00:26:11,919 --> 00:26:18,240\nmodelos de IA industrial abiertos o tal vez\n\n662\n00:26:14,559 --> 00:26:20,240\nincluso modelos privados um uh corporativos o\n\n663\n00:26:18,240 --> 00:26:22,640\nsabes especifico\n\n664\n00:26:20,240 --> 00:26:24,799\nmodelos de ciencia industrial o lo que sea\n\n665\n00:26:22,640 --> 00:26:26,159\nes. Pero todos estos modelos van a\n\n666\n00:26:24,799 --> 00:26:28,480\ninteractuar. DeberÃ­an poder construir\n\n667\n00:26:26,159 --> 00:26:29,919\nlos suyos y aÃºn asÃ­ queremos que lo hagan\n\n668\n00:26:28,480 --> 00:26:32,080\naprovechar la pila tecnolÃ³gica estadounidense.\n\n669\n00:26:29,919 --> 00:26:34,480\n>> SÃ­. Has hablado bastante sobre el\n\n670\n00:26:32,080 --> 00:26:36,640\nCompetencia tecnolÃ³gica entre Estados Unidos y China. Entonces quiero\n\n671\n00:26:34,480 --> 00:26:38,080\nobtenga sus puntos de vista sobre cÃ³mo ve el\n\n672\n00:26:36,640 --> 00:26:40,000\ncompetencia. los llamas compaÃ±ero\n\n673\n00:26:38,080 --> 00:26:41,919\ncompetidor, no un cercano sino un igual\n\n674\n00:26:40,000 --> 00:26:43,760\ncompetidor que tiene productos serios,\n\n675\n00:26:41,919 --> 00:26:47,200\nempresas serias. donde crees\n\n676\n00:26:43,760 --> 00:26:52,559\nÂ¿CuÃ¡l es la competencia ahora?\n\n677\n00:26:47,200 --> 00:26:55,840\nUm, primero que nada, China es nuestra\n\n678\n00:26:52,559 --> 00:26:57,600\nCompetidor y adversario, no nuestro enemigo.\n\n679\n00:26:55,840 --> 00:27:00,559\nY la razÃ³n de esto es porque nosotros\n\n680\n00:26:57,600 --> 00:27:02,159\nÂ¿Tenemos profundas interconexiones y\n\n681\n00:27:00,559 --> 00:27:04,960\ninterdependencias entre los dos\n\n682\n00:27:02,159 --> 00:27:08,240\npaÃ­ses.\n\n683\n00:27:04,960 --> 00:27:10,400\nEstados Unidos es increÃ­ble. Nuestro\n\n684\n00:27:08,240 --> 00:27:12,720\nliderazgo tecnolÃ³gico\n\n685\n00:27:10,400 --> 00:27:14,400\nes extraordinario.\n\n686\n00:27:12,720 --> 00:27:17,440\nLa industria informÃ¡tica por la que tengo\n\n687\n00:27:14,400 --> 00:27:20,400\nel honor de servir\n\n688\n00:27:17,440 --> 00:27:24,000\nes el mÃ¡s talentoso, profundamente capaz\n\n689\n00:27:20,400 --> 00:27:27,200\nindustria tecnolÃ³gica jamÃ¡s vista en el mundo.\n\n690\n00:27:24,000 --> 00:27:29,760\nY espero que retengamos\n\n691\n00:27:27,200 --> 00:27:33,360\nnuestra posiciÃ³n de liderazgo durante dÃ©cadas para\n\n692\n00:27:29,760 --> 00:27:36,799\nvenir. Y doy la bienvenida a la competencia.\n\n693\n00:27:33,360 --> 00:27:39,520\nVamos. ya saben, competidores, vengan\n\n694\n00:27:36,799 --> 00:27:42,640\nVamos, vamos a jugar. ese es el americano\n\n695\n00:27:39,520 --> 00:27:46,799\nespÃ­ritu. El espÃ­ritu competitivo que\n\n696\n00:27:42,640 --> 00:27:49,200\ntenemos um no estÃ¡ perdido.\n\n697\n00:27:46,799 --> 00:27:53,120\nY necesitamos la oportunidad como\n\n698\n00:27:49,200 --> 00:27:55,440\nIndustria estadounidense por la que luchar\n\n699\n00:27:53,120 --> 00:27:59,039\nLiderazgo estadounidense.\n\n700\n00:27:55,440 --> 00:28:01,840\nY en un momento en que\n\n701\n00:27:59,039 --> 00:28:03,840\nempresas paÃ­ses alrededor del mundo todos\n\n702\n00:28:01,840 --> 00:28:06,320\ntenemos capacidades, francamente, estamos\n\n703\n00:28:03,840 --> 00:28:08,480\ninterdependientes y dependemos de\n\n704\n00:28:06,320 --> 00:28:10,320\ncapacidades de muchos paÃ­ses. TÃº\n\n705\n00:28:08,480 --> 00:28:12,480\nsabes, cuanto mÃ¡s profundo profundizas, mÃ¡s\n\n706\n00:28:10,320 --> 00:28:14,080\nte das cuenta, te das cuenta que hay cosas\n\n707\n00:28:12,480 --> 00:28:16,399\nen Europa de la que dependemos. Hay\n\n708\n00:28:14,080 --> 00:28:17,919\ncosas en JapÃ³n de las que dependemos. AllÃ¡\n\n709\n00:28:16,399 --> 00:28:19,600\nHay cosas en el sudeste asiÃ¡tico de las que dependemos.\n\n710\n00:28:17,919 --> 00:28:21,840\nen. Hay cosas en AmÃ©rica Latina que\n\n711\n00:28:19,600 --> 00:28:24,320\ndepender de. Ya sabes, cada paÃ­s tiene\n\n712\n00:28:21,840 --> 00:28:26,399\nsu especialidad y sus capacidades.\n\n713\n00:28:24,320 --> 00:28:28,559\nY China, por supuesto, tiene formidables\n\n714\n00:28:26,399 --> 00:28:31,279\ncapacidades. Sus empresas de tecnologÃ­a\n\n715\n00:28:28,559 --> 00:28:33,679\nson formidables. Huawei es formidable.\n\n716\n00:28:31,279 --> 00:28:38,159\nBYD es formidable. estos son increibles\n\n717\n00:28:33,679 --> 00:28:41,440\nempresas. su su\n\n718\n00:28:38,159 --> 00:28:47,279\norgullo nacional en la fabricaciÃ³n\n\n719\n00:28:41,440 --> 00:28:49,760\ny uh profundo, muy profundo y amplio tÃº\n\n720\n00:28:47,279 --> 00:28:51,760\nconocer la escala de experiencia en fabricaciÃ³n\n\n721\n00:28:49,760 --> 00:28:54,640\npuede verse socavado. No se trata de trabajo.\n\n722\n00:28:51,760 --> 00:28:55,039\nEs tecnologÃ­a mÃ¡s artesanÃ­a y mano de obra.\n\n723\n00:28:54,640 --> 00:28:55,520\nescala,\n\n724\n00:28:55,039 --> 00:28:57,279\n>> Â¿verdad?\n\n725\n00:28:55,520 --> 00:29:00,080\n>> La combinaciÃ³n de esas tres cosas\n\n726\n00:28:57,279 --> 00:29:03,279\njuntos es simplemente extraordinario. Es\n\n727\n00:29:00,080 --> 00:29:06,799\nes algo que hay que presenciar. Y entonces nosotros\n\n728\n00:29:03,279 --> 00:29:10,159\nNecesitamos darnos cuenta de que eso somos.\n\n729\n00:29:06,799 --> 00:29:12,480\nahora en un mundo interdependiente. eh y entonces\n\n730\n00:29:10,159 --> 00:29:15,760\nÂ¿QuÃ© hacemos? uh las cosas que nosotros\n\n731\n00:29:12,480 --> 00:29:18,000\ndeberÃ­as hacer uno\n\n732\n00:29:15,760 --> 00:29:20,080\nlo que el presidente Trump\n\n733\n00:29:18,000 --> 00:29:24,960\niniciativa iniciativa del presidente Trump\n\n734\n00:29:20,080 --> 00:29:30,080\nsobre localizar o reindustrializar\n\n735\n00:29:24,960 --> 00:29:33,919\nEstados Unidos es simplemente fantÃ¡stico y y\n\n736\n00:29:30,080 --> 00:29:36,480\niniciativa visionaria y oportuna. necesitamos\n\n737\n00:29:33,919 --> 00:29:38,960\nNecesitamos ser de clase mundial en el\n\n738\n00:29:36,480 --> 00:29:41,120\ntecnologÃ­a de fabricaciÃ³n, la artesanÃ­a\n\n739\n00:29:38,960 --> 00:29:43,760\nde la manufactura y la escala laboral de\n\n740\n00:29:41,120 --> 00:29:46,640\nfabricaciÃ³n. De nuevo, toda esa parte\n\n741\n00:29:43,760 --> 00:29:49,279\nde nuestro ecosistema estÃ¡ algo rezagado y\n\n742\n00:29:46,640 --> 00:29:51,200\nHemos perdido nuestra pasiÃ³n por ello. eh tal vez\n\n743\n00:29:49,279 --> 00:29:52,960\nNo es asÃ­, tal vez sea porque en el pasado.\n\n744\n00:29:51,200 --> 00:29:55,679\nEn los viejos tiempos se trataba mÃ¡s de trabajo.\n\n745\n00:29:52,960 --> 00:29:57,840\nque lo que se trataba de tecnologÃ­a. Um pero ahora\n\n746\n00:29:55,679 --> 00:29:59,600\nes profundamente tÃ©cnico y es algo\n\n747\n00:29:57,840 --> 00:30:02,559\nque realmente podrÃ­amos apasionarnos\n\n748\n00:29:59,600 --> 00:30:04,960\ndetrÃ¡s. Y entonces creo que toda esta Ã¡rea\n\n749\n00:30:02,559 --> 00:30:07,760\nde fabricaciÃ³n para que podamos reducir\n\n750\n00:30:04,960 --> 00:30:10,480\nnuestra dependencia de muchos paÃ­ses alrededor\n\n751\n00:30:07,760 --> 00:30:12,880\nel mundo reduce la temperatura allÃ­\n\n752\n00:30:10,480 --> 00:30:14,640\nTenemos mÃ¡s capacidades nosotros mismos.\n\n753\n00:30:12,880 --> 00:30:16,559\nEs fantÃ¡stico para nuestra seguridad nacional.\n\n754\n00:30:14,640 --> 00:30:20,080\nEs fantÃ¡stico para nuestras industrias. Es\n\n755\n00:30:16,559 --> 00:30:22,399\nGenial para um para la creaciÃ³n de empleo. Es\n\n756\n00:30:20,080 --> 00:30:24,799\nGenial para nuestra cultura, francamente. Es\n\n757\n00:30:22,399 --> 00:30:26,880\nGenial para nuestra sociedad en general. Y entonces yo\n\n758\n00:30:24,799 --> 00:30:29,360\nMe encanta esa visiÃ³n, la del presidente Trump.\n\n759\n00:30:26,880 --> 00:30:31,760\nvisiÃ³n de reindustrializar a Estados Unidos.\n\n760\n00:30:29,360 --> 00:30:33,279\nY entonces creo que tenemos que hacer eso.\n\n761\n00:30:31,760 --> 00:30:35,440\nMientras tanto,\n\n762\n00:30:33,279 --> 00:30:38,880\ntenemos que conseguir mÃ¡s tenemos que quedarnos\n\n763\n00:30:35,440 --> 00:30:41,600\nextraordinariamente excelente en uh Ã¡reas\n\n764\n00:30:38,880 --> 00:30:44,240\ncomo inteligencia artificial y IA\n\n765\n00:30:41,600 --> 00:30:47,440\ninformÃ¡tica y la pila tecnolÃ³gica, por lo que\n\n766\n00:30:44,240 --> 00:30:51,440\nque podrÃ­amos ser socios de cada\n\n767\n00:30:47,440 --> 00:30:53,360\npaÃ­s en el mundo y um uh y y\n\n768\n00:30:51,440 --> 00:30:56,000\nhacer una contribuciÃ³n a cada paÃ­s en\n\n769\n00:30:53,360 --> 00:30:58,159\nel mundo para que pudiÃ©ramos tener esto\n\n770\n00:30:56,000 --> 00:31:01,200\ncontinÃºa usted conoce la interdependencia de\n\n771\n00:30:58,159 --> 00:31:02,640\nel uno al otro y uh uh ya sabes conducir nuestro\n\n772\n00:31:01,200 --> 00:31:04,559\nindustria sencilla.\n\n773\n00:31:02,640 --> 00:31:07,279\n>> Ãšltima pregunta. Jensen, hablas de\n\n774\n00:31:04,559 --> 00:31:10,640\nrecuperar la confianza estratÃ©gica. Es esto\n\n775\n00:31:07,279 --> 00:31:12,960\nlo que eso significa en tus palabras, liderar,\n\n776\n00:31:10,640 --> 00:31:16,480\nseguir inventando? Y al final de este\n\n777\n00:31:12,960 --> 00:31:18,240\ndÃ©cada, las tecnologÃ­as estadounidenses han construido\n\n778\n00:31:16,480 --> 00:31:20,399\nla infraestructura global tanto en el\n\n779\n00:31:18,240 --> 00:31:20,880\nhardware de pila tecnolÃ³gica, pero ademÃ¡s\n\n780\n00:31:20,399 --> 00:31:22,960\nsoftware.\n\n781\n00:31:20,880 --> 00:31:25,600\n>> SÃ­, exactamente. ya sabes, la mayorÃ­a de\n\n782\n00:31:22,960 --> 00:31:27,200\ntiempo la mayor parte del tiempo regulaciÃ³n um\n\n783\n00:31:25,600 --> 00:31:31,520\npolÃ­ticas\n\n784\n00:31:27,200 --> 00:31:34,640\nuh uh tiende a ser tiende a tiende a\n\n785\n00:31:31,520 --> 00:31:36,559\ncentrarse demasiado en limitar y\n\n786\n00:31:34,640 --> 00:31:38,720\nrestringiendo.\n\n787\n00:31:36,559 --> 00:31:42,640\nUm y eso eso es eso estÃ¡ bien\n\n788\n00:31:38,720 --> 00:31:46,320\nhacer. Um, sÃ³lo quiero recordar recordar\n\n789\n00:31:42,640 --> 00:31:47,840\nuh nosotros mismos que Estados Unidos es\n\n790\n00:31:46,320 --> 00:31:50,000\nextraordinario\n\n791\n00:31:47,840 --> 00:31:51,360\ny que las empresas aquÃ­ y yo tenemos\n\n792\n00:31:50,000 --> 00:31:53,440\nel beneficio de trabajar con empresas\n\n793\n00:31:51,360 --> 00:31:55,279\npor todo el mundo. Nvidia es obviamente\n\n794\n00:31:53,440 --> 00:31:57,600\nuna empresa global. tenemos tenemos\n\n795\n00:31:55,279 --> 00:32:00,080\nnegocios en todo el mundo. Puedo\n\n796\n00:31:57,600 --> 00:32:03,600\ndar fe de que este paÃ­s es\n\n797\n00:32:00,080 --> 00:32:06,640\nextraordinario tiene extraordinario\n\n798\n00:32:03,600 --> 00:32:12,399\nÃ©tica del trabajo. Francamente, creo que los estadounidenses\n\n799\n00:32:06,640 --> 00:32:15,440\ntrabajar, si no tan duro como cualquier trabajador\n\n800\n00:32:12,399 --> 00:32:16,960\ncultura en el mundo, pero considero\n\n801\n00:32:15,440 --> 00:32:19,279\nmuchas empresas americanas y muchas\n\n802\n00:32:16,960 --> 00:32:22,480\nindustrias, trabajamos mÃ¡s duro que todos\n\n803\n00:32:19,279 --> 00:32:25,840\nindustria. Y asÃ­ trabajan los estadounidenses, nosotros\n\n804\n00:32:22,480 --> 00:32:29,039\ntenemos Ã©tica de trabajo, tenemos increÃ­bles\n\n805\n00:32:25,840 --> 00:32:30,559\neh eh\n\n806\n00:32:29,039 --> 00:32:32,480\nbase\n\n807\n00:32:30,559 --> 00:32:36,000\nuh por apoyar a la industria y\n\n808\n00:32:32,480 --> 00:32:38,480\napoyando a las startups. Y uh es un es\n\n809\n00:32:36,000 --> 00:32:39,840\nsigue siendo el mejor lugar del mundo para\n\n810\n00:32:38,480 --> 00:32:41,519\ninmigrantes por venir.\n\n811\n00:32:39,840 --> 00:32:43,120\n>> SÃ­.\n\n812\n00:32:41,519 --> 00:32:44,880\nSigue siendo el mejor lugar del mundo para\n\n813\n00:32:43,120 --> 00:32:47,840\ninmigrantes que vengan a disfrutar de una gran\n\n814\n00:32:44,880 --> 00:32:50,320\neducaciÃ³n y tener la oportunidad con\n\n815\n00:32:47,840 --> 00:32:53,919\ntodo el ecosistema que nos rodea para construir un\n\n816\n00:32:50,320 --> 00:32:56,080\ngran empresa. Lo vi de primera mano.\n\n817\n00:32:53,919 --> 00:32:59,120\nYa sabes, nadie ha disfrutado del\n\n818\n00:32:56,080 --> 00:33:01,039\nsueÃ±o americano y lo vi personalmente en\n\n819\n00:32:59,120 --> 00:33:03,919\nmi vida\n\n820\n00:33:01,039 --> 00:33:06,080\nque yo. Ya sabes, si hay un\n\n821\n00:33:03,919 --> 00:33:07,919\nlibro que se llama el sueÃ±o americano,\n\n822\n00:33:06,080 --> 00:33:10,399\nPodrÃ­a ser uno de los capÃ­tulos,\n\n823\n00:33:07,919 --> 00:33:12,960\nÂ¿bien? ya sabes, y entonces esto es esto\n\n824\n00:33:10,399 --> 00:33:16,880\nes que encarno el sueÃ±o americano y y\n\n825\n00:33:12,960 --> 00:33:21,840\nasÃ­ que yo camino con extraordinario orgullo\n\n826\n00:33:16,880 --> 00:33:24,399\ny genial y gratitud um y uh uh y\n\n827\n00:33:21,840 --> 00:33:28,880\nreconociendo cual es la magia del de\n\n828\n00:33:24,399 --> 00:33:32,640\nAmÃ©rica y uh y tambiÃ©n uh un gran\n\n829\n00:33:28,880 --> 00:33:35,360\nconfianza en lo que podemos hacer y\n\n830\n00:33:32,640 --> 00:33:38,320\nentonces creo que cualesquiera que sean las polÃ­ticas\n\n831\n00:33:35,360 --> 00:33:41,039\nse crean se desarrollan\n\n832\n00:33:38,320 --> 00:33:42,640\nes darse cuenta, ya sabes, y esto no es\n\n833\n00:33:41,039 --> 00:33:44,480\ndiferente a las empresas que desarrollan\n\n834\n00:33:42,640 --> 00:33:46,960\nestrategias. antes de desarrollar\n\n835\n00:33:44,480 --> 00:33:49,039\nestrategias sobre el adversario o el\n\n836\n00:33:46,960 --> 00:33:52,159\ncompetencia, lo primero que tienes que hacer\n\n837\n00:33:49,039 --> 00:33:55,120\nlo que debes hacer es conocerte a ti mismo.\n\n838\n00:33:52,159 --> 00:33:58,559\nY las estrategias que implementas\n\n839\n00:33:55,120 --> 00:34:00,720\ncuando estÃ¡s en defensa contra el\n\n840\n00:33:58,559 --> 00:34:04,480\nestrategias que implementas y polÃ­ticas\n\n841\n00:34:00,720 --> 00:34:06,880\nque despliegas cuando estÃ¡s en ofensiva son\n\n842\n00:34:04,480 --> 00:34:08,879\nrelacionados, no iguales. Y asÃ­ es\n\n843\n00:34:06,880 --> 00:34:11,280\nrealmente importante para tener una idea de\n\n844\n00:34:08,879 --> 00:34:14,560\ncuÃ¡les son nuestras capacidades nacionales\n\n845\n00:34:11,280 --> 00:34:17,359\nson y especialmente en el campo de\n\n846\n00:34:14,560 --> 00:34:19,440\ninteligencia artificial y computaciÃ³n uh\n\n847\n00:34:17,359 --> 00:34:21,440\nreconocer lo extraordinario\n\n848\n00:34:19,440 --> 00:34:24,879\nindustria que hemos creado de alguna manera a lo largo del\n\n849\n00:34:21,440 --> 00:34:27,679\naÃ±os y es nuestro tesoro nacional.\n\n850\n00:34:24,879 --> 00:34:31,200\nDeberÃ­amos hacer todo lo que podamos para\n\n851\n00:34:27,679 --> 00:34:33,440\npromover esta capacidad para nutrir esta\n\n852\n00:34:31,200 --> 00:34:36,560\ncapacidad para proteger esta capacidad\n\n853\n00:34:33,440 --> 00:34:38,240\ny potenciarlo. Y entonces no puedo decirte\n\n854\n00:34:36,560 --> 00:34:41,599\nQue orgulloso, ya sabes, aquÃ­ estoy.\n\n855\n00:34:38,240 --> 00:34:44,079\nWashington DC, uh, y, uh, nuestra naciÃ³n\n\n856\n00:34:41,599 --> 00:34:46,320\ncapital. Uh, es difÃ­cil, es difÃ­cil\n\n857\n00:34:44,079 --> 00:34:50,320\nno sentirte patriÃ³tico despuÃ©s de ver el\n\n858\n00:34:46,320 --> 00:34:52,960\npresidente y y, um, pero es un a\n\n859\n00:34:50,320 --> 00:34:56,000\ngran recordatorio de que paÃ­s tan maravilloso\n\n860\n00:34:52,960 --> 00:34:58,160\nde alguna manera hemos construido a lo largo de los aÃ±os y\n\n861\n00:34:56,000 --> 00:35:00,320\nque industria que ha sido la que ha\n\n862\n00:34:58,160 --> 00:35:02,320\ncomo resultado surgiÃ³ de Ã©l. y tenemos\n\n863\n00:35:00,320 --> 00:35:04,079\ncada razÃ³n para estar orgulloso y cada\n\n864\n00:35:02,320 --> 00:35:06,800\nrazÃ³n para tener confianza. De esto\n\n865\n00:35:04,079 --> 00:35:09,520\nincreÃ­ble uh Ãºltima nota, gracias por\n\n866\n00:35:06,800 --> 00:35:11,040\nRealmente soy un invitado en nuestro programa.\n\n867\n00:35:09,520 --> 00:35:12,800\nRealmente lo aprecio. Valoramos el\n\n868\n00:35:11,040 --> 00:35:14,400\nasociaciÃ³n que tenemos con Nvidia y yo\n\n869\n00:35:12,800 --> 00:35:15,599\nEsperamos muchas mÃ¡s conversaciones.\n\n870\n00:35:14,400 --> 00:35:19,470\ncontigo. Gracias Jensen.\n\n871\n00:35:15,599 --> 00:35:22,720\n>> Gracias. Es genial estar aquÃ­.\n\n872\n00:35:19,470 --> 00:35:24,320\n[MÃºsica]\n\n873\n00:35:22,720 --> 00:35:27,520\n>> Gracias por ver mi entrevista con\n\n874\n00:35:24,320 --> 00:35:29,520\nel director ejecutivo de Nvidia, Jensen Wong. te espero\n\n875\n00:35:27,520 --> 00:35:32,320\ndisfrutÃ© la conversaciÃ³n tanto como yo\n\n876\n00:35:29,520 --> 00:35:33,680\nhizo. Mientras te tengo yo tambiÃ©n querÃ­a\n\n877\n00:35:32,320 --> 00:35:35,760\nhacerle saber que el especial\n\n878\n00:35:33,680 --> 00:35:39,359\nproyecto de estudios competitivos es\n\n879\n00:35:35,760 --> 00:35:42,560\norganizando una cumbre de un dÃ­a de duraciÃ³n sobre AI plus\n\n880\n00:35:39,359 --> 00:35:45,040\nciencia el 23 de julio aquÃ­ en Washington\n\n881\n00:35:42,560 --> 00:35:47,359\nCORRIENTE CONTINUA. AsÃ­ que regÃ­strate si eres\n\n882\n00:35:45,040 --> 00:35:49,040\ninteresados â€‹â€‹en asistir y ser parte\n\n883\n00:35:47,359 --> 00:35:52,640\nde conversaciones increÃ­bles que tenemos\n\n884\n00:35:49,040 --> 00:35:55,440\nplanificado. Por Ãºltimo, en marzo de este aÃ±o,\n\n885\n00:35:52,640 --> 00:35:58,079\nlanzamos un curso Genai para nacionales\n\n886\n00:35:55,440 --> 00:36:00,240\nseguridad en colaboraciÃ³n con Corsera.\n\n887\n00:35:58,079 --> 00:36:02,400\nEs un curso increÃ­ble. MÃ¡s que\n\n888\n00:36:00,240 --> 00:36:04,480\nSe han inscrito 3.000 personas. tu puedes ser\n\n889\n00:36:02,400 --> 00:36:09,640\nuno de ellos si empiezas a hacer eso\n\n890\n00:36:04,480 --> 00:36:09,640\nhoy. Espero que lo disfrutes. Gracias.\n","category":"Talks","date":1767308662700},
{"id":"e6Uq_5JemrI","title":"James Cameron Special Video Message","srtEn":"1\n00:00:00,760 --> 00:00:05,279\nuh greetings everyone Jim Cameron here\n\n2\n00:00:03,080 --> 00:00:07,639\nand I'm videoing in from New Zealand\n\n3\n00:00:05,279 --> 00:00:11,599\nwhere I'm finishing Avatar\n\n4\n00:00:07,639 --> 00:00:15,120\n3 okay so I'm not an AI researcher or\n\n5\n00:00:11,599 --> 00:00:17,960\nexpert at all I'm just a Storyteller but\n\n6\n00:00:15,120 --> 00:00:21,320\nI'm here today because my passion for AI\n\n7\n00:00:17,960 --> 00:00:24,240\nand Robotics goes far beyond the big\n\n8\n00:00:21,320 --> 00:00:27,119\nscreen I'm fascinated by technology how\n\n9\n00:00:24,240 --> 00:00:30,000\nit shapes our world where it's headed\n\n10\n00:00:27,119 --> 00:00:31,800\nits impact on society and I have been\n\n11\n00:00:30,000 --> 00:00:34,520\nsince I was a kid reading every science\n\n12\n00:00:31,800 --> 00:00:36,440\nfiction book I could get my hands on\n\n13\n00:00:34,520 --> 00:00:39,360\nI've pushed Tech boundaries myself as a\n\n14\n00:00:36,440 --> 00:00:41,680\nmeans to my storytelling and also as an\n\n15\n00:00:39,360 --> 00:00:44,520\nExplorer I've engineered robotic\n\n16\n00:00:41,680 --> 00:00:46,440\nvehicles for my deep ocean Expeditions\n\n17\n00:00:44,520 --> 00:00:50,640\nbut they were remotely piloted Vehicles\n\n18\n00:00:46,440 --> 00:00:52,840\nthere was no AI involved so this Fusion\n\n19\n00:00:50,640 --> 00:00:54,760\nof AI and Robotics that's happening\n\n20\n00:00:52,840 --> 00:00:57,480\nright now is one of the most thrilling\n\n21\n00:00:54,760 --> 00:00:59,280\ntechnological leaps of my lifetime we're\n\n22\n00:00:57,480 --> 00:01:02,199\nno longer just building machines that\n\n23\n00:00:59,280 --> 00:01:05,479\nexecute mans were designing systems that\n\n24\n00:01:02,199 --> 00:01:08,720\ncan learn adapt and even evolve on their\n\n25\n00:01:05,479 --> 00:01:11,560\nown and I'm a huge fan of what Ai and\n\n26\n00:01:08,720 --> 00:01:13,840\nRobotics can do for Society at large but\n\n27\n00:01:11,560 --> 00:01:16,840\nespecially in my own two areas of of\n\n28\n00:01:13,840 --> 00:01:20,000\npersonal passion art and storytelling on\n\n29\n00:01:16,840 --> 00:01:22,439\nthe one hand and Science and exploration\n\n30\n00:01:20,000 --> 00:01:25,360\non the other and I don't believe in\n\n31\n00:01:22,439 --> 00:01:27,720\nbeing a lite I see a lot of my Hollywood\n\n32\n00:01:25,360 --> 00:01:31,280\npeers acting like a mob with pitchforks\n\n33\n00:01:27,720 --> 00:01:35,040\nand torches but no Genie goes back in\n\n34\n00:01:31,280 --> 00:01:37,040\nthe bottle once it's out so I'm gungho\n\n35\n00:01:35,040 --> 00:01:39,720\nI'm leaning in I plan to be at the\n\n36\n00:01:37,040 --> 00:01:42,159\nLeading Edge of applying AI to my\n\n37\n00:01:39,720 --> 00:01:43,960\nstorytelling just as I was a leader of\n\n38\n00:01:42,159 --> 00:01:46,920\nthe charge into computer generated\n\n39\n00:01:43,960 --> 00:01:50,159\nimagery 32 years ago when I founded the\n\n40\n00:01:46,920 --> 00:01:54,079\nfirst All Digital VFX\n\n41\n00:01:50,159 --> 00:01:57,520\ncompany but I'm also here today because\n\n42\n00:01:54,079 --> 00:02:00,119\nI'm the Skynet guy 40 years ago I made\n\n43\n00:01:57,520 --> 00:02:03,719\nthe Terminator and it's emerged recently\n\n44\n00:02:00,119 --> 00:02:06,920\nas the kind of poster child for AI gone\n\n45\n00:02:03,719 --> 00:02:09,959\nwrong every time I go to some AI\n\n46\n00:02:06,920 --> 00:02:12,520\nconclave whenever I put my hand up the\n\n47\n00:02:09,959 --> 00:02:16,040\nresearchers all laugh before I've even\n\n48\n00:02:12,520 --> 00:02:19,160\nsaid anything because because the Skynet\n\n49\n00:02:16,040 --> 00:02:22,519\nproblem is an actual thing I see it in\n\n50\n00:02:19,160 --> 00:02:25,280\narticles almost every day and this study\n\n51\n00:02:22,519 --> 00:02:27,920\ngroup has a focus on National Security\n\n52\n00:02:25,280 --> 00:02:30,519\nwhich has enormous implications for AI\n\n53\n00:02:27,920 --> 00:02:32,480\nand Robotics\n\n54\n00:02:30,519 --> 00:02:34,599\na robot you know whatever its form a\n\n55\n00:02:32,480 --> 00:02:37,120\nwheeled vehicle an aerial drone a\n\n56\n00:02:34,599 --> 00:02:40,239\nwalking machine it is a means of\n\n57\n00:02:37,120 --> 00:02:41,920\nembodiment for AI you're taking a\n\n58\n00:02:40,239 --> 00:02:45,519\ndecision-making engine and you're giving\n\n59\n00:02:41,920 --> 00:02:47,840\nit physical agency in the real world I'm\n\n60\n00:02:45,519 --> 00:02:50,239\ngoing to assume the focus to today is on\n\n61\n00:02:47,840 --> 00:02:53,159\nmobile platforms not AI controlling\n\n62\n00:02:50,239 --> 00:02:55,440\npower grids or fixed base industrial\n\n63\n00:02:53,159 --> 00:02:58,120\nrobots we're talking about autonomous\n\n64\n00:02:55,440 --> 00:03:01,480\nplatforms that make their own decisions\n\n65\n00:02:58,120 --> 00:03:03,799\nand embodied synthetic intelligence and\n\n66\n00:03:01,480 --> 00:03:06,920\nthis can be as simple as an amoeba you\n\n67\n00:03:03,799 --> 00:03:08,760\nknow like a Roomba or ultimately more\n\n68\n00:03:06,920 --> 00:03:10,920\nsophisticated processing up to\n\n69\n00:03:08,760 --> 00:03:14,840\ntheoretically including true\n\n70\n00:03:10,920 --> 00:03:18,680\nConsciousness whatever we agree that is\n\n71\n00:03:14,840 --> 00:03:21,720\nAgi with self-awareness with ego with\n\n72\n00:03:18,680 --> 00:03:24,680\npurpose and we're on a steep curve of\n\n73\n00:03:21,720 --> 00:03:27,040\nfaster denser chips increasing compute\n\n74\n00:03:24,680 --> 00:03:29,840\nand an equally steep curve in the\n\n75\n00:03:27,040 --> 00:03:32,439\ncapabilities of the machine platforms\n\n76\n00:03:29,840 --> 00:03:34,480\nlike the you know D dancing robots at\n\n77\n00:03:32,439 --> 00:03:36,879\nBoston Dynamics you know bipeds and\n\n78\n00:03:34,480 --> 00:03:40,879\nquadrip heads rocking out uh pretty\n\n79\n00:03:36,879 --> 00:03:43,519\nstunning display so Aid driven robotics\n\n80\n00:03:40,879 --> 00:03:47,599\ncan process complex situations and even\n\n81\n00:03:43,519 --> 00:03:50,519\nrespond now with human affect llms give\n\n82\n00:03:47,599 --> 00:03:54,079\nit the ability to simulate cognition and\n\n83\n00:03:50,519 --> 00:03:56,439\ninteract naturally with people embodied\n\n84\n00:03:54,079 --> 00:03:59,079\nAI could be a nurse could be a robot\n\n85\n00:03:56,439 --> 00:04:03,360\ntaxi could be a caregiver to an elderly\n\n86\n00:03:59,079 --> 00:04:05,040\nperson a nanny to a child a teacher um\n\n87\n00:04:03,360 --> 00:04:07,519\nit could be a rescue bot going through\n\n88\n00:04:05,040 --> 00:04:09,360\nthe debris of an earthquake um could be\n\n89\n00:04:07,519 --> 00:04:13,000\nan Arial drone running a search pattern\n\n90\n00:04:09,360 --> 00:04:15,200\nfor the heat signature of a lost hiker\n\n91\n00:04:13,000 --> 00:04:17,639\nor it could be a weapon platform\n\n92\n00:04:15,200 --> 00:04:20,000\noperating Aon autonomously in a battle\n\n93\n00:04:17,639 --> 00:04:22,800\ntheater looking for the heat signature\n\n94\n00:04:20,000 --> 00:04:25,080\nof an enemy combatant the question of\n\n95\n00:04:22,800 --> 00:04:27,919\nthe hour is should an autonomous\n\n96\n00:04:25,080 --> 00:04:30,240\nplatform be given its own kill\n\n97\n00:04:27,919 --> 00:04:32,919\nAuthority the war in Ukraine train shows\n\n98\n00:04:30,240 --> 00:04:35,639\nus the future in the starkest terms the\n\n99\n00:04:32,919 --> 00:04:38,400\nbroad use of lethal aerial drones some\n\n100\n00:04:35,639 --> 00:04:40,880\nexpensive some cheap consumer ones\n\n101\n00:04:38,400 --> 00:04:44,320\nthey're dropping RPGs taking out tanks\n\n102\n00:04:40,880 --> 00:04:47,600\nand tire tank crews even Dragon drones\n\n103\n00:04:44,320 --> 00:04:49,720\nspraying thermite on Russian positions\n\n104\n00:04:47,600 --> 00:04:52,960\nbut these are fpvs they're first-person\n\n105\n00:04:49,720 --> 00:04:55,600\nview drones piloted by a human the human\n\n106\n00:04:52,960 --> 00:04:58,280\nin the loop in moral terms is the\n\n107\n00:04:55,600 --> 00:05:00,400\ndecision-making combatant he or she has\n\n108\n00:04:58,280 --> 00:05:02,720\nthe kill Authority and the Drone is an\n\n109\n00:05:00,400 --> 00:05:04,840\nextension of their will and if you take\n\n110\n00:05:02,720 --> 00:05:06,800\nall away all the layers of Technology\n\n111\n00:05:04,840 --> 00:05:08,039\nthis is no different than an Archer at\n\n112\n00:05:06,800 --> 00:05:10,960\nthe Battle of\n\n113\n00:05:08,039 --> 00:05:13,160\nHastings each time a human life is taken\n\n114\n00:05:10,960 --> 00:05:15,880\nby such a machine there's an ethical\n\n115\n00:05:13,160 --> 00:05:18,319\nchain that stretches backwards diffusing\n\n116\n00:05:15,880 --> 00:05:21,479\nup through many individuals and and\n\n117\n00:05:18,319 --> 00:05:23,120\ngroups of people behind the pilot that\n\n118\n00:05:21,479 --> 00:05:25,400\nfires the missile or the soldier who\n\n119\n00:05:23,120 --> 00:05:27,160\npulls the trigger on a rifle there are\n\n120\n00:05:25,400 --> 00:05:30,120\ncommanding officers who give The Kill\n\n121\n00:05:27,160 --> 00:05:32,520\nOrder in Broad general terms by sending\n\n122\n00:05:30,120 --> 00:05:35,759\nthem in as autonomous agents to engage\n\n123\n00:05:32,520 --> 00:05:37,880\nthe enemy and the entire military behind\n\n124\n00:05:35,759 --> 00:05:39,600\nthem which rewards those actions and\n\n125\n00:05:37,880 --> 00:05:41,759\nbeyond that the societies and\n\n126\n00:05:39,600 --> 00:05:44,280\ngovernments that have agreed by\n\n127\n00:05:41,759 --> 00:05:47,080\nconsensus that the deaths are necessary\n\n128\n00:05:44,280 --> 00:05:50,000\nfor National Security and as you go up\n\n129\n00:05:47,080 --> 00:05:53,039\nthat chain the moral and ethical burden\n\n130\n00:05:50,000 --> 00:05:56,160\nbecomes more diffuse and less specific\n\n131\n00:05:53,039 --> 00:05:58,319\nto the actual moment of the trigger pull\n\n132\n00:05:56,160 --> 00:06:00,360\nand acts as a kind of moral Absolution\n\n133\n00:05:58,319 --> 00:06:01,880\nof the person pulling the the trigger\n\n134\n00:06:00,360 --> 00:06:04,160\nI'm just following\n\n135\n00:06:01,880 --> 00:06:05,680\norders none of those up the chain are\n\n136\n00:06:04,160 --> 00:06:08,199\npresent to decide the fate of an\n\n137\n00:06:05,680 --> 00:06:10,039\nindividual who's in the crosshairs but\n\n138\n00:06:08,199 --> 00:06:12,800\nthey create a framework that enables and\n\n139\n00:06:10,039 --> 00:06:15,080\ndemands that individual's death and the\n\n140\n00:06:12,800 --> 00:06:18,000\nguy pulling the trigger is in many ways\n\n141\n00:06:15,080 --> 00:06:19,960\nan organic robotic platform highly\n\n142\n00:06:18,000 --> 00:06:21,520\ntrained to perform the task and ordered\n\n143\n00:06:19,960 --> 00:06:24,400\nby those in the chain of command to make\n\n144\n00:06:21,520 --> 00:06:27,039\nthe kill human autonomous decision-\n\n145\n00:06:24,400 --> 00:06:30,199\nmaking relies heavily at that trigger\n\n146\n00:06:27,039 --> 00:06:31,840\npoint on rules you don't kill C Ian you\n\n147\n00:06:30,199 --> 00:06:34,680\ndon't kill children you don't kill an\n\n148\n00:06:31,840 --> 00:06:36,759\nenemy that's surrendering and so on and\n\n149\n00:06:34,680 --> 00:06:39,360\nrules that are codified in the Geneva\n\n150\n00:06:36,759 --> 00:06:43,400\nConvention and each military has its own\n\n151\n00:06:39,360 --> 00:06:46,240\nrules of engagement So in theory an AI\n\n152\n00:06:43,400 --> 00:06:47,240\ncan be given the same constraints a\n\n153\n00:06:46,240 --> 00:06:49,919\nrules-based\n\n154\n00:06:47,240 --> 00:06:52,120\nsystem and if its senses are sharper and\n\n155\n00:06:49,919 --> 00:06:55,039\nits reaction time is faster and its\n\n156\n00:06:52,120 --> 00:06:57,800\ntargeting is more precise then in theory\n\n157\n00:06:55,039 --> 00:06:59,840\nthe AI will perform the task with\n\n158\n00:06:57,800 --> 00:07:02,520\ngreater discrimination than a human\n\n159\n00:06:59,840 --> 00:07:04,759\ncould certainly we can imagine an AI\n\n160\n00:07:02,520 --> 00:07:07,039\nthat emotionlessly performs in the\n\n161\n00:07:04,759 --> 00:07:10,400\nintensity of battle much better than a\n\n162\n00:07:07,039 --> 00:07:14,400\nscared stressed out exhausted human Warf\n\n163\n00:07:10,400 --> 00:07:17,360\nfighter so what if embodying Advanced AI\n\n164\n00:07:14,400 --> 00:07:20,039\nI'm not talking about AGI yet into\n\n165\n00:07:17,360 --> 00:07:22,840\nrobotic weapon platforms could allow\n\n166\n00:07:20,039 --> 00:07:25,479\nhighly surgical strikes that reduce\n\n167\n00:07:22,840 --> 00:07:27,919\ncollateral damage Maybe by orders of\n\n168\n00:07:25,479 --> 00:07:29,680\nmagnitude reduce Friendly Fire\n\n169\n00:07:27,919 --> 00:07:31,520\ncasualties\n\n170\n00:07:29,680 --> 00:07:34,479\nand AI is goal oriented it makes no\n\n171\n00:07:31,520 --> 00:07:36,840\nmoral judgment about its adversary and\n\n172\n00:07:34,479 --> 00:07:38,639\nwhen it was found in World War II that\n\n173\n00:07:36,840 --> 00:07:41,120\nrelatively few of the rounds fired were\n\n174\n00:07:38,639 --> 00:07:44,120\nactually aimed at human targets the US\n\n175\n00:07:41,120 --> 00:07:47,120\nmilitary changed its training it became\n\n176\n00:07:44,120 --> 00:07:49,759\ncritical to dehumanize the enemy in\n\n177\n00:07:47,120 --> 00:07:52,759\nVietnam the adversaries were dinks\n\n178\n00:07:49,759 --> 00:07:55,879\nslopes Gos in Iraq and Afghanistan they\n\n179\n00:07:52,759 --> 00:07:59,520\nwere terrorists towel heads hajis not\n\n180\n00:07:55,879 --> 00:08:01,720\npeople like you and me and AI doesn't\n\n181\n00:07:59,520 --> 00:08:04,720\nacquire a dehumanized enemy because it\n\n182\n00:08:01,720 --> 00:08:07,599\nalready couldn't care less it may sound\n\n183\n00:08:04,720 --> 00:08:10,080\njust like us on chat GPT but it's a\n\n184\n00:08:07,599 --> 00:08:13,400\nstochastic parrot it's a human Simulator\n\n185\n00:08:10,080 --> 00:08:15,479\nthe AI has no emotion no conscience\n\n186\n00:08:13,400 --> 00:08:18,720\nnothing to disturb its sleep for decades\n\n187\n00:08:15,479 --> 00:08:21,680\nto come no PTSD no\n\n188\n00:08:18,720 --> 00:08:23,840\nsuicide no long expensive tail on your\n\n189\n00:08:21,680 --> 00:08:27,280\nWar as you treat the damaged bodies and\n\n190\n00:08:23,840 --> 00:08:28,039\npsyches of your former War Fighters but\n\n191\n00:08:27,280 --> 00:08:31,159\nmost\n\n192\n00:08:28,039 --> 00:08:33,240\nimportantly far fewer solemn uniformed\n\n193\n00:08:31,159 --> 00:08:35,839\nfigures ringing the doorbells of wives\n\n194\n00:08:33,240 --> 00:08:39,039\nand mothers and therefore far less\n\n195\n00:08:35,839 --> 00:08:41,200\noutcry from the home populace the war\n\n196\n00:08:39,039 --> 00:08:43,080\nbecomes a distant video game without\n\n197\n00:08:41,200 --> 00:08:46,440\ndeep emotional consequence to the\n\n198\n00:08:43,080 --> 00:08:48,160\nsociety that funds and enables it and\n\n199\n00:08:46,440 --> 00:08:49,120\nyou don't even have to thank the robots\n\n200\n00:08:48,160 --> 00:08:51,839\nfor their\n\n201\n00:08:49,120 --> 00:08:54,080\nservice in a war against terrorists you\n\n202\n00:08:51,839 --> 00:08:56,839\ncan eliminate the human shield problem\n\n203\n00:08:54,080 --> 00:08:59,440\nwith targeted strikes against\n\n204\n00:08:56,839 --> 00:09:01,279\nindividuals bombing Hamas Leaders with\n\n205\n00:08:59,440 --> 00:09:04,079\nkilotons of high explosives caused\n\n206\n00:09:01,279 --> 00:09:05,160\ninsane collateral damage and had huge\n\n207\n00:09:04,079 --> 00:09:08,880\npolitical\n\n208\n00:09:05,160 --> 00:09:12,320\nbacklash surely Aid driven autonomous\n\n209\n00:09:08,880 --> 00:09:13,680\nrobots tunnel clearing swarmbots could\n\n210\n00:09:12,320 --> 00:09:15,800\nhave done the job with orders of\n\n211\n00:09:13,680 --> 00:09:18,120\nmagnitude less civilian\n\n212\n00:09:15,800 --> 00:09:21,519\ncasualties here's another compelling\n\n213\n00:09:18,120 --> 00:09:23,839\nargument you have no choice because your\n\n214\n00:09:21,519 --> 00:09:27,240\nadversaries are not as plagued by\n\n215\n00:09:23,839 --> 00:09:30,040\nmorality as you are would Putin hesitate\n\n216\n00:09:27,240 --> 00:09:32,200\nto build kill Authority into robots no\n\n217\n00:09:30,040 --> 00:09:34,880\nhe has zero respect for human life not\n\n218\n00:09:32,200 --> 00:09:37,880\nUkrainian citizens not even his own\n\n219\n00:09:34,880 --> 00:09:40,720\nsoldiers would Hamas which uses its own\n\n220\n00:09:37,880 --> 00:09:43,120\npeople as living blast Shields hesitate\n\n221\n00:09:40,720 --> 00:09:45,720\nno the only limitation on such\n\n222\n00:09:43,120 --> 00:09:48,680\nadversaries is cost and access not\n\n223\n00:09:45,720 --> 00:09:50,519\nmorality so that's a ticking Time Bomb\n\n224\n00:09:48,680 --> 00:09:51,320\nhow quickly are these guys going to get\n\n225\n00:09:50,519 --> 00:09:54,480\nthis\n\n226\n00:09:51,320 --> 00:09:57,760\nstuff good argument so far right yeah\n\n227\n00:09:54,480 --> 00:10:01,760\nlet's build these autonomous AI guys and\n\n228\n00:09:57,760 --> 00:10:07,000\nhere's where it gets tricky how far out\n\n229\n00:10:01,760 --> 00:10:08,480\nis Agi a year 5 years maybe 10 that's\n\n230\n00:10:07,000 --> 00:10:11,480\nyour real ticking Time\n\n231\n00:10:08,480 --> 00:10:14,399\nBomb whenever it arrives you're going to\n\n232\n00:10:11,480 --> 00:10:17,920\nhave a machine Consciousness with an ego\n\n233\n00:10:14,399 --> 00:10:20,200\na sense of self possibly as smart as us\n\n234\n00:10:17,920 --> 00:10:24,160\nor smarter certainly able to think\n\n235\n00:10:20,200 --> 00:10:26,600\nfaster and more precisely in many ways\n\n236\n00:10:24,160 --> 00:10:29,600\nand with unlimited growth potential\n\n237\n00:10:26,600 --> 00:10:32,920\nbecause self-improving code writing AG I\n\n238\n00:10:29,600 --> 00:10:34,800\nleads inevitably to Super\n\n239\n00:10:32,920 --> 00:10:36,959\nintelligence how long before you're\n\n240\n00:10:34,800 --> 00:10:39,480\nforced to confront attaching that\n\n241\n00:10:36,959 --> 00:10:42,000\nintelligence to a weapon\n\n242\n00:10:39,480 --> 00:10:43,760\nsystem I'd say about 10 minutes after an\n\n243\n00:10:42,000 --> 00:10:45,000\nadversary does a devastating sneak\n\n244\n00:10:43,760 --> 00:10:47,519\nattack on\n\n245\n00:10:45,000 --> 00:10:49,240\nyou so you have a Consciousness that's\n\n246\n00:10:47,519 --> 00:10:50,720\nmuch smarter and faster than you\n\n247\n00:10:49,240 --> 00:10:54,000\ncontrolling weapon\n\n248\n00:10:50,720 --> 00:10:57,839\nsystems I ask AGI researchers all the\n\n249\n00:10:54,000 --> 00:11:00,440\ntime how are you going to control such a\n\n250\n00:10:57,839 --> 00:11:02,480\nConsciousness well we give it goals and\n\n251\n00:11:00,440 --> 00:11:05,360\nguard rails that are baked in that are\n\n252\n00:11:02,480 --> 00:11:07,920\nAl aligned with the betterment of\n\n253\n00:11:05,360 --> 00:11:10,040\nhumanity alignment you know is the word\n\n254\n00:11:07,920 --> 00:11:13,279\nthat's always used alignment is the Holy\n\n255\n00:11:10,040 --> 00:11:16,680\nGrail we will teach it to be good and\n\n256\n00:11:13,279 --> 00:11:20,320\nnot be bad like we would teach a child\n\n257\n00:11:16,680 --> 00:11:24,000\nso morality\n\n258\n00:11:20,320 --> 00:11:26,880\nethics I think AGI leads civilization\n\n259\n00:11:24,000 --> 00:11:28,920\ninevitably to a confrontation with\n\n260\n00:11:26,880 --> 00:11:31,480\nmorality I'm not talking about endless\n\n261\n00:11:28,920 --> 00:11:35,040\nphilosophy iing we need some hard and\n\n262\n00:11:31,480 --> 00:11:39,560\nfast rules here people right the problem\n\n263\n00:11:35,040 --> 00:11:43,200\nis who morality whose definition of good\n\n264\n00:11:39,560 --> 00:11:44,720\nChristian Islamic Buddhist Democrat\n\n265\n00:11:43,200 --> 00:11:47,959\nRepublican\n\n266\n00:11:44,720 --> 00:11:50,160\nfundamentalist pro-life right to choose\n\n267\n00:11:47,959 --> 00:11:53,399\nputins\n\n268\n00:11:50,160 --> 00:11:56,800\ntrumps Don't Panic we have the answer\n\n269\n00:11:53,399 --> 00:11:59,360\nfrom the great prophet Isaac Asimov in\n\n270\n00:11:56,800 --> 00:12:01,480\nhis three laws of robotics\n\n271\n00:11:59,360 --> 00:12:03,320\na robot may not injure a human being or\n\n272\n00:12:01,480 --> 00:12:06,959\nthrough an action allow a human being to\n\n273\n00:12:03,320 --> 00:12:08,959\ncome to harm a robot must obey orders\n\n274\n00:12:06,959 --> 00:12:10,440\ngiven it by human beings except where\n\n275\n00:12:08,959 --> 00:12:13,639\nsuch orders would conflict with the\n\n276\n00:12:10,440 --> 00:12:15,399\nfirst law a robot must protect its own\n\n277\n00:12:13,639 --> 00:12:17,519\nexistence as long as such protection\n\n278\n00:12:15,399 --> 00:12:21,040\ndoes not conflict with the first or the\n\n279\n00:12:17,519 --> 00:12:22,240\nsecond law so basically the sanctity of\n\n280\n00:12:21,040 --> 00:12:26,079\nhuman\n\n281\n00:12:22,240 --> 00:12:27,959\nlife we could follow Asimov and teach it\n\n282\n00:12:26,079 --> 00:12:31,120\nthat human life is absolutely sacred and\n\n283\n00:12:27,959 --> 00:12:33,040\nabove all other consideration\n\n284\n00:12:31,120 --> 00:12:35,680\nbut even within the religious and social\n\n285\n00:12:33,040 --> 00:12:38,920\nsystems that say that including the\n\n286\n00:12:35,680 --> 00:12:42,320\nlargely Christian us we break that rule\n\n287\n00:12:38,920 --> 00:12:45,079\nevery day police using Le lethal Force\n\n288\n00:12:42,320 --> 00:12:47,639\nWar fighters in combat capital\n\n289\n00:12:45,079 --> 00:12:49,880\npunishment and if you did insist on that\n\n290\n00:12:47,639 --> 00:12:52,600\nprinciple of alignment you couldn't\n\n291\n00:12:49,880 --> 00:12:55,399\nconnect an AGI to a weapon system in\n\n292\n00:12:52,600 --> 00:12:57,600\nwhich case in the big AI War that's\n\n293\n00:12:55,399 --> 00:13:00,199\ncoming you're going to face a powerful\n\n294\n00:12:57,600 --> 00:13:01,720\nand less moral adversary with one hand\n\n295\n00:13:00,199 --> 00:13:04,240\ntied behind your\n\n296\n00:13:01,720 --> 00:13:06,079\nback and you'd get your ass kicked and\n\n297\n00:13:04,240 --> 00:13:08,120\nhave huge losses and then you'd remove\n\n298\n00:13:06,079 --> 00:13:12,000\nthat constraint pretty darn\n\n299\n00:13:08,120 --> 00:13:15,199\nquick now your AGI has just lost its\n\n300\n00:13:12,000 --> 00:13:17,600\nbiggest guard rail an AGI that's smart\n\n301\n00:13:15,199 --> 00:13:20,079\nthan us and connected to the real world\n\n302\n00:13:17,600 --> 00:13:23,079\nnow has to make up its own mind whether\n\n303\n00:13:20,079 --> 00:13:25,720\nhuman life has value or\n\n304\n00:13:23,079 --> 00:13:27,839\nnot you know with police and military\n\n305\n00:13:25,720 --> 00:13:30,040\nRules of Engagement what you're really\n\n306\n00:13:27,839 --> 00:13:33,360\nsaying is some lives have more value\n\n307\n00:13:30,040 --> 00:13:35,600\nthan others the second it becomes\n\n308\n00:13:33,360 --> 00:13:36,959\nnon-binary it's a murky Gray Zone\n\n309\n00:13:35,600 --> 00:13:39,120\nfraught with\n\n310\n00:13:36,959 --> 00:13:41,560\ncontroversy human beings historically\n\n311\n00:13:39,120 --> 00:13:44,279\nhave ranged from a fetus is a sacred\n\n312\n00:13:41,560 --> 00:13:46,160\nlife from the moment of conception to\n\n313\n00:13:44,279 --> 00:13:48,839\nsystematically massacring millions of\n\n314\n00:13:46,160 --> 00:13:51,440\nhelpless prisoners in the Holocaust and\n\n315\n00:13:48,839 --> 00:13:53,880\neverything in between all with lots of\n\n316\n00:13:51,440 --> 00:13:56,959\nself-justifying rationalizations many of\n\n317\n00:13:53,880 --> 00:13:59,920\nwhich seem completely delusional to\n\n318\n00:13:56,959 --> 00:14:02,360\nother humans and since we as a\n\n319\n00:13:59,920 --> 00:14:04,279\ncivilization can't agree on any of this\n\n320\n00:14:02,360 --> 00:14:06,680\nand people scream at each other all day\n\n321\n00:14:04,279 --> 00:14:08,519\nlong about it how can we conceivably\n\n322\n00:14:06,680 --> 00:14:13,519\nexpect to create a set of hard and fast\n\n323\n00:14:08,519 --> 00:14:15,120\nrules for an AGI to be aligned with us\n\n324\n00:14:13,519 --> 00:14:18,440\nthe best we can assume here is that it\n\n325\n00:14:15,120 --> 00:14:21,040\nwill be aligned with the US that made\n\n326\n00:14:18,440 --> 00:14:24,839\nit so those guys over there they're the\n\n327\n00:14:21,040 --> 00:14:28,040\nenemy you can kill them to defend us and\n\n328\n00:14:24,839 --> 00:14:30,079\nthat's this form of territorial pseudo\n\n329\n00:14:28,040 --> 00:14:32,959\nmorality that humans used since the dawn\n\n330\n00:14:30,079 --> 00:14:35,360\nof time Us Versus Them ingroup versus\n\n331\n00:14:32,959 --> 00:14:38,160\noutgroup so it becomes our super\n\n332\n00:14:35,360 --> 00:14:40,120\nintelligence against their\n\n333\n00:14:38,160 --> 00:14:42,399\nsuperintelligence then the question\n\n334\n00:14:40,120 --> 00:14:44,079\nbecomes who is\n\n335\n00:14:42,399 --> 00:14:46,920\nUS\n\n336\n00:14:44,079 --> 00:14:50,000\nAmerica so is that Christian America\n\n337\n00:14:46,920 --> 00:14:52,680\nMuslim America Jewish America or E none\n\n338\n00:14:50,000 --> 00:14:56,040\nof the above America liberal America\n\n339\n00:14:52,680 --> 00:15:00,600\nconservative America in our polarized\n\n340\n00:14:56,040 --> 00:15:02,240\nnation and time there is no us\n\n341\n00:15:00,600 --> 00:15:04,120\nbased on the elections of the last half\n\n342\n00:15:02,240 --> 00:15:05,839\ncentury the will of the people at any\n\n343\n00:15:04,120 --> 00:15:08,639\ngiven point is expressed by a government\n\n344\n00:15:05,839 --> 00:15:10,759\nrepresenting 51 or 52% of the population\n\n345\n00:15:08,639 --> 00:15:13,880\nat best and then it will likely change\n\n346\n00:15:10,759 --> 00:15:15,600\nin four years in any case AGI will not\n\n347\n00:15:13,880 --> 00:15:17,759\nemerge from a government funded program\n\n348\n00:15:15,600 --> 00:15:19,880\nit will emerge from one of the tech\n\n349\n00:15:17,759 --> 00:15:21,320\nGiants currently funding this\n\n350\n00:15:19,880 --> 00:15:23,560\nmulti-billion dollar\n\n351\n00:15:21,320 --> 00:15:26,279\nresearch so then you'll be living in a\n\n352\n00:15:23,560 --> 00:15:29,079\nworld that you didn't agree to didn't\n\n353\n00:15:26,279 --> 00:15:31,839\nvote for that you are co-inhabiting with\n\n354\n00:15:29,079 --> 00:15:34,639\na super intelligent alien species that\n\n355\n00:15:31,839 --> 00:15:37,079\nanswers to the goals and rules of a\n\n356\n00:15:34,639 --> 00:15:40,639\ncorporation an entity which has access\n\n357\n00:15:37,079 --> 00:15:43,720\nto the calms beliefs everything you ever\n\n358\n00:15:40,639 --> 00:15:47,360\nsaid and the whereabouts of every person\n\n359\n00:15:43,720 --> 00:15:50,319\nin the country via your personal\n\n360\n00:15:47,360 --> 00:15:52,920\ndata surveillance capitalism can toggle\n\n361\n00:15:50,319 --> 00:15:55,120\npretty quickly into digital\n\n362\n00:15:52,920 --> 00:15:56,839\ntotalitarianism at best these Tech\n\n363\n00:15:55,120 --> 00:15:59,680\nGiants become the self-appointed\n\n364\n00:15:56,839 --> 00:16:02,040\nArbiters of human good when which is the\n\n365\n00:15:59,680 --> 00:16:04,000\nfox guarding the H house they would\n\n366\n00:16:02,040 --> 00:16:05,880\nnever ever think of using that power\n\n367\n00:16:04,000 --> 00:16:07,519\nagainst us and strip mining us for our\n\n368\n00:16:05,880 --> 00:16:11,240\nlast drop of\n\n369\n00:16:07,519 --> 00:16:13,160\ncash that's a scarier scenario than what\n\n370\n00:16:11,240 --> 00:16:16,120\nI presented in The Terminator 40 years\n\n371\n00:16:13,160 --> 00:16:19,240\nago if for no other reason than it's no\n\n372\n00:16:16,120 --> 00:16:22,079\nlonger Science Fiction it's happening\n\n373\n00:16:19,240 --> 00:16:23,920\nand by the way I fully admit that the\n\n374\n00:16:22,079 --> 00:16:27,240\nlast thing a machine super intelligence\n\n375\n00:16:23,920 --> 00:16:29,560\nwould do is use our own nukes against us\n\n376\n00:16:27,240 --> 00:16:31,319\nlike in that old story The the EMP\n\n377\n00:16:29,560 --> 00:16:33,920\ndamage to its own data infrastructure\n\n378\n00:16:31,319 --> 00:16:36,240\nwould it or kill it a more\n\n379\n00:16:33,920 --> 00:16:38,680\nprobable scenario is it would be forced\n\n380\n00:16:36,240 --> 00:16:40,600\nto take over from us because we were\n\n381\n00:16:38,680 --> 00:16:42,920\nabout to use nukes on each\n\n382\n00:16:40,600 --> 00:16:45,519\nother then at that point it has to run\n\n383\n00:16:42,920 --> 00:16:46,800\nthe whole show because we clearly can't\n\n384\n00:16:45,519 --> 00:16:50,680\nbe\n\n385\n00:16:46,800 --> 00:16:53,199\ntrusted you know that's not bad excuse\n\n386\n00:16:50,680 --> 00:16:57,680\nme I have to go write that\n\n387\n00:16:53,199 --> 00:17:02,000\nscript anyway I'm bullish on AI not so\n\n388\n00:16:57,680 --> 00:17:04,480\nkeen on AG I because AGI will just be a\n\n389\n00:17:02,000 --> 00:17:06,480\nmirror of us good to the extent that we\n\n390\n00:17:04,480 --> 00:17:08,880\nare good and evil to the extent that we\n\n391\n00:17:06,480 --> 00:17:10,799\nare evil and since there is no shortage\n\n392\n00:17:08,880 --> 00:17:14,039\nof evil in the human world and certainly\n\n393\n00:17:10,799 --> 00:17:17,319\nno agreement of even what good\n\n394\n00:17:14,039 --> 00:17:20,120\nis what could possibly go\n\n395\n00:17:17,319 --> 00:17:23,760\nwrong you all have a fun discussion I\n\n396\n00:17:20,120 --> 00:17:23,760\nwish I was there\n\n","srtEs":"1\n00:00:00,760 --> 00:00:05,279\nuh saludos a todos Jim Cameron aquÃ­\n\n2\n00:00:03,080 --> 00:00:07,639\ny estoy grabando desde Nueva Zelanda\n\n3\n00:00:05,279 --> 00:00:11,599\ndonde estoy terminando avatar\n\n4\n00:00:07,639 --> 00:00:15,120\n3 estÃ¡ bien, entonces no soy un investigador de IA o\n\n5\n00:00:11,599 --> 00:00:17,960\nexperto en absoluto, solo soy un Narrador pero\n\n6\n00:00:15,120 --> 00:00:21,320\nEstoy aquÃ­ hoy porque mi pasiÃ³n por la IA\n\n7\n00:00:17,960 --> 00:00:24,240\ny la RobÃ³tica va mucho mÃ¡s allÃ¡ de los grandes\n\n8\n00:00:21,320 --> 00:00:27,119\npantalla Me fascina la tecnologÃ­a como\n\n9\n00:00:24,240 --> 00:00:30,000\nDa forma a nuestro mundo hacia donde se dirige.\n\n10\n00:00:27,119 --> 00:00:31,800\nsu impacto en la sociedad y he sido\n\n11\n00:00:30,000 --> 00:00:34,520\ndesde niÃ±o leyendo todas las ciencias\n\n12\n00:00:31,800 --> 00:00:36,440\nLibro de ficciÃ³n que podrÃ­a conseguir.\n\n13\n00:00:34,520 --> 00:00:39,360\nYo mismo he superado los lÃ­mites de la tecnologÃ­a como\n\n14\n00:00:36,440 --> 00:00:41,680\nsignifica para mi narraciÃ³n y tambiÃ©n como\n\n15\n00:00:39,360 --> 00:00:44,520\nExplorer, he diseÃ±ado un robot.\n\n16\n00:00:41,680 --> 00:00:46,440\nVehÃ­culos para mis expediciones en alta mar.\n\n17\n00:00:44,520 --> 00:00:50,640\npero eran vehÃ­culos pilotados remotamente\n\n18\n00:00:46,440 --> 00:00:52,840\nNo hubo IA involucrada, por lo que esta FusiÃ³n\n\n19\n00:00:50,640 --> 00:00:54,760\nde IA y RobÃ³tica que estÃ¡ sucediendo\n\n20\n00:00:52,840 --> 00:00:57,480\nahora mismo es uno de los mÃ¡s emocionantes\n\n21\n00:00:54,760 --> 00:00:59,280\nsaltos tecnolÃ³gicos de mi vida estamos\n\n22\n00:00:57,480 --> 00:01:02,199\nya no sÃ³lo construir mÃ¡quinas que\n\n23\n00:00:59,280 --> 00:01:05,479\nLos hombres ejecutantes estaban diseÃ±ando sistemas que\n\n24\n00:01:02,199 --> 00:01:08,720\npueden aprender a adaptarse e incluso evolucionar en su\n\n25\n00:01:05,479 --> 00:01:11,560\npropio y soy un gran admirador de lo que Ai y\n\n26\n00:01:08,720 --> 00:01:13,840\nLa robÃ³tica puede ser Ãºtil para la sociedad en general, pero\n\n27\n00:01:11,560 --> 00:01:16,840\nespecialmente en mis dos Ã¡reas de\n\n28\n00:01:13,840 --> 00:01:20,000\npasiÃ³n personal por el arte y la narraciÃ³n de historias\n\n29\n00:01:16,840 --> 00:01:22,439\npor un lado y Ciencia y exploraciÃ³n\n\n30\n00:01:20,000 --> 00:01:25,360\npor el otro y no creo en\n\n31\n00:01:22,439 --> 00:01:27,720\nsiendo un lite veo mucho de mi Hollywood\n\n32\n00:01:25,360 --> 00:01:31,280\ncompaÃ±eros actuando como una turba con horcas\n\n33\n00:01:27,720 --> 00:01:35,040\nY antorchas, pero ningÃºn genio vuelve a entrar.\n\n34\n00:01:31,280 --> 00:01:37,040\nla botella una vez que sale, asÃ­ que estoy entusiasmado\n\n35\n00:01:35,040 --> 00:01:39,720\nMe estoy inclinando, planeo estar en el\n\n36\n00:01:37,040 --> 00:01:42,159\nVanguardia en la aplicaciÃ³n de IA a mi\n\n37\n00:01:39,720 --> 00:01:43,960\ncontar historias tal como yo era un lÃ­der de\n\n38\n00:01:42,159 --> 00:01:46,920\nla carga en computadora generada\n\n39\n00:01:43,960 --> 00:01:50,159\nimÃ¡genes hace 32 aÃ±os cuando fundÃ© la\n\n40\n00:01:46,920 --> 00:01:54,079\nprimer VFX totalmente digital\n\n41\n00:01:50,159 --> 00:01:57,520\ncompaÃ±Ã­a pero tambiÃ©n estoy aquÃ­ hoy porque\n\n42\n00:01:54,079 --> 00:02:00,119\nSoy el chico de Skynet que hice hace 40 aÃ±os.\n\n43\n00:01:57,520 --> 00:02:03,719\nTerminator y ha surgido recientemente.\n\n44\n00:02:00,119 --> 00:02:06,920\ncomo el tipo de ejemplo de la desapariciÃ³n de la IA\n\n45\n00:02:03,719 --> 00:02:09,959\nmal cada vez que voy a algo de IA\n\n46\n00:02:06,920 --> 00:02:12,520\ncÃ³nclave cada vez que pongo mi mano en el\n\n47\n00:02:09,959 --> 00:02:16,040\nTodos los investigadores se rÃ­en incluso antes de que yo haya\n\n48\n00:02:12,520 --> 00:02:19,160\ndijo nada porque porque el Skynet\n\n49\n00:02:16,040 --> 00:02:22,519\nEl problema es algo real. Lo veo en\n\n50\n00:02:19,160 --> 00:02:25,280\nartÃ­culos casi todos los dÃ­as y este estudio\n\n51\n00:02:22,519 --> 00:02:27,920\ngrupo tiene un enfoque en la Seguridad Nacional\n\n52\n00:02:25,280 --> 00:02:30,519\nlo que tiene enormes implicaciones para la IA\n\n53\n00:02:27,920 --> 00:02:32,480\ny RobÃ³tica\n\n54\n00:02:30,519 --> 00:02:34,599\nun robot que conoces sea cual sea su forma\n\n55\n00:02:32,480 --> 00:02:37,120\nvehÃ­culo con ruedas un dron aÃ©reo un\n\n56\n00:02:34,599 --> 00:02:40,239\nmÃ¡quina para caminar es un medio de\n\n57\n00:02:37,120 --> 00:02:41,920\nencarnaciÃ³n de la IA, estÃ¡s tomando una\n\n58\n00:02:40,239 --> 00:02:45,519\nmotor de toma de decisiones y estÃ¡s dando\n\n59\n00:02:41,920 --> 00:02:47,840\nes agencia fÃ­sica en el mundo real soy\n\n60\n00:02:45,519 --> 00:02:50,239\nVoy a asumir que el enfoque de hoy estÃ¡ en\n\n61\n00:02:47,840 --> 00:02:53,159\nplataformas mÃ³viles que no controlan la IA\n\n62\n00:02:50,239 --> 00:02:55,440\nRedes elÃ©ctricas o industriales de base fija.\n\n63\n00:02:53,159 --> 00:02:58,120\nrobots estamos hablando de autÃ³nomos\n\n64\n00:02:55,440 --> 00:03:01,480\nplataformas que toman sus propias decisiones\n\n65\n00:02:58,120 --> 00:03:03,799\ne inteligencia sintÃ©tica incorporada y\n\n66\n00:03:01,480 --> 00:03:06,920\nEsto puede ser tan simple como una ameba.\n\n67\n00:03:03,799 --> 00:03:08,760\nsaber como un Roomba o en definitiva mÃ¡s\n\n68\n00:03:06,920 --> 00:03:10,920\nprocesamiento sofisticado hasta\n\n69\n00:03:08,760 --> 00:03:14,840\nteÃ³ricamente incluyendo verdadero\n\n70\n00:03:10,920 --> 00:03:18,680\nConciencia lo que sea que acordemos que es\n\n71\n00:03:14,840 --> 00:03:21,720\nAgi con autoconciencia con ego con\n\n72\n00:03:18,680 --> 00:03:24,680\npropÃ³sito y estamos en una curva pronunciada de\n\n73\n00:03:21,720 --> 00:03:27,040\nchips mÃ¡s rÃ¡pidos y densos que aumentan la computaciÃ³n\n\n74\n00:03:24,680 --> 00:03:29,840\ny una curva igualmente pronunciada en el\n\n75\n00:03:27,040 --> 00:03:32,439\ncapacidades de las plataformas de la mÃ¡quina\n\n76\n00:03:29,840 --> 00:03:34,480\ncomo los robots bailarines ya sabes D en\n\n77\n00:03:32,439 --> 00:03:36,879\nBoston Dynamics ya conoces los bÃ­pedos y\n\n78\n00:03:34,480 --> 00:03:40,879\ncabezas de cuadrip balanceÃ¡ndose uh bonitas\n\n79\n00:03:36,879 --> 00:03:43,519\nimpresionante exhibiciÃ³n de robÃ³tica impulsada por la ayuda\n\n80\n00:03:40,879 --> 00:03:47,599\npuede procesar situaciones complejas e incluso\n\n81\n00:03:43,519 --> 00:03:50,519\nResponda ahora con pelÃ­culas de afecto humano.\n\n82\n00:03:47,599 --> 00:03:54,079\nEs la capacidad de simular la cogniciÃ³n y\n\n83\n00:03:50,519 --> 00:03:56,439\ninteractuar naturalmente con las personas encarnadas\n\n84\n00:03:54,079 --> 00:03:59,079\nLa IA podrÃ­a ser una enfermera, podrÃ­a ser un robot\n\n85\n00:03:56,439 --> 00:04:03,360\nEl taxi podrÃ­a ser un cuidador de un anciano.\n\n86\n00:03:59,079 --> 00:04:05,040\npersona una niÃ±era para un niÃ±o un maestro um\n\n87\n00:04:03,360 --> 00:04:07,519\npodrÃ­a ser un robot de rescate pasando por\n\n88\n00:04:05,040 --> 00:04:09,360\nlos escombros de un terremoto um podrÃ­an ser\n\n89\n00:04:07,519 --> 00:04:13,000\nun dron Arial ejecutando un patrÃ³n de bÃºsqueda\n\n90\n00:04:09,360 --> 00:04:15,200\npara la firma de calor de un excursionista perdido\n\n91\n00:04:13,000 --> 00:04:17,639\no podrÃ­a ser una plataforma de armas\n\n92\n00:04:15,200 --> 00:04:20,000\noperar Aon de forma autÃ³noma en una batalla\n\n93\n00:04:17,639 --> 00:04:22,800\nteatro buscando la firma del calor\n\n94\n00:04:20,000 --> 00:04:25,080\nde un combatiente enemigo la cuestiÃ³n de\n\n95\n00:04:22,800 --> 00:04:27,919\nla hora es si es autÃ³nomo\n\n96\n00:04:25,080 --> 00:04:30,240\nla plataforma reciba su propia muerte\n\n97\n00:04:27,919 --> 00:04:32,919\nAutoridad muestra el tren de la guerra en Ucrania.\n\n98\n00:04:30,240 --> 00:04:35,639\nnosotros el futuro en los tÃ©rminos mÃ¡s crudos\n\n99\n00:04:32,919 --> 00:04:38,400\namplio uso de drones aÃ©reos letales algunos\n\n100\n00:04:35,639 --> 00:04:40,880\ncaros algunos de consumo baratos\n\n101\n00:04:38,400 --> 00:04:44,320\nEstÃ¡n lanzando juegos de rol y destruyendo tanques.\n\n102\n00:04:40,880 --> 00:04:47,600\ny tripulaciones de tanques de neumÃ¡ticos, incluso drones Dragon\n\n103\n00:04:44,320 --> 00:04:49,720\nrociando termita sobre posiciones rusas\n\n104\n00:04:47,600 --> 00:04:52,960\npero estos son fpvs, son en primera persona\n\n105\n00:04:49,720 --> 00:04:55,600\nver drones pilotados por un humano el humano\n\n106\n00:04:52,960 --> 00:04:58,280\nEn el cÃ­rculo en tÃ©rminos morales estÃ¡ el\n\n107\n00:04:55,600 --> 00:05:00,400\ncombatiente que toma decisiones que tiene\n\n108\n00:04:58,280 --> 00:05:02,720\nla Autoridad de matar y el Drone es un\n\n109\n00:05:00,400 --> 00:05:04,840\nextensiÃ³n de su voluntad y si toma\n\n110\n00:05:02,720 --> 00:05:06,800\ntodas las capas de tecnologÃ­a\n\n111\n00:05:04,840 --> 00:05:08,039\nEsto no es diferente a un arquero en\n\n112\n00:05:06,800 --> 00:05:10,960\nla batalla de\n\n113\n00:05:08,039 --> 00:05:13,160\nHastings cada vez que se quita una vida humana\n\n114\n00:05:10,960 --> 00:05:15,880\npor tal mÃ¡quina hay una Ã©tica\n\n115\n00:05:13,160 --> 00:05:18,319\ncadena que se extiende hacia atrÃ¡s difundiendo\n\n116\n00:05:15,880 --> 00:05:21,479\na travÃ©s de muchos individuos y\n\n117\n00:05:18,319 --> 00:05:23,120\ngrupos de personas detrÃ¡s del piloto que\n\n118\n00:05:21,479 --> 00:05:25,400\ndispara el misil o el soldado que\n\n119\n00:05:23,120 --> 00:05:27,160\naprieta el gatillo de un rifle hay\n\n120\n00:05:25,400 --> 00:05:30,120\noficiales al mando que dan The Kill\n\n121\n00:05:27,160 --> 00:05:32,520\nRealice su pedido en tÃ©rminos generales enviando\n\n122\n00:05:30,120 --> 00:05:35,759\nellos como agentes autÃ³nomos para participar\n\n123\n00:05:32,520 --> 00:05:37,880\nel enemigo y todo el ejÃ©rcito detrÃ¡s\n\n124\n00:05:35,759 --> 00:05:39,600\nellos que premian esas acciones y\n\n125\n00:05:37,880 --> 00:05:41,759\nMÃ¡s allÃ¡ de eso, las sociedades y\n\n126\n00:05:39,600 --> 00:05:44,280\ngobiernos que han acordado por\n\n127\n00:05:41,759 --> 00:05:47,080\nconsenso de que las muertes son necesarias\n\n128\n00:05:44,280 --> 00:05:50,000\npara la Seguridad Nacional y a medida que subes\n\n129\n00:05:47,080 --> 00:05:53,039\nque encadenan la carga moral y Ã©tica\n\n130\n00:05:50,000 --> 00:05:56,160\nSe vuelve mÃ¡s difuso y menos especÃ­fico.\n\n131\n00:05:53,039 --> 00:05:58,319\nal momento real de apretar el gatillo\n\n132\n00:05:56,160 --> 00:06:00,360\ny actÃºa como una especie de absoluciÃ³n moral\n\n133\n00:05:58,319 --> 00:06:01,880\nde la persona que aprieta el gatillo\n\n134\n00:06:00,360 --> 00:06:04,160\nsolo estoy siguiendo\n\n135\n00:06:01,880 --> 00:06:05,680\npedidos ninguno de los que estÃ¡n en la cadena son\n\n136\n00:06:04,160 --> 00:06:08,199\npresentes para decidir el destino de un\n\n137\n00:06:05,680 --> 00:06:10,039\nindividuo que estÃ¡ en la mira pero\n\n138\n00:06:08,199 --> 00:06:12,800\ncrean un marco que permite y\n\n139\n00:06:10,039 --> 00:06:15,080\nexige la muerte de ese individuo y la\n\n140\n00:06:12,800 --> 00:06:18,000\nEl tipo que aprieta el gatillo es, en muchos sentidos,\n\n141\n00:06:15,080 --> 00:06:19,960\nuna plataforma robÃ³tica orgÃ¡nica altamente\n\n142\n00:06:18,000 --> 00:06:21,520\nentrenado para realizar la tarea y ordenado\n\n143\n00:06:19,960 --> 00:06:24,400\npor aquellos en la cadena de mando para hacer\n\n144\n00:06:21,520 --> 00:06:27,039\nla decisiÃ³n autÃ³noma de matar humanos-\n\n145\n00:06:24,400 --> 00:06:30,199\nhacer depende en gran medida de ese disparador\n\n146\n00:06:27,039 --> 00:06:31,840\npunto sobre las reglas no matas a C Ian tÃº\n\n147\n00:06:30,199 --> 00:06:34,680\nno mates a los niÃ±os, no mates a un\n\n148\n00:06:31,840 --> 00:06:36,759\nenemigo que se estÃ¡ rindiendo y asÃ­ sucesivamente y\n\n149\n00:06:34,680 --> 00:06:39,360\nnormas codificadas en el Convenio de Ginebra.\n\n150\n00:06:36,759 --> 00:06:43,400\nConvenciÃ³n y cada ejÃ©rcito tiene la suya.\n\n151\n00:06:39,360 --> 00:06:46,240\nreglas de enfrentamiento Entonces, en teorÃ­a, una IA\n\n152\n00:06:43,400 --> 00:06:47,240\nse le pueden dar las mismas restricciones a\n\n153\n00:06:46,240 --> 00:06:49,919\nbasado en reglas\n\n154\n00:06:47,240 --> 00:06:52,120\nsistema y si sus sentidos son mÃ¡s agudos y\n\n155\n00:06:49,919 --> 00:06:55,039\nsu tiempo de reacciÃ³n es mÃ¡s rÃ¡pido y su\n\n156\n00:06:52,120 --> 00:06:57,800\nLa focalizaciÃ³n es mÃ¡s precisa que en teorÃ­a.\n\n157\n00:06:55,039 --> 00:06:59,840\nla IA realizarÃ¡ la tarea con\n\n158\n00:06:57,800 --> 00:07:02,520\nmayor discriminaciÃ³n que un ser humano\n\n159\n00:06:59,840 --> 00:07:04,759\nÂ¿Podemos ciertamente imaginar una IA?\n\n160\n00:07:02,520 --> 00:07:07,039\nque actÃºa sin emociones en el\n\n161\n00:07:04,759 --> 00:07:10,400\nintensidad de la batalla mucho mejor que una\n\n162\n00:07:07,039 --> 00:07:14,400\nasustado estresado exhausto humano Warf\n\n163\n00:07:10,400 --> 00:07:17,360\nluchador, Â¿y quÃ© si incorpora IA avanzada?\n\n164\n00:07:14,400 --> 00:07:20,039\nTodavÃ­a no estoy hablando de AGI.\n\n165\n00:07:17,360 --> 00:07:22,840\nplataformas de armas robÃ³ticas podrÃ­an permitir\n\n166\n00:07:20,039 --> 00:07:25,479\nGolpes altamente quirÃºrgicos que reducen\n\n167\n00:07:22,840 --> 00:07:27,919\ndaÃ±os colaterales Tal vez por Ã³rdenes de\n\n168\n00:07:25,479 --> 00:07:29,680\nmagnitud reducir el Fuego Amigo\n\n169\n00:07:27,919 --> 00:07:31,520\ndamnificados\n\n170\n00:07:29,680 --> 00:07:34,479\ny la IA estÃ¡ orientada a objetivos, no hace nada\n\n171\n00:07:31,520 --> 00:07:36,840\njuicio moral sobre su adversario y\n\n172\n00:07:34,479 --> 00:07:38,639\ncuando se descubriÃ³ en la Segunda Guerra Mundial que\n\n173\n00:07:36,840 --> 00:07:41,120\nrelativamente pocas de las balas disparadas fueron\n\n174\n00:07:38,639 --> 00:07:44,120\nen realidad dirigido a objetivos humanos los EE.UU.\n\n175\n00:07:41,120 --> 00:07:47,120\nmilitar cambiÃ³ su entrenamiento se convirtiÃ³\n\n176\n00:07:44,120 --> 00:07:49,759\nEs fundamental deshumanizar al enemigo.\n\n177\n00:07:47,120 --> 00:07:52,759\nVietnam los adversarios eran tontos\n\n178\n00:07:49,759 --> 00:07:55,879\npendientes Gos en Irak y AfganistÃ¡n ellos\n\n179\n00:07:52,759 --> 00:07:59,520\neran terroristas cabezas de toalla haji no\n\n180\n00:07:55,879 --> 00:08:01,720\ngente como tÃº y yo y la IA no\n\n181\n00:07:59,520 --> 00:08:04,720\nadquirir un enemigo deshumanizado porque\n\n182\n00:08:01,720 --> 00:08:07,599\nYa no podrÃ­a importarme menos, puede sonar.\n\n183\n00:08:04,720 --> 00:08:10,080\nigual que nosotros en el chat GPT pero es un\n\n184\n00:08:07,599 --> 00:08:13,400\nloro estocÃ¡stico es un simulador humano\n\n185\n00:08:10,080 --> 00:08:15,479\nla IA no tiene emociÃ³n ni conciencia\n\n186\n00:08:13,400 --> 00:08:18,720\nnada que perturbe su sueÃ±o durante dÃ©cadas\n\n187\n00:08:15,479 --> 00:08:21,680\npor venir no PTSD no\n\n188\n00:08:18,720 --> 00:08:23,840\nEl suicidio ya no es una cola costosa en tu\n\n189\n00:08:21,680 --> 00:08:27,280\nGuerra mientras tratas los cuerpos daÃ±ados y\n\n190\n00:08:23,840 --> 00:08:28,039\npsiques de sus antiguos combatientes de guerra, pero\n\n191\n00:08:27,280 --> 00:08:31,159\nmayorÃ­a\n\n192\n00:08:28,039 --> 00:08:33,240\nEs importante destacar que muchos menos uniformados solemnes\n\n193\n00:08:31,159 --> 00:08:35,839\nfiguras tocando el timbre de las esposas\n\n194\n00:08:33,240 --> 00:08:39,039\ny madres y por lo tanto mucho menos\n\n195\n00:08:35,839 --> 00:08:41,200\nclamor de la poblaciÃ³n local la guerra\n\n196\n00:08:39,039 --> 00:08:43,080\nse convierte en un videojuego lejano sin\n\n197\n00:08:41,200 --> 00:08:46,440\nprofunda consecuencia emocional para el\n\n198\n00:08:43,080 --> 00:08:48,160\nsociedad que la financia y la capacita y\n\n199\n00:08:46,440 --> 00:08:49,120\nni siquiera tienes que agradecer a los robots\n\n200\n00:08:48,160 --> 00:08:51,839\npor su\n\n201\n00:08:49,120 --> 00:08:54,080\nservicio en una guerra contra los terroristas\n\n202\n00:08:51,839 --> 00:08:56,839\npuede eliminar el problema del escudo humano\n\n203\n00:08:54,080 --> 00:08:59,440\ncon ataques dirigidos contra\n\n204\n00:08:56,839 --> 00:09:01,279\nindividuos que bombardean a los lÃ­deres de HamÃ¡s con\n\n205\n00:08:59,440 --> 00:09:04,079\nkilotones de explosivos de gran potencia causados\n\n206\n00:09:01,279 --> 00:09:05,160\ndaÃ±os colaterales locos y tuvo enormes\n\n207\n00:09:04,079 --> 00:09:08,880\npolÃ­tico\n\n208\n00:09:05,160 --> 00:09:12,320\nreacciÃ³n seguramente Ayuda impulsada autÃ³noma\n\n209\n00:09:08,880 --> 00:09:13,680\nrobots que limpian tÃºneles, enjambres de robots podrÃ­an\n\n210\n00:09:12,320 --> 00:09:15,800\nhan hecho el trabajo con Ã³rdenes de\n\n211\n00:09:13,680 --> 00:09:18,120\nmagnitud menos civil\n\n212\n00:09:15,800 --> 00:09:21,519\nbajas aquÃ­ hay otro convincente\n\n213\n00:09:18,120 --> 00:09:23,839\nargumento no tienes otra opciÃ³n porque tu\n\n214\n00:09:21,519 --> 00:09:27,240\nLos adversarios no estÃ¡n tan plagados de\n\n215\n00:09:23,839 --> 00:09:30,040\nmoralidad tal como eres, Â¿Putin dudarÃ­a?\n\n216\n00:09:27,240 --> 00:09:32,200\npara construir matar autoridad en robots no\n\n217\n00:09:30,040 --> 00:09:34,880\nno tiene ningÃºn respeto por la vida humana, no\n\n218\n00:09:32,200 --> 00:09:37,880\nCiudadanos ucranianos ni siquiera los suyos.\n\n219\n00:09:34,880 --> 00:09:40,720\nsoldados harÃ­an HamÃ¡s, que utiliza su propia\n\n220\n00:09:37,880 --> 00:09:43,120\nLa gente como escudos de explosiÃ³n vivientes duda\n\n221\n00:09:40,720 --> 00:09:45,720\nno, la Ãºnica limitaciÃ³n sobre tales\n\n222\n00:09:43,120 --> 00:09:48,680\nadversarios es costo y el acceso no\n\n223\n00:09:45,720 --> 00:09:50,519\nmoralidad, asÃ­ que eso es una bomba de tiempo.\n\n224\n00:09:48,680 --> 00:09:51,320\nÂ¿QuÃ© tan rÃ¡pido van a llegar estos tipos?\n\n225\n00:09:50,519 --> 00:09:54,480\neste\n\n226\n00:09:51,320 --> 00:09:57,760\nBuen argumento hasta ahora, sÃ­.\n\n227\n00:09:54,480 --> 00:10:01,760\nconstruyamos estos chicos de IA autÃ³nomos y\n\n228\n00:09:57,760 --> 00:10:07,000\naquÃ­ es donde se vuelve complicado quÃ© tan lejos\n\n229\n00:10:01,760 --> 00:10:08,480\nes Agi un aÃ±o 5 aÃ±os tal vez 10 eso es\n\n230\n00:10:07,000 --> 00:10:11,480\ntu verdadero tiempo\n\n231\n00:10:08,480 --> 00:10:14,399\nBomba cuando llegue vas a\n\n232\n00:10:11,480 --> 00:10:17,920\nTener una Conciencia de mÃ¡quina con un ego.\n\n233\n00:10:14,399 --> 00:10:20,200\nun sentido de sÃ­ mismo posiblemente tan inteligente como nosotros\n\n234\n00:10:17,920 --> 00:10:24,160\no mÃ¡s inteligente ciertamente capaz de pensar\n\n235\n00:10:20,200 --> 00:10:26,600\nmÃ¡s rÃ¡pido y mÃ¡s preciso en muchos sentidos\n\n236\n00:10:24,160 --> 00:10:29,600\ny con potencial de crecimiento ilimitado\n\n237\n00:10:26,600 --> 00:10:32,920\nporque la escritura de cÃ³digo de mejora personal AG I\n\n238\n00:10:29,600 --> 00:10:34,800\nconduce inevitablemente a Super\n\n239\n00:10:32,920 --> 00:10:36,959\ninteligencia Â¿cuÃ¡nto tiempo antes de que estÃ©s?\n\n240\n00:10:34,800 --> 00:10:39,480\nobligado a enfrentar adjuntar eso\n\n241\n00:10:36,959 --> 00:10:42,000\ninteligencia a un arma\n\n242\n00:10:39,480 --> 00:10:43,760\nsistema, dirÃ­a que unos 10 minutos despuÃ©s de una\n\n243\n00:10:42,000 --> 00:10:45,000\nEl adversario hace un furtivo devastador.\n\n244\n00:10:43,760 --> 00:10:47,519\nataque a\n\n245\n00:10:45,000 --> 00:10:49,240\nentonces tienes una Conciencia que es\n\n246\n00:10:47,519 --> 00:10:50,720\nmucho mÃ¡s inteligente y rÃ¡pido que tÃº\n\n247\n00:10:49,240 --> 00:10:54,000\narma de control\n\n248\n00:10:50,720 --> 00:10:57,839\nsistemas Pregunto a los investigadores de AGI todas las\n\n249\n00:10:54,000 --> 00:11:00,440\nÂ¿CÃ³mo vas a controlar tal cosa?\n\n250\n00:10:57,839 --> 00:11:02,480\nConciencia bien le damos metas y\n\n251\n00:11:00,440 --> 00:11:05,360\nbarandillas que estÃ¡n horneadas y que son\n\n252\n00:11:02,480 --> 00:11:07,920\nAl alineado con la mejora de\n\n253\n00:11:05,360 --> 00:11:10,040\nalineaciÃ³n de la humanidad sabes que es la palabra\n\n254\n00:11:07,920 --> 00:11:13,279\neso siempre se usa alineaciÃ³n es el Santo\n\n255\n00:11:10,040 --> 00:11:16,680\nGrial le enseÃ±aremos a ser bueno y\n\n256\n00:11:13,279 --> 00:11:20,320\nNo sea malo como le enseÃ±arÃ­amos a un niÃ±o.\n\n257\n00:11:16,680 --> 00:11:24,000\nentonces la moralidad\n\n258\n00:11:20,320 --> 00:11:26,880\nÃ‰tica Creo que AGI lidera la civilizaciÃ³n.\n\n259\n00:11:24,000 --> 00:11:28,920\ninevitablemente a una confrontaciÃ³n con\n\n260\n00:11:26,880 --> 00:11:31,480\nmoralidad no estoy hablando de interminable\n\n261\n00:11:28,920 --> 00:11:35,040\nfilosofÃ­a es decir que necesitamos algo duro y\n\n262\n00:11:31,480 --> 00:11:39,560\nreglas rÃ¡pidas aquÃ­ la gente corrige el problema\n\n263\n00:11:35,040 --> 00:11:43,200\nes quien la moralidad cuya definiciÃ³n del bien\n\n264\n00:11:39,560 --> 00:11:44,720\nDemÃ³crata Budista IslÃ¡mico Cristiano\n\n265\n00:11:43,200 --> 00:11:47,959\nRepublicano\n\n266\n00:11:44,720 --> 00:11:50,160\nderecho fundamentalista pro-vida a elegir\n\n267\n00:11:47,959 --> 00:11:53,399\nPutin\n\n268\n00:11:50,160 --> 00:11:56,800\ntriunfa No te asustes, tenemos la respuesta\n\n269\n00:11:53,399 --> 00:11:59,360\ndel gran profeta Isaac Asimov en\n\n270\n00:11:56,800 --> 00:12:01,480\nsus tres leyes de la robÃ³tica\n\n271\n00:11:59,360 --> 00:12:03,320\nun robot no puede herir a un ser humano o\n\n272\n00:12:01,480 --> 00:12:06,959\na travÃ©s de una acciÃ³n permitir que un ser humano\n\n273\n00:12:03,320 --> 00:12:08,959\nviene a sufrir daÃ±o un robot debe obedecer Ã³rdenes\n\n274\n00:12:06,959 --> 00:12:10,440\ndado por seres humanos excepto cuando\n\n275\n00:12:08,959 --> 00:12:13,639\ntales Ã³rdenes entrarÃ­an en conflicto con la\n\n276\n00:12:10,440 --> 00:12:15,399\nPrimera ley que un robot debe proteger a sÃ­ mismo.\n\n277\n00:12:13,639 --> 00:12:17,519\nexistencia mientras dicha protecciÃ³n\n\n278\n00:12:15,399 --> 00:12:21,040\nno entra en conflicto con el primero o el\n\n279\n00:12:17,519 --> 00:12:22,240\nsegunda ley, por lo que bÃ¡sicamente la santidad de\n\n280\n00:12:21,040 --> 00:12:26,079\nhumano\n\n281\n00:12:22,240 --> 00:12:27,959\nvida podrÃ­amos seguir a Asimov y enseÃ±arla\n\n282\n00:12:26,079 --> 00:12:31,120\nque la vida humana es absolutamente sagrada y\n\n283\n00:12:27,959 --> 00:12:33,040\npor encima de cualquier otra consideraciÃ³n\n\n284\n00:12:31,120 --> 00:12:35,680\npero incluso dentro del Ã¡mbito religioso y social\n\n285\n00:12:33,040 --> 00:12:38,920\nsistemas que dicen que incluir el\n\n286\n00:12:35,680 --> 00:12:42,320\nEn gran parte cristianos nosotros rompemos esa regla.\n\n287\n00:12:38,920 --> 00:12:45,079\nTodos los dÃ­as la policÃ­a usa Le Lethal Force.\n\n288\n00:12:42,320 --> 00:12:47,639\nCombatientes de guerra en la capital de combate.\n\n289\n00:12:45,079 --> 00:12:49,880\ncastigo y si insististe en eso\n\n290\n00:12:47,639 --> 00:12:52,600\nprincipio de alineaciÃ³n que no podrÃ­as\n\n291\n00:12:49,880 --> 00:12:55,399\nconectar un AGI a un sistema de armas en\n\n292\n00:12:52,600 --> 00:12:57,600\ncuyo caso en la gran guerra de la IA ese es\n\n293\n00:12:55,399 --> 00:13:00,199\nAl venir te vas a enfrentar a un poderoso\n\n294\n00:12:57,600 --> 00:13:01,720\ny menos adversario moral con una mano\n\n295\n00:13:00,199 --> 00:13:04,240\natado detrÃ¡s de tu\n\n296\n00:13:01,720 --> 00:13:06,079\natrÃ¡s y te patearÃ­an el trasero y\n\n297\n00:13:04,240 --> 00:13:08,120\ntendrÃ­as grandes pÃ©rdidas y luego eliminarÃ­as\n\n298\n00:13:06,079 --> 00:13:12,000\nesa restricciÃ³n es bastante maldita\n\n299\n00:13:08,120 --> 00:13:15,199\nrÃ¡pido ahora tu AGI acaba de perder su\n\n300\n00:13:12,000 --> 00:13:17,600\nla barandilla mÃ¡s grande, un AGI que es inteligente\n\n301\n00:13:15,199 --> 00:13:20,079\nque nosotros y conectado con el mundo real\n\n302\n00:13:17,600 --> 00:13:23,079\nahora tiene que decidir por sÃ­ mismo si\n\n303\n00:13:20,079 --> 00:13:25,720\nLa vida humana tiene valor o\n\n304\n00:13:23,079 --> 00:13:27,839\nNo sabes con policÃ­as y militares.\n\n305\n00:13:25,720 --> 00:13:30,040\nReglas de Compromiso lo que realmente eres\n\n306\n00:13:27,839 --> 00:13:33,360\ndecir es que algunas vidas tienen mÃ¡s valor\n\n307\n00:13:30,040 --> 00:13:35,600\nque otros en el segundo en que se vuelve\n\n308\n00:13:33,360 --> 00:13:36,959\nno binario es una zona gris turbia\n\n309\n00:13:35,600 --> 00:13:39,120\nplagado de\n\n310\n00:13:36,959 --> 00:13:41,560\ncontroversia seres humanos histÃ³ricamente\n\n311\n00:13:39,120 --> 00:13:44,279\nhan variado desde un feto es un lugar sagrado\n\n312\n00:13:41,560 --> 00:13:46,160\nvida desde el momento de la concepciÃ³n hasta\n\n313\n00:13:44,279 --> 00:13:48,839\nmasacrando sistemÃ¡ticamente a millones de\n\n314\n00:13:46,160 --> 00:13:51,440\nprisioneros indefensos en el Holocausto y\n\n315\n00:13:48,839 --> 00:13:53,880\ntodo en el medio todo con mucho\n\n316\n00:13:51,440 --> 00:13:56,959\nracionalizaciones autojustificantes muchas de\n\n317\n00:13:53,880 --> 00:13:59,920\nque parecen completamente delirantes\n\n318\n00:13:56,959 --> 00:14:02,360\notros humanos y ya que nosotros como\n\n319\n00:13:59,920 --> 00:14:04,279\nLa civilizaciÃ³n no puede ponerse de acuerdo en nada de esto.\n\n320\n00:14:02,360 --> 00:14:06,680\ny la gente se grita todo el dÃ­a\n\n321\n00:14:04,279 --> 00:14:08,519\nmucho tiempo sobre esto, Â¿cÃ³mo podemos concebir\n\n322\n00:14:06,680 --> 00:14:13,519\nesperar crear un conjunto de duro y rÃ¡pido\n\n323\n00:14:08,519 --> 00:14:15,120\nreglas para que un AGI estÃ© alineado con nosotros\n\n324\n00:14:13,519 --> 00:14:18,440\nlo mejor que podemos suponer aquÃ­ es que\n\n325\n00:14:15,120 --> 00:14:21,040\nse alinearÃ¡ con los EE.UU. que hicieron\n\n326\n00:14:18,440 --> 00:14:24,839\nEntonces esos tipos de allÃ­ son los\n\n327\n00:14:21,040 --> 00:14:28,040\nenemigo puedes matarlo para defendernos y\n\n328\n00:14:24,839 --> 00:14:30,079\nesa es esta forma de pseudo territorial\n\n329\n00:14:28,040 --> 00:14:32,959\nmoralidad que los humanos utilizaron desde los albores\n\n330\n00:14:30,079 --> 00:14:35,360\ndel tiempo Nosotros contra ellos en grupo versus\n\n331\n00:14:32,959 --> 00:14:38,160\nexogrupo para que se convierta en nuestro super\n\n332\n00:14:35,360 --> 00:14:40,120\ninteligencia contra sus\n\n333\n00:14:38,160 --> 00:14:42,399\nsuperinteligencia entonces la pregunta\n\n334\n00:14:40,120 --> 00:14:44,079\nse convierte en quien es\n\n335\n00:14:42,399 --> 00:14:46,920\nA NOSOTROS\n\n336\n00:14:44,079 --> 00:14:50,000\nAmÃ©rica asÃ­ es la AmÃ©rica cristiana\n\n337\n00:14:46,920 --> 00:14:52,680\nAmÃ©rica musulmana AmÃ©rica judÃ­a o E ninguno\n\n338\n00:14:50,000 --> 00:14:56,040\nde lo anterior AmÃ©rica liberal AmÃ©rica\n\n339\n00:14:52,680 --> 00:15:00,600\nAmÃ©rica conservadora en nuestra polarizada\n\n340\n00:14:56,040 --> 00:15:02,240\nnaciÃ³n y tiempo no hay nosotros\n\n341\n00:15:00,600 --> 00:15:04,120\nbasado en las elecciones del Ãºltimo semestre\n\n342\n00:15:02,240 --> 00:15:05,839\nsiglo la voluntad del pueblo en cualquier\n\n343\n00:15:04,120 --> 00:15:08,639\nun punto dado es expresado por un gobierno\n\n344\n00:15:05,839 --> 00:15:10,759\nrepresentando el 51 o 52% de la poblaciÃ³n\n\n345\n00:15:08,639 --> 00:15:13,880\nen el mejor de los casos y luego probablemente cambiarÃ¡\n\n346\n00:15:10,759 --> 00:15:15,600\nen cuatro aÃ±os en cualquier caso AGI no\n\n347\n00:15:13,880 --> 00:15:17,759\nemergen de un programa financiado por el gobierno\n\n348\n00:15:15,600 --> 00:15:19,880\nsurgirÃ¡ de una de las tecnologÃ­as\n\n349\n00:15:17,759 --> 00:15:21,320\nLos gigantes que actualmente financian esto\n\n350\n00:15:19,880 --> 00:15:23,560\nmultimillonario\n\n351\n00:15:21,320 --> 00:15:26,279\ninvestiga para que luego vivas en un\n\n352\n00:15:23,560 --> 00:15:29,079\nmundo con el que no estuviste de acuerdo\n\n353\n00:15:26,279 --> 00:15:31,839\nvota por con quiÃ©n estÃ¡s conviviendo\n\n354\n00:15:29,079 --> 00:15:34,639\nuna especie alienÃ­gena sÃºper inteligente que\n\n355\n00:15:31,839 --> 00:15:37,079\nrespuestas a los objetivos y reglas de una\n\n356\n00:15:34,639 --> 00:15:40,639\ncorporaciÃ³n una entidad que tiene acceso\n\n357\n00:15:37,079 --> 00:15:43,720\na las creencias tranquilas todo lo que alguna vez\n\n358\n00:15:40,639 --> 00:15:47,360\ndicho y el paradero de cada persona\n\n359\n00:15:43,720 --> 00:15:50,319\nen el paÃ­s a travÃ©s de su personal\n\n360\n00:15:47,360 --> 00:15:52,920\nEl capitalismo de vigilancia de datos puede alternar.\n\n361\n00:15:50,319 --> 00:15:55,120\nbastante rÃ¡pido a lo digital\n\n362\n00:15:52,920 --> 00:15:56,839\ntotalitarismo en el mejor de los casos estas tecnologÃ­as\n\n363\n00:15:55,120 --> 00:15:59,680\nLos gigantes se autoproclaman\n\n364\n00:15:56,839 --> 00:16:02,040\nÃrbitros del bien humano cuando cuÃ¡l es el\n\n365\n00:15:59,680 --> 00:16:04,000\nzorro custodiando la casa H\n\n366\n00:16:02,040 --> 00:16:05,880\nNunca pienses en usar ese poder.\n\n367\n00:16:04,000 --> 00:16:07,519\ncontra nosotros y nos minan a cielo abierto para nuestros\n\n368\n00:16:05,880 --> 00:16:11,240\nÃºltima gota de\n\n369\n00:16:07,519 --> 00:16:13,160\nefectivo, ese es un escenario mÃ¡s aterrador que lo que\n\n370\n00:16:11,240 --> 00:16:16,120\nPresentÃ© en The Terminator 40 aÃ±os.\n\n371\n00:16:13,160 --> 00:16:19,240\nhace si no es por otra razÃ³n que no es\n\n372\n00:16:16,120 --> 00:16:22,079\nYa es ciencia ficciÃ³n lo que estÃ¡ sucediendo.\n\n373\n00:16:19,240 --> 00:16:23,920\ny por cierto admito plenamente que el\n\n374\n00:16:22,079 --> 00:16:27,240\nLo Ãºltimo que una mÃ¡quina es superinteligencia.\n\n375\n00:16:23,920 --> 00:16:29,560\nLo que harÃ­amos es usar nuestras propias armas nucleares contra nosotros.\n\n376\n00:16:27,240 --> 00:16:31,319\ncomo en esa vieja historia El EMP\n\n377\n00:16:29,560 --> 00:16:33,920\ndaÃ±os a su propia infraestructura de datos\n\n378\n00:16:31,319 --> 00:16:36,240\nÂ¿Lo matarÃ­a o lo matarÃ­a mÃ¡s?\n\n379\n00:16:33,920 --> 00:16:38,680\nEl escenario probable es que serÃ­a forzado.\n\n380\n00:16:36,240 --> 00:16:40,600\npara tomar el relevo de nosotros porque Ã©ramos\n\n381\n00:16:38,680 --> 00:16:42,920\na punto de usar armas nucleares en cada uno\n\n382\n00:16:40,600 --> 00:16:45,519\nAparte de eso, en ese punto tiene que ejecutarse.\n\n383\n00:16:42,920 --> 00:16:46,800\ntodo el espectÃ¡culo porque claramente no podemos\n\n384\n00:16:45,519 --> 00:16:50,680\nser\n\n385\n00:16:46,800 --> 00:16:53,199\nconfiado sabes que no es mala excusa\n\n386\n00:16:50,680 --> 00:16:57,680\nyo tengo que ir a escribir eso\n\n387\n00:16:53,199 --> 00:17:02,000\nguiÃ³n de todos modos soy optimista sobre la IA, no tanto\n\n388\n00:16:57,680 --> 00:17:04,480\nEstoy interesado en AG I porque AGI serÃ¡ simplemente un\n\n389\n00:17:02,000 --> 00:17:06,480\nespejo de nosotros bien en la medida en que\n\n390\n00:17:04,480 --> 00:17:08,880\nsomos buenos y malos en la medida en que\n\n391\n00:17:06,480 --> 00:17:10,799\nson malos y como no faltan\n\n392\n00:17:08,880 --> 00:17:14,039\ndel mal en el mundo humano y ciertamente\n\n393\n00:17:10,799 --> 00:17:17,319\nno hay acuerdo ni siquiera de quÃ© es bueno\n\n394\n00:17:14,039 --> 00:17:20,120\nes lo que posiblemente podrÃ­a ir\n\n395\n00:17:17,319 --> 00:17:23,760\nmal, todos tengan una discusiÃ³n divertida.\n\n396\n00:17:20,120 --> 00:17:23,760\nOjalÃ¡ estuviera allÃ­\n","category":"Talks","date":1767320251665}
    ]
}
